[{"authors":["admin"],"categories":null,"content":"I am currently a PhD candidate in Economics at Sciences Po - LIEPP under the supervision of Jean-Marc Robin. My thesis committee also includes Xavier Ragot and Florian Oswald.\nMy research focuses on heterogeneous agents models and frictional markets. Prior to starting my PhD, I worked at the Economics Department of the OECD. You can find my resume here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://julienpascal.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am currently a PhD candidate in Economics at Sciences Po - LIEPP under the supervision of Jean-Marc Robin. My thesis committee also includes Xavier Ragot and Florian Oswald.\nMy research focuses on heterogeneous agents models and frictional markets. Prior to starting my PhD, I worked at the Economics Department of the OECD. You can find my resume here.","tags":null,"title":"Julien Pascal","type":"author"},{"authors":[],"categories":[],"content":" A Primer to Parallel Computing with Julia With this post, my aim is to provide a non-technical introduction to parallel computing using Julia. Our goal is to calculate an approximation of $\\pi$ using Monte-Carlo. I will use this example to introduce some basic Julia functions and concepts. For a more rigorous explanation, the manual is a must-read.\nCalculating $\\pi$ using Monte-Carlo Our strategy to calculate an approximation of $\\pi$ is quite simple. Let us consider a circle with radius $R$ inscribed in a square with side $2R$. The area of the circle, denoted by $a$, divided by the area of the square, denoted by $b$, is equal to $\\frac{\\pi}{4}$. Multiplying $\\frac{a}{b}$ by $4$ gives us $\\pi$. A slow but robust way of approximating areas is given by Monte-Carlo integration. In a nutshell, if we draw $N$ points within the square at random and we calculate the number of them falling within the circle denoted by $N_c$, $\\frac{N_c}{N}$ gives us an approximation for $\\frac{a}{b}$. The more draws, the more accurate the approximation.\nA serial implementation Let\u0026rsquo;s start with a serial version of the code\nusing Distributions using BenchmarkTools using Plots using Distributed  ┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] └ @ Base loading.jl:1192  #------------------------------------------------------------ # Function that returns 1 if the point with coordinates (x,y) # is within the unit circle; 0 otherwise #------------------------------------------------------------ function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end  inside_circle (generic function with 1 method)  #------------------------------------------------------------ # Function to calculate an approximation of pi #------------------------------------------------------------ function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end  pi_serial (generic function with 2 methods)  We can draw an increasing number of points and see how well the approximation for $\\pi$ performs. The following figure shows that increasing the number of points leads to a smaller error, even though the decreasing pattern is not uniform. The dashed line shows that the error descreases at a rate equal to the inverse of the square root of $N$.\nminPoints = 128 * 100000 maxPoints = 128 * 1000000 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTime1W = zeros(nbGridPoints) approximationPi1W = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPi1W[index] = pi_serial(nbDraws); #Store value elapsedTime1W[index] = @elapsed pi_serial(nbDraws); #Store time end  p = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, 1 ./(sqrt.(gridPoints)), label = \u0026quot;1/sqrt(n)\u0026quot;, linestyle = :dash) display(p) Plots.savefig(p, \u0026quot;convergence_rate.png\u0026quot;)  Adding \u0026ldquo;workers\u0026rdquo; When starting Julia, by default, only one processor is available. To increase the number of processors, one can use the command addprocs\nprintln(\u0026quot;Initial number of workers = $(nworkers())\u0026quot;) addprocs(4) println(\u0026quot;Current number of workers = $(nworkers())\u0026quot;)  Initial number of workers = 1 Current number of workers = 4  @spawn and fetch With Julia, one can go quite far only using the @spawnat and fetch functions. The command @spawnat starts an operation on a given process and returns an object of type Future. For instance, the next line starts the operation myid() on process 2:\nf = @spawnat 2 myid()  Future(2, 1, 6, nothing)  To get the result from the operation we just started on process 2, we need to \u0026ldquo;fetch\u0026rdquo; the results using the Future created above. As expected, the result is 2:\nfetch(f)  2  An important thing to know about @spawnat is that the \u0026ldquo;spawning\u0026rdquo; process will not wait for the operation to be finished before moving to the next task. This can be illustrated with following example:\n@time @spawnat 2 sleep(2.0)   0.008938 seconds (11.45 k allocations: 592.538 KiB) Future(2, 1, 8, nothing)  If the expected behavior is to wait for 2 seconds, this can be achieved by \u0026ldquo;fetching\u0026rdquo; the above operation:\n@time fetch(@spawnat 2 sleep(2.0))   2.101521 seconds (47.66 k allocations: 2.357 MiB, 0.48% gc time)  The bottom line is that process 1 can be used to start many operations in parallel using @spawnat and then collects the results from the different processes using fetch.\nA parallel implementation The strategy we used to approximate $\\pi$ does not need to be executed in serial. Since each draw is independent from previous ones, we could split the work between available workers (4 workers in this example). Each worker will calculate its own approximation for $\\pi$ and the final result will be average value across workers.\n#------------------------------------------------------------ # Let's redefine the function @everywhere so it can run on # the newly added workers #----------------------------------------------------------- @everywhere function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end  @everywhere using Distributions #------------------------------------------------------------ # Let's redefine the function @everywhere so it can run on # the newly added workers #----------------------------------------------------------- @everywhere function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end  @everywhere function pi_parallel(nbPoints::Int64 = 128 * 1000) # to store different approximations #---------------------------------- piWorkers = zeros(nworkers()) # to store Futures #----------------- listFutures=[] # divide the draws among workers #------------------------------- nbDraws = Int(nbPoints/4) # each calculate its own approximation #------------------------------------- for (workerIndex, w) in enumerate(workers()) push!(listFutures, @spawnat w pi_serial(nbDraws)) end # let's fetch results #-------------------- for (workerIndex, w) in enumerate(workers()) piWorkers[workerIndex] = fetch(listFutures[workerIndex]) end # return the mean value across worker return mean(piWorkers) end  minPoints = 128 * 100000 maxPoints = 128 * 1000000 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTimeNW = zeros(nbGridPoints) approximationPiNW = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPiNW[index] = pi_parallel(nbDraws); #Store value elapsedTimeNW[index] = @elapsed pi_parallel(nbDraws); #Store time end  Serial vs parallel comparisons In terms of accuracy, the serial and the parallel codes generate the same results (modulo randomness). In terms of speed, the parallel version is up to 2.5 times faster. The more points are drawn, the higher the speed-gains. This example shows the well-established fact that the advantages of parallel computing start to kick-in when the underlying tasks are time-consuming in the first place.\np = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, abs.(approximationPiNW .- pi), label = \u0026quot;Parallel\u0026quot;) Plots.title!(\u0026quot;Error\u0026quot;) Plots.xlabel!(\u0026quot;nb Draws\u0026quot;) Plots.ylabel!(\u0026quot;Error\u0026quot;) display(p) Plots.savefig(p,\u0026quot;error_comparison.png\u0026quot;)  p = Plots.plot(gridPoints, elapsedTime1W, label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, elapsedTimeNW, label = \u0026quot;Parallel\u0026quot;) Plots.plot!(gridPoints, elapsedTime1W./elapsedTimeNW, label = \u0026quot;Speed-up\u0026quot;) Plots.xlabel!(\u0026quot;nb Draws\u0026quot;) Plots.ylabel!(\u0026quot;Time (s)\u0026quot;) display(p) Plots.savefig(\u0026quot;Speed_gains.png\u0026quot;)  ","date":1552931602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552931602,"objectID":"56a6614a63bc5a3030cb63aa8903ae8d","permalink":"https://julienpascal.github.io/post/primerparallel/","publishdate":"2019-03-18T18:53:22+01:00","relpermalink":"/post/primerparallel/","section":"post","summary":"A Primer to Parallel Computing with Julia With this post, my aim is to provide a non-technical introduction to parallel computing using Julia. Our goal is to calculate an approximation of $\\pi$ using Monte-Carlo. I will use this example to introduce some basic Julia functions and concepts. For a more rigorous explanation, the manual is a must-read.\nCalculating $\\pi$ using Monte-Carlo Our strategy to calculate an approximation of $\\pi$ is quite simple.","tags":[],"title":"A Primer to Parallel Computing with Julia","type":"post"},{"authors":null,"categories":null,"content":"We develop a theoretical framework to evaluate the contribution of different payroll tax schedules to business cycle fluctuations. We build and estimate a dynamic search-and-matching model of the labor market featuring heterogeneous workers, aggregate and idiosyncratic shocks and a non-linear payroll tax schedule. We estimate the model on Italian administrative data for the period 1977-2012 and use our estimated framework to quantitatively evaluate how different payroll tax schedules can amplify business cycle shocks for different types of workers. This is a joint project with Nicolò Dalvit.\n","date":1552583478,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552583478,"objectID":"f0eb6212aae6aa66301eaf6a155ac2bd","permalink":"https://julienpascal.github.io/project/taxation_labor/","publishdate":"2019-03-14T18:11:18+01:00","relpermalink":"/project/taxation_labor/","section":"project","summary":"We analyse the impact of labor income tax in an heterogeneous workers framework with search frictions and aggregate shocks.","tags":["WIP"],"title":"Labor Tax in a Dynamic Search-and-Matching Model","type":"project"},{"authors":null,"categories":null,"content":"This is a joint project with Tyler Abbot.\n","date":1552583467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552583467,"objectID":"8f00f2830975dcb8fc1edcb0b10b0cd8","permalink":"https://julienpascal.github.io/project/dcapm/","publishdate":"2019-03-14T18:11:07+01:00","relpermalink":"/project/dcapm/","section":"project","summary":"This is a joint project with Tyler Abbot.","tags":["WIP"],"title":"DCAPM","type":"project"},{"authors":[],"categories":[],"content":" NOTE\nThis post is outdated. With the advent of Julia 1.0, the workflow for creating packages was significantly altered. An excellent guide can be found here.\nIn this post, my goal is to briefly explain how to create an unregistered Julia package for Julia 0.6.4, how to synchronize it with your Github account, and how to start testing your code automatically using TRAVIS CI. I started writing this post as a reminder to myself. I am posting it here with the hope that it may be useful for someone else. More on this topic can be found by reading the official Julia\u0026rsquo;s manual.\nWhy Creating a Package? A package to share academic work My research projects often involve data manipulation and/or implementing algorithms. I discovered that writing my codes in the form of a package helps me in producing better and reusable code. Creating a package to share your academic work is also very much in line with the idea that scientific research should be reproducible. Users can download your work and install the required dependencies using a single line :\ngit.clone(\u0026quot;https://github.com/YourGithubUsername/YourPackage.jl.git\u0026quot;)  Continuous Integration Another major advantage of creating a package is that it makes your life much easier when it comes to testing your code automatically using TRAVIS CI. TRAVIS CI is a continuous integration system, which considerably helps in detecting and resolving bugs at an early stage.\nStep-by-step tutorial In what follows, I am assuming you are using Linux, with julia version 0.6 installed. If you are using a different version, just replace v0.6 by the number corresponding to your current version of julia. You also need to have the package PkgDev installed.\nStep 1: Generate your package The following two lines will create a directory called \u0026quot;MyPackage.jl\u0026quot; with an MIT License, in Julia\u0026rsquo;s package location:\nusing PkgDev PkgDev.generate(\u0026quot;MyPackage.jl\u0026quot;,\u0026quot;MIT\u0026quot;)  By convention, Julia repository names and with .jl. If you change your working directory to your newly created package (cd ~/.julia/v0.6/MyPackage), you will notice that the following files and directories have been created:\n\\src The \\src folder will contain your source code. By default, it contains a file \u0026ldquo;MyPackage.jl\u0026rdquo;, which you will use to load other packages and to include .jl files that you created. In this file, you also state the functions and types you want to export. As an example, you may consult the package Distributions.\n\\test This folder contains a file runtests.jl, in which you can include unit-tests. Within julia, you can simply run your series of unit-tests with the command:\nPkg.test(\u0026quot;MyPackage\u0026quot;)  REQUIRE This file is used to specify the required dependencies. When a user Pkg.clone() your package, Julia\u0026rsquo;s package manager will make sure that these requirements are met. For instance, let\u0026rsquo;s say that your package relies on the version 0.6 of Julia (or higher) and the package JSON. The REQUIRE file will be the following :\njulia 0.6 JSON  README.md You can use this file to add a description of you package.\nLICENSE.md To guide you in the choice of a licence, you may want to consult the following website: https://choosealicense.com/\nStep 2: Set-up your working environment This step is optional. While you may want to develop you package directly from Julia\u0026rsquo;s package directory (~/.julia/v0.6 if you are using julia v0.6), I personally find it unpleasant. I usually create a symlink to a more convenient location:\nln -s ~/.julia/v0.6/MyPackage your/convenient/directory/MyPackage  After running this line in the terminal, you can start working on your package directly from your/convenient/directory.\nStep 3: Synchronize with GitHub The following step will synchronize your package with your GitHub account. After creating a repository named \u0026ldquo;MyPackage.jl\u0026rdquo; on GitHub, enter the following commands in the terminal:\ngit add -A git commit -m \u0026quot;First commit\u0026quot; git remote add origin https://github.com/YourGithubUsername/MyPackage.jl.git git push -u origin master  Going to the page https://github.com/YourGithubUsername/MyPackage.jl.git, you should now see folders and files mentioned above. Some extra files are also going to be there, for instance .gitignore or appveyor.yml. You can ignore them for the time being. After this initial commit, you are almost all set and you can use the usual GitHub workflow. A good idea though is to enable TRAVIS CI for the repository just you created.\nStep 4: Set-up TRAVIS CI From your GitHub account, sign in to either:\n TravisCI.org if your repository is public TravisCI.com if your repository is private  On TRAVIS CI, go to your profile page. Enable your repository \u0026ldquo;YourGithubUsername/MyPackage.jl\u0026rdquo; by flicking the switch one. Every time you push a new commit, your set of tests, launched by the file /test/runtests.jl, will be automatically executed on a separate virtual environment. If one of your tests fails, you will be notified by e-mail and (most of the time) you will be able to spot the origin of the error quite easily.\n","date":1528295678,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528295678,"objectID":"a2695d3b77c42a7382e2c7a98d4099a5","permalink":"https://julienpascal.github.io/post/julia_package/","publishdate":"2018-06-06T15:34:38+01:00","relpermalink":"/post/julia_package/","section":"post","summary":"NOTE\nThis post is outdated. With the advent of Julia 1.0, the workflow for creating packages was significantly altered. An excellent guide can be found here.\nIn this post, my goal is to briefly explain how to create an unregistered Julia package for Julia 0.6.4, how to synchronize it with your Github account, and how to start testing your code automatically using TRAVIS CI. I started writing this post as a reminder to myself.","tags":[],"title":"How to Create a Julia Package","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" In my previous post, I discussed how the the simulated method of moments can be used to estimate parameters without using the likelihood function. This method is useful because many \u0026ldquo;real-life\u0026rdquo; applications result in untractable likelihood functions. In this post, I use the same toy example (estimation of the mean of a mutlivariate normal random variable) and show how to use the parallel computing capabilities of Julia and MomentOpt to speed-up the estimation.\nAdding workers In this example, the goal is to estimate the mean of 4-dimension normal random variable with unit variance, without using any information on the likelihood. If you start Julia with several processors, MomentOpt will notice it and execute the code in parallel. The first step is to add \u0026ldquo;workers\u0026rdquo; to Julia. A rule of thumb is to use as many workers as you have processors on your system (4 in my case).\n# Current number of workers #-------------------------- currentWorkers = nprocs() println(\u0026quot;Initial number of workers = $(currentWorkers)\u0026quot;) # I want to have 4 workers running #-------------------------------- maxNumberWorkers = 4 while nprocs() \u0026lt; maxNumberWorkers addprocs(1) end # check the number of workers: #---------------------------- currentWorkers = nprocs() println(\u0026quot;Number of workers = $(currentWorkers)\u0026quot;) Initial number of workers = 1 Number of workers = 4  @everywhere When running Julia with several workers, you have to add the macro @everywhere when loading packages and defining functions. More details on parallel computing with Julia can be found here.\n#--------------------------------------------------------------------------------------------------------- # Julien Pascal # https://julienpascal.github.io/ # last edit: 06/06/2018 # # Julia script that shows how the simulated method of moments can be used in # a simple setting: estimation of the mean of a Normal r.v. # This version was built to be executed with several processors # For instance, start julia with: julia -p 4 # # I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl # # Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl #---------------------------------------------------------------------------------------------------------- @everywhere using MomentOpt @everywhere using GLM @everywhere using DataStructures @everywhere using DataFrames @everywhere using Plots #plotlyjs() @everywhere pyplot() #------------------------------------------------ # Options #------------------------------------------------- # Boolean: do you want to save the plots to disk? savePlots = true #------------------------ # initialize the problem: #------------------------ # Specify the initial values for the parameters, and their support: #------------------------------------------------------------------ pb = OrderedDict(\u0026quot;p1\u0026quot; =\u0026gt; [0.2,-3,3] , \u0026quot;p2\u0026quot; =\u0026gt; [-0.2,-2,2], \u0026quot;p3\u0026quot; =\u0026gt; [0.1,0,10], \u0026quot;p4\u0026quot; =\u0026gt; [-0.1,-10,0]) # Specify moments to be matched + subjective weights: #---------------------------------------------------- trueValues = OrderedDict(\u0026quot;mu1\u0026quot; =\u0026gt; [-1.0] , \u0026quot;mu2\u0026quot; =\u0026gt; [1.0], \u0026quot;mu3\u0026quot; =\u0026gt; [5.0], \u0026quot;mu4\u0026quot; =\u0026gt; [-4.0]) moms = DataFrame(name=[\u0026quot;mu1\u0026quot;,\u0026quot;mu2\u0026quot;,\u0026quot;mu3\u0026quot;, \u0026quot;mu4\u0026quot;],value=[-1.0,1.0, 5.0, -4.0], weight=ones(4)) # objfunc_normal(ev::Eval) # # GMM objective function to be minized. # It returns a weigthed distance between empirical and simulated moments # @everywhere function objfunc_normal(ev::Eval; verbose = false) start(ev) # when running in parallel, display worker's id: #----------------------------------------------- if verbose == true if nprocs() \u0026gt; 1 println(myid()) end end # extract parameters from ev: #---------------------------- mu = collect(values(ev.params)) # compute simulated moments #-------------------------- # Monte-Carlo: #------------- ns = 10000 #number of i.i.d draws from N([mu], sigma) #initialize a multivariate normal N([mu], sigma) #mu is a four dimensional object #sigma is set to be the identity matrix sigma = [1.0 ;1.0; 1.0; 1.0] # draw ns observations from N([mu], sigma): randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma)) # calculate the mean of the simulated data simM = mean(rand(randMultiNormal,ns),2) # store simulated moments in a dictionary simMoments = Dict(:mu1 =\u0026gt; simM[1], :mu2 =\u0026gt; simM[2], :mu3 =\u0026gt; simM[3], :mu4 =\u0026gt; simM[4]) # Calculate the weighted distance between empirical moments # and simulated ones: #----------------------------------------------------------- v = Dict{Symbol,Float64}() for (k, mom) in dataMomentd(ev) # If weight for moment k exists: #------------------------------- if haskey(MomentOpt.dataMomentWd(ev), k) # divide by weight associated to moment k: #---------------------------------------- v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2 else v[k] = ((simMoments[k] .- mom) ) .^2 end end # Set value of the objective function: #------------------------------------ setValue(ev, mean(collect(values(v)))) # also return the moments #----------------------- setMoment(ev, simMoments) # flag for success: #------------------- ev.status = 1 # finish and return finish(ev) return ev end # Initialize an empty MProb() object: #------------------------------------ mprob = MProb() # Add structural parameters to MProb(): # specify starting values and support #-------------------------------------- addSampledParam!(mprob,pb) # Add moments to be matched to MProb(): #-------------------------------------- addMoment!(mprob,moms) # Attach an objective function to MProb(): #---------------------------------------- addEvalFunc!(mprob, objfunc_normal) # estimation options: #-------------------- # number of iterations for each chain niter = 1000 # number of chains # nchains = nprocs() nchains = 4 opts = Dict(\u0026quot;N\u0026quot;=\u0026gt;nchains, \u0026quot;maxiter\u0026quot;=\u0026gt;niter, \u0026quot;maxtemp\u0026quot;=\u0026gt; 5, \u0026quot;coverage\u0026quot;=\u0026gt;0.025, \u0026quot;sigma_update_steps\u0026quot;=\u0026gt;10, \u0026quot;sigma_adjust_by\u0026quot;=\u0026gt;0.01, \u0026quot;smpl_iters\u0026quot;=\u0026gt;1000, \u0026quot;parallel\u0026quot;=\u0026gt;true, \u0026quot;maxdists\u0026quot;=\u0026gt;[0.05 for i in 1:nchains], \u0026quot;mixprob\u0026quot;=\u0026gt;0.3, \u0026quot;acc_tuner\u0026quot;=\u0026gt;12.0, \u0026quot;animate\u0026quot;=\u0026gt;false) #--------------------------------------- # Let's set-up and run the optimization #--------------------------------------- # set-up BGP algorithm: MA = MAlgoBGP(mprob,opts) # run the estimation: @time MomentOpt.runMOpt!(MA) # show a summary of the optimization: @show MomentOpt.summary(MA)  Inference When using the BGP algorithm, inference can be done using the first chain only. Other chains are used to explore the state space and help to exit potential local minima, but they are not meant to be used for inference. I discard the first 10th observations to get rid of the influence of the starting values. Visual inspection of the first chain suggests that the stationary part of the Markov chain was reached at this stage. I then report the quasi posterior mean and median for each parameter. As reported below, we are quite close to the true values.\n# Plot histograms for the first chain, the one with which inference should be done. # Other chains are used to explore the space and avoid local minima #------------------------------------------------------------------------------- p1 = histogram(MA.chains[1]) display(p1) if savePlots == true savefig(p1, joinpath(pwd(),\u0026quot;histogram_chain1.svg\u0026quot;)) end # Plot the realization of the first chain #---------------------------------------- p2 = plot(MA.chains[1]) if savePlots == true savefig(p2, joinpath(pwd(),\u0026quot;history_chain_1.svg\u0026quot;)) end display(p2) # Realization of the first chain: #------------------------------- dat_chain1 = MomentOpt.history(MA.chains[1]) # discard the first 10th of the observations (\u0026quot;burn-in\u0026quot; phase): #-------------------------------------------------------------- dat_chain1[round(Int, niter/10):niter, :] # keep only accepted draws: #-------------------------- dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ] # create a list with the parameters to be estimated parameters = [Symbol(String(\u0026quot;mu$(i)\u0026quot;)) for i=1:4] # list with the corresponding priors: #------------------------------------ estimatedParameters = [Symbol(String(\u0026quot;p$(i)\u0026quot;)) for i=1:4] # Quasi Posterior mean and quasi posterior median for each parameter: #------------------------------------------------------------------- for (estimatedParameter, param) in zip(estimatedParameters, parameters) println(\u0026quot;Quasi posterior mean for $(String(estimatedParameter)) = $(mean(dat_chain1[estimatedParameter]))\u0026quot;) println(\u0026quot;Quasi posterior median for $(String(estimatedParameter)) = $(median(dat_chain1[estimatedParameter]))\u0026quot;) println(\u0026quot;True value = $(trueValues[String(param)][])\u0026quot;) end # Output: #-------- # Quasi posterior mean for p1 = -0.9160461484604642 # Quasi posterior median for p1 = -0.9589739759449558 # True value = -1.0 # Quasi posterior mean for p2 = 0.9888798123473025 # Quasi posterior median for p2 = 1.0675028518780796 # True value = 1.0 # Quasi posterior mean for p3 = 4.922658319685393 # Quasi posterior median for p3 = 4.989662707150382 # True value = 5.0 # Quasi posterior mean for p4 = -3.898597557236236 # Quasi posterior median for p4 = -3.968649064061086 # True value = -4.0  Figure 1         History of chain 1    Figure 2         Histograms for chain 1    Safety checks In this toy example, we know that the conditions for global identification are met. However, in more complicated applications, global identification may be hard to prove analytically. A common practice is to make sure that the objective function is \u0026ldquo;well-behaved\u0026rdquo; in a neighborhood of the solution using slices. The graph below shows that there is no flat region in the neighborhood of the solution, suggesting (at least) local identification of the parameters.\n# plot slices of objective function # grid with 20 points #----------------------------------- s = doSlices(mprob,20) # plot slices of the objective function: #--------------------------------------- p = MomentOpt.plot(s,:value) display(p) if savePlots == true Plots.savefig(p, joinpath(pwd(),\u0026quot;slices_Normal.svg\u0026quot;)) end # Produce more precise plots with respect to each parameter: #----------------------------------------------------------- for symbol in parameters p = MomentOpt.plot(s,symbol) display(p) if savePlots == true Plots.savefig(p, joinpath(pwd(),\u0026quot;slices_Normal_$(String(symbol)).svg\u0026quot;)) end end  Figure 3         Slices of the objective function    Parallel versus Serial Here is benchmark of running the code above in serial versus in parallel (starting julia with 4 workers):\n Serial: 639.661831 seconds (12.52 M allocations: 1.972 GiB, 97.50% gc time) Parallel: 372.454707 seconds (279.32 M allocations: 14.843 GiB, 2.19% gc time)  Computing time is approximately divided by 2 when executing the parallel version.\nNotebook A jupyter notebook containing the code in this post (with some slight modifications) can be downloaded here.\n","date":1528295169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528295169,"objectID":"a160ada05108f0a665014ad22feafebf","permalink":"https://julienpascal.github.io/post/smm_parallel/","publishdate":"2018-06-06T15:26:09+01:00","relpermalink":"/post/smm_parallel/","section":"post","summary":"In my previous post, I discussed how the the simulated method of moments can be used to estimate parameters without using the likelihood function. This method is useful because many \u0026ldquo;real-life\u0026rdquo; applications result in untractable likelihood functions. In this post, I use the same toy example (estimation of the mean of a mutlivariate normal random variable) and show how to use the parallel computing capabilities of Julia and MomentOpt to speed-up the estimation.","tags":[],"title":"The Simulated Method of Moments: a Parallel Implementation","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" As Thomas Sargent said:\n \u0026ldquo;A rational expectations equilibrium model is a likelihood function\u0026rdquo;\n However in many cases, the likelihood function is too complicated to be written down in closed form. To estimate the structural parameters of a given model, one can still use Monte-Carlo methods. In this post, I would like to describe the simulated method of moments (SMM), which is a widely used simulation-based estimation technique.\nA Simple Setting I want to illustrate the SMM in one of the simplest settings you could think of: the estimation of the mean of a normal density with known variance. Let\u0026rsquo;s say we have access to a (bi-dimensional) time series and we suspect it to be normally distributed with mean $[a,\\,b]\u0026lsquo;$ and variance the identity matrix $\\mathcal{N}([a,\\,b]\u0026lsquo;,\\,I_2)$. Let\u0026rsquo;s pretend that we have no idea on how to write down the associated likelihood function. The good news is that if we have access to a \u0026ldquo;black box\u0026rdquo; that generates $i.i.d$ draws from the law $\\mathcal{N}([c,\\,d]\u0026lsquo;,\\,I_2)$, it is enough for us to do inference.\nSMM is GMM The SMM estimator can be viewed as an extension of GMM. The difference being that the function mapping the set of parameters to moments has no closed-form expression. Mathematically, we want to minimize the following objective function:\nwhere $m^*$ is a vector empirical moments, $m(\\theta)$ a vector of the same moments calculated using simulated data when the structural parameters are equal to $\\theta$, and $W$ a weighing matrix.\nThe SMM estimate is such that the (weighted) distance between simulated and empirical moments is minimized. This estimator is quite intuitive: under the hypothesis that the model is correctly specified, it should be able to reproduce empirical moments when parameters values are equal to the \u0026ldquo;true\u0026rdquo; ones.\nInference Under some regularity conditions (see McFadden 1989), the extra noise introduced by simulation is not problematic and inference is possible. That is, we can build a confidence intervals for the SMM estimates.\nImplementation in Julia The code below shows how one can recover the true parameters of the Normal density $\\mathcal{N}([a,\\,b]\u0026lsquo;,\\,I_2)$. I use the MomentOpt package, which relies on some refinements of the MCMC method to explore the state-space with several Markov chains in parallel. These Markov chains communicate between themselves to avoid being stuck in a local mode of the posterior distribution. In practice, I choose 5 Markov chains. Figure 1 shows the realizations for the first chain. As illustrated in Figure 2, we successfully recovered the true values for $a$ and $b$. For each chain, the quasi-posterior mean and median for $a$ and $b$ are extremely close to the true values $1$ and $-1$.\nFigure 1         History of one chain    Figure 2         Histograms for the 5 chains    #--------------------------------------------------------------------------------------------------------- # Julien Pascal # last edit: 12/02/2018 # # Julia script that shows how the simulated # method of moments can be used in a simple # setting: estimation of the mean of a Normal r.v. # # I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl # # Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl #---------------------------------------------------------------------------------------------------------- using MomentOpt using GLM using DataStructures using DataFrames using Plots #plotlyjs() pyplot() #------------------------------------------------ # Options #------------------------------------------------- # Boolean: do you want to save the plots to disk? savePlots = true # initialize the problem: #------------------------ # initial values: #---------------- pb = OrderedDict(\u0026quot;p1\u0026quot; =\u0026gt; [0.2,-3,3] , \u0026quot;p2\u0026quot; =\u0026gt; [-0.2,-2,2] ) # moments to be matched: #----------------------- moms = DataFrame(name=[\u0026quot;mu1\u0026quot;,\u0026quot;mu2\u0026quot;],value=[-1.0,1.0], weight=ones(2)) \u0026quot;\u0026quot;\u0026quot; objfunc_normal(ev::Eval) GMM objective function to be minized. It returns a weigthed distance between empirical and simulated moments copy-paste of the function objfunc_norm(ev::Eval) I only made minor modifications to the original fuction \u0026quot;\u0026quot;\u0026quot; function objfunc_normal(ev::Eval) start(ev) # extract parameters from ev: #---------------------------- mu = collect(values(ev.params)) # compute simulated moments #-------------------------- # Monte-Carlo: #------------- ns = 10000 #number of i.i.d draws from N([a,b], sigma) #initialize a multivariate normal N([a,b], sigma) #using a = mu[1], b=mu[2] sigma = [1.0 ;1.0] randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma)) simM = mean(rand(randMultiNormal,ns),2) #mean of simulated data simMoments = Dict(:mu1 =\u0026gt; simM[1], :mu2 =\u0026gt; simM[2])#store simulated moments in a dictionary # Calculate the weighted distance between empirical moments # and simulated ones: #----------------------------------------------------------- v = Dict{Symbol,Float64}() for (k, mom) in dataMomentd(ev) # If weight for moment k exists: #------------------------------- if haskey(MomentOpt.dataMomentWd(ev), k) # divide by weight associated to moment k: #---------------------------------------- v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2 else v[k] = ((simMoments[k] .- mom) ) .^2 end end # Set value of the objective function: #------------------------------------ setValue(ev, mean(collect(values(v)))) # also return the moments #----------------------- setMoment(ev, simMoments) # flag for success: #------------------- ev.status = 1 # finish and return finish(ev) return ev end # Initialize an empty MProb() object: #------------------------------------ mprob = MProb() # Add structural parameters to MProb(): # specify starting values and support #-------------------------------------- addSampledParam!(mprob,pb) # Add moments to be matched to MProb(): #-------------------------------------- addMoment!(mprob,moms) # Attach an objective function to MProb(): #---------------------------------------- addEvalFunc!(mprob, objfunc_normal) # estimation options: #-------------------- # number of iterations for each chain niter = 1000 # number of chains nchains = 5 opts = Dict(\u0026quot;N\u0026quot;=\u0026gt;nchains, \u0026quot;maxiter\u0026quot;=\u0026gt;niter, \u0026quot;maxtemp\u0026quot;=\u0026gt; 5, # choose inital sd for each parameter p # such that Pr( x \\in [init-b,init+b]) = 0.975 # where b = (p[:ub]-p[:lb])*opts[\u0026quot;coverage\u0026quot;] i.e. the fraction of the search interval you want to search around the initial value \u0026quot;coverage\u0026quot;=\u0026gt;0.025, # i.e. this gives you a 95% CI about the current parameter on chain number 1. \u0026quot;sigma_update_steps\u0026quot;=\u0026gt;10, \u0026quot;sigma_adjust_by\u0026quot;=\u0026gt;0.01, \u0026quot;smpl_iters\u0026quot;=\u0026gt;1000, \u0026quot;parallel\u0026quot;=\u0026gt;true, \u0026quot;maxdists\u0026quot;=\u0026gt;[0.05 for i in 1:nchains], \u0026quot;mixprob\u0026quot;=\u0026gt;0.3, \u0026quot;acc_tuner\u0026quot;=\u0026gt;12.0, \u0026quot;animate\u0026quot;=\u0026gt;false) # plot slices of objective function #--------------------------------- s = doSlices(mprob,30) # plot objective function over param values: #------------------------------------------- p1 = MomentOpt.plot(s,:value) if savePlots == true Plots.savefig(p1, joinpath(pwd(),\u0026quot;slices_Normal1.svg\u0026quot;)) end # plot value of moment :mu1 over param values #-------------------------------------------- p2 = MomentOpt.plot(s,:mu1) if savePlots == true Plots.savefig(p2, joinpath(pwd(),\u0026quot;slices_Normal2.svg\u0026quot;)) end # plot value of moment :mu2 over param values #-------------------------------------------- p3 = Plots.plot(s,:mu2) if savePlots == true Plots.savefig(p3, joinpath(pwd(),\u0026quot;slices_Normal3.svg\u0026quot;)) end #--------------------------------------- # Let's set-up and run the optimization #--------------------------------------- # set-up BGP algorithm: MA = MAlgoBGP(mprob,opts) # run the estimation: @time MomentOpt.runMOpt!(MA) # show a summary of the optimization: @show MomentOpt.summary(MA) # Plot histograms for chains: #---------------------------- p4 = histogram(MA.chains[1]) p5 = histogram(MA.chains[2]) p6 = histogram(MA.chains[3]) p7 = histogram(MA.chains[4]) p8 = histogram(MA.chains[5]) p9 = Plots.plot(p4, p5, p6, p7, p8, layout=(5,1), legend=false) if savePlots == true savefig(p9, joinpath(pwd(),\u0026quot;histogram.svg\u0026quot;)) end # Plot the \u0026quot;history\u0026quot; of one chain: #-------------------------------- p10 = plot(MA.chains[1]) if savePlots == true savefig(p10, joinpath(pwd(),\u0026quot;history_chain_1.svg\u0026quot;)) end # Realization of chain 1: #----------------------- dat_chain1 = MomentOpt.history(MA.chains[1]) # keep only accepted draws: #------------------------- dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ] # Quasi Posterior mean #--------------------- QP_mean_p1 = mean(dat_chain1[:p1]) QP_mean_p2 = mean(dat_chain1[:p2]) # Quasi Posterior median #----------------------- QP_median_p1 = median(dat_chain1[:p1]) QP_median_p2 = median(dat_chain1[:p2])  ","date":1518444453,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518444453,"objectID":"65cd9e8ec5e2630b42fad15e6ec31b76","permalink":"https://julienpascal.github.io/post/smm/","publishdate":"2018-02-12T15:07:33+01:00","relpermalink":"/post/smm/","section":"post","summary":"As Thomas Sargent said:\n \u0026ldquo;A rational expectations equilibrium model is a likelihood function\u0026rdquo;\n However in many cases, the likelihood function is too complicated to be written down in closed form. To estimate the structural parameters of a given model, one can still use Monte-Carlo methods. In this post, I would like to describe the simulated method of moments (SMM), which is a widely used simulation-based estimation technique.","tags":[],"title":"The Simulated Method of Moments","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" A large class of economic models involves solving for functional equations of the form:\nA well known example is the stochastic optimal growth model. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period\u0026rsquo;s output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.\nA common technique for solving this class of problem is value function iteration. While value function iteration is quite intuitive, it is not the only one available. In this post, I describe the collocation method, which transforms the problem of finding a function into a problem of finding a vector that satisfies a given set of conditions. The gain from this change of perspective is that any root-finding algorithm can then be used. In particular, one may use the Newton method, which converges at a quadratic rate in the neighborhood of the solution if the function is smooth enough.\nValue function iteration Value function iteration takes advantage of the fact that the Bellman operator $T$ is a contraction mapping on the set of continuous bounded functions on $\\mathbb R_+$ under the supremum distance\nAn immediate consequence if that the sequence $w,Tw,T^2w$,… converges uniformly to $w$ (starting with any bounded and continuous function $w$). The following code in Julia v0.6.4 illustrates the convergence of the series ${T^nw}$.\n#= Julien Pascal Code heavily based on: ---------------------- https://lectures.quantecon.org/jl/optgrowth.html by Spencer Lyon, John Stachurski I have only made minor modifications: #------------------------------------ * I added a type optGrowth * I use the package Interpolations * I calculate the expectation w.r.t the aggregate shock using a Gauss-Legendre quadrature scheme instead of Monte-Carlo =# using QuantEcon using Optim using CompEcon using PyPlot using Interpolations using FileIO type optGrowth w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool w_func::Function end function optGrowth(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true ) grid_y = collect(linspace(1e-5, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log(c) # Production f(k) = k^α w = 5 * log.(grid_y) Tw = 5 * log.(grid_y) σ = 5 * log.(grid_y) el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) w_func = x -\u0026gt; x optGrowth( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, w_func ) end \u0026quot;\u0026quot;\u0026quot; The approximate Bellman operator, which computes and returns the updated value function Tw on the grid points. #### Arguments `model` : a model of type optGrowth `Modifies model.σ, model.w and model.Tw \u0026quot;\u0026quot;\u0026quot; function bellman_operator!(model::optGrowth) # === Apply linear interpolation to w === # knots = (model.grid,) itp = interpolate(knots, model.w, Gridded(Linear())) #w_func(x) = itp[x] model.w_func = x -\u0026gt; itp[x] if model.compute_policy model.σ = similar(model.w) end # == set Tw[i] = max_c { u(c) + β E w(f(y - c) z)} == # for (i, y) in enumerate(model.grid) #Monte Carlo #----------- #objective(c) = - model.u(c) - model.β * mean(w_func(model.f(y - c) .* model.shocks)) #Gauss-Legendre #-------------- function objective(c) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end res = optimize(objective, 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = - Optim.minimum(res) end end model = optGrowth() function solve_optgrowth!(model::optGrowth; tol::AbstractFloat=1e-6, max_iter::Integer=500) w_old = copy(model.w) # Set initial condition error = tol + 1 i = 0 # Iterate to find solution while i \u0026lt; max_iter #update model.w bellman_operator!(model) error = maximum(abs, model.w - w_old) if error \u0026lt; tol break end w_old = copy(model.w) i += 1 end end #----------------------------------- # Solve by value function iteration #----------------------------------- @time solve_optgrowth!(model) # 3.230501 seconds (118.18 M allocations: 1.776 GiB, 3.51% gc time) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Approximated (VFI)\u0026quot;, linestyle=\u0026quot;--\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Exact value\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;value_function_iteration.png\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](0.00, 0.008) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;error_value_function_iteration.png\u0026quot;)  Output The following figure shows the exact function, which we know in this very specific case, and the one calculated using VFI. Both quantities are almost indistinguishable. As the illustrated in the next figure, the (absolute value) of the distance between the true and the approximated values is bounded above by $0.006$. The accuracy of the current approximation could be improved by iterating the process further. The collocation method The collocation method takes a different route. Let us remember that we are looking for a function $w$. Instead of solving for the values of $w$ on a grid and then interpolating, why not looking for a function directly? To do so, let us assume that $w$ can reasonably be approximated by a function $\\hat{w}$:\nwith $ \\phi_1(x) $ , $ \\phi_2(x) $,\u0026hellip;, $ \\phi_n(x) $ a set of linearly independent basis functions and $c_1$, $c_2$, \u0026hellip;, $c_n$ $n$ coefficient to be found. Replacing $w(x)$ with $\\hat{w(x)}$ into the functional equation and reorganizing gives:\nThis equation has to hold (almost) exactly at $n$ points (also called nodes): $y_1$, $y_2$, \u0026hellip;, $y_n$:\nThe equation above defines a system of $n$ equation with as many unknown, which can be compactly written as: $$ f(\\boldsymbol{c}) = \\boldsymbol{0} $$\nNewton or quasi-Newton can be used to solve for the root of $f$. In the code that follows, I use Broyden\u0026rsquo;s method. Let us illustrate this technique using a Chebychev polynomial basis and Chebychev nodes. In doing so, we avoid Runge\u0026rsquo;s phenomenon associated with a uniform grid.\nImplementation using CompEcon #--------------------------------------------- # Julien Pascal # Solve the stochastic optimal growth problem # using the collocation method #--------------------------------------------- using QuantEcon using Optim using CompEcon using PyPlot using Interpolations type optGrowthCollocation w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool order_approximation::Int64 #number of element in the functional basis along each dimension functional_basis_type::String #type of functional basis fspace::Dict{Symbol,Any} #functional basis fnodes::Array{Float64,1} #collocation nodes residual::Array{Float64,1} #vector of residual. Should be close to zero a::Array{Float64,1} #polynomial coefficients w_func::Function end ##################################### # Function that finds a solution # to f(x) = 0 # using Broyden's \u0026quot;good\u0026quot; method # and a simple backstepping procedure as described # in Miranda and Fackler (2009) # # input : # -------- # * x0: initial guess for the root # * f: function in f(x) = 0 # * maxit: maximum number of iterations # * tol: tolerance level for the zero # * fjavinc: initial inverse of the jacobian. If not provided, then inverse of the # Jacobian is calculated by finite differences # * maxsteps: maximum number of backsteps # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian # # output : # -------- # * x: one zero of f # * it: number of iterations necessary to reached the solution # * fjacinv: pseudo jacobian at the last iteration # * fnorm: norm f(x) at the last iteration # ####################################### function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0)); maxsteps = 5, recaculateJacobian = 1) println(\u0026quot;a0 = $(x0)\u0026quot;) fnorm = tol*2 it2 = 0 #to re-initialize the jacobian ################################ #initialize guess for the matrix ################################ fjacinv_function = x-\u0026gt; Calculus.finite_difference_jacobian(f, x) #fjacinv_function = x -\u0026gt; ForwardDiff.gradient(f, x) # If the user do not provide an initial guess for the jacobian # One is calculated using finite differences. if fjacinv == eye(length(x0)) ################################################ # finite differences to approximate the Jacobian # at the initial value # this is slow. Seems to improve performances # when x0 is of high dimension. println(\u0026quot;Calculating the Jacobian by finite differences\u0026quot;) #@time fjacinv = Calculus.finite_difference_jacobian(f, x0) @time fjacinv = fjacinv_function(x0) println(\u0026quot;Inverting the Jacobian\u0026quot;) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end println(\u0026quot;Done\u0026quot;) else println(\u0026quot;Using User's input as a guess for the Jacobian.\u0026quot;) end fval = f(x0) for it=1:maxit it2 +=1 #every 30 iterations, reinitilize the jacobian if mod(it2, recaculateJacobian) == 0 println(\u0026quot;Re-calculating the Jacobian\u0026quot;) fjacinv = fjacinv_function(x0) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end end println(\u0026quot;it = $(it)\u0026quot;) fnorm = norm(fval) if fnorm \u0026lt; tol println(\u0026quot;fnorm = $(fnorm)\u0026quot;) return x0, it, fjacinv, fnorm end d = -(fjacinv*fval) fnormold = Inf ######################## # Backstepping procedure ######################## for backstep = 1:maxsteps if backstep \u0026gt; 1 println(\u0026quot;backstep = $(backstep-1)\u0026quot;) end fvalnew = f(x0 + d) fnormnew = norm(fvalnew) if fnormnew \u0026lt; fnorm break end if fnormold \u0026lt; fnormnew d=2*d break end fnormold = fnormnew d = d/2 end #################### #################### x0 = x0 + d fold = fval fval = f(x0) u = fjacinv*(fval - fold) #Update the pseudo Jacobian: fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u)) println(\u0026quot;a$(it) = $(x0)\u0026quot;) println(\u0026quot;fnorm = $(fnorm)\u0026quot;) if isnan.(x0) == trues(length(x0)) println(\u0026quot;Error. a$(it) = NaN for each component\u0026quot;) x0 = zeros(length(x0)) return x0, it, fjacinv, fnorm end end println(\u0026quot;In function find_broyden\\n\u0026quot;) println(\u0026quot;Maximum number of iterations reached.\\n\u0026quot;) println(\u0026quot;No convergence.\u0026quot;) println(\u0026quot;Returning fnorm = NaN as a solution\u0026quot;) fnorm = NaN return x0, maxit, fjacinv, fnorm end function optGrowthCollocation(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true, order_approximation = 40, functional_basis_type = \u0026quot;chebychev\u0026quot;, ) grid_y = collect(linspace(1e-5, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log.(c) # Production f(k) = k^α el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) lower_bound_support = minimum(grid_y) upper_bound_support = maximum(grid_y) n_functional_basis = [order_approximation] if functional_basis_type == \u0026quot;chebychev\u0026quot; fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support) elseif functional_basis_type == \u0026quot;splines\u0026quot; fspace = fundefn(:spli, n_functional_basis, lower_bound_support, upper_bound_support, 1) elseif functional_basis_type == \u0026quot;linear\u0026quot; fspace = fundefn(:lin, n_functional_basis, lower_bound_support, upper_bound_support) else error(\u0026quot;functional_basis_type has to be either chebychev, splines or linear.\u0026quot;) end fnodes = funnode(fspace)[1] residual = zeros(size(fnodes)[1]) a = ones(size(fnodes)[1]) w = ones(size(fnodes)[1]) Tw = ones(size(fnodes)[1]) σ = ones(size(fnodes)[1]) w_func = x-\u0026gt; x optGrowthCollocation( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, order_approximation, functional_basis_type, fspace, fnodes, residual, a, w_func ) end function residual!(model::optGrowthCollocation) model.w_func = y -\u0026gt; funeval(model.a, model.fspace, [y])[1][1] function objective(c, y) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end # Loop over nodes for i in 1:size(model.fnodes)[1] y = model.fnodes[i,1] res = optimize(c -\u0026gt; objective(c, y), 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = model.w_func(y) model.residual[i] = - model.w[i] + model.Tw[i] end end model = optGrowthCollocation(functional_basis_type = \u0026quot;chebychev\u0026quot;) residual!(model) function solve_optgrowth!(model::optGrowthCollocation; tol=1e-6, max_iter=500) # Initialize guess for coefficients # by giving the \u0026quot;right shape\u0026quot; # --------------------------------- function objective_initialize!(x, model) #update polynomial coeffficients model.a = copy(x) model.w_func = y -\u0026gt; funeval(model.a, model.fspace, [y])[1][1] return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1])) end minx, iterations, Jac0, fnorm = find_broyden(model.a, x -\u0026gt; objective_initialize!(x, model), max_iter, tol) # Solving the model by collocation # using the initial guess calculated above #----------------------------------------- function objective_residual!(x, model) #update polynomial coeffficients model.a = copy(x) #calculate residual residual!(model) return abs.(model.residual) end minx, iterations, Jac, fnorm = find_broyden(model.a, x -\u0026gt; objective_residual!(x, model), max_iter, tol) end #----------------------------------- # Solve by collocation #----------------------------------- @time solve_optgrowth!(model) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Approximated (collocation)\u0026quot;, linestyle = \u0026quot;--\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Exact value\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;collocation.png\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](-0.05, 0.05) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;error_collocation.png\u0026quot;)  Output The following figure shows the exact function and the one calculated using the collocation method. In terms of accuracy, both VFI and the collocation method generate reliable results.\nIn terms of speed, it turns out that the value function iteration implementation is much faster. One reason seems to be the efficiency associated with the package Interpolations: it is more than 20 times faster to evaluate $w$ using the package Interpolations rather than using the package CompEcon:\n#using Interpolations #-------------------- @time for i=1:1000000 model.w_func.(model.grid[1]) end #0.230861 seconds (2.00 M allocations: 30.518 MiB, 1.28% gc time) #using CompEcon #-------------- @time for i=1:1000000 model.w_func.(model.grid[1]) end # 4.998902 seconds (51.00 M allocations: 3.546 GiB, 13.39% gc time)  Implementation using ApproxFun Significant speed-gains can be obtained by using the package ApproxFun, as illustrated by the code below. Computing time is divided by approximately $5$ compared to the implementation using CompEcon. Yet, the value function iteration implementation is still the fastest one. One bottleneck seems to be the calculation of the Jacobian by finite differences when using Broyden\u0026rsquo;s method. It is likely that using automatic differentiation would further improve results.\n#--------------------------------------------- # Julien Pascal # Solve the stochastic optimal growth problem # using the collocation method # Implementation using ApproxFun #--------------------------------------------- using QuantEcon using Optim using CompEcon using PyPlot using Interpolations using FileIO using ApproxFun using ProfileView type optGrowthCollocation w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool order_approximation::Int64 #number of element in the functional basis along each dimension functional_basis_type::String #type of functional basis fspace::Dict{Symbol,Any} #functional basis fnodes::Array{Float64,1} #collocation nodes residual::Array{Float64,1} #vector of residual. Should be close to zero a::Array{Float64,1} #polynomial coefficients fApprox::ApproxFun.Fun{ApproxFun.Chebyshev{ApproxFun.Segment{Float64},Float64},Float64,Array{Float64,1}} w_func::Function tol::Float64 end ##################################### # Function that find a solution # to f(x) = 0 # using Broyden's \u0026quot;good\u0026quot; method # and simple backstepping procedure as described # in Miranda and Fackler (2009) # # input : # -------- # * x0: initial guess for the root # * f: function in f(x) = 0 # * maxit: maximum number of iterations # * tol: tolerance level for the zero # * fjavinc: initial inverse of the jacobian. If not provided, then inverse of the # Jacobian is calculated by finite differences # * maxsteps: maximum number of backsteps # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian # # output : # -------- # * x: one zero of f # * it: number of iterations necessary to reached the solution # * fjacinv: pseudo jacobian at the last iteration # * fnorm: norm f(x) at the last iteration # ####################################### function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0)); maxsteps = 5, recaculateJacobian = 1) println(\u0026quot;a0 = $(x0)\u0026quot;) fnorm = tol*2 it2 = 0 #to re-initialize the jacobian ################################ #initialize guess for the matrix ################################ # with Calculus #-------------- fjacinv_function = x-\u0026gt; Calculus.finite_difference_jacobian(f, x) # If the user do not provide an initial guess for the jacobian # One is calculated using finite differences. if fjacinv == eye(length(x0)) ################################################ # finite differences to approximate the Jacobian # at the initial value # this is slow. Seems to improve performances # when x0 is of high dimension. println(\u0026quot;Calculating the Jacobian by finite differences\u0026quot;) #@time fjacinv = Calculus.finite_difference_jacobian(f, x0) @time fjacinv = fjacinv_function(x0) println(\u0026quot;Inverting the Jacobian\u0026quot;) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end println(\u0026quot;Done\u0026quot;) else println(\u0026quot;Using User's input as a guess for the Jacobian.\u0026quot;) end fval = f(x0) for it=1:maxit it2 +=1 #every 30 iterations, reinitilize the jacobian if mod(it2, recaculateJacobian) == 0 println(\u0026quot;Re-calculating the Jacobian\u0026quot;) fjacinv = fjacinv_function(x0) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end end println(\u0026quot;it = $(it)\u0026quot;) fnorm = norm(fval) if fnorm \u0026lt; tol println(\u0026quot;fnorm = $(fnorm)\u0026quot;) return x0, it, fjacinv, fnorm end d = -(fjacinv*fval) fnormold = Inf ######################## # Backstepping procedure ######################## for backstep = 1:maxsteps if backstep \u0026gt; 1 println(\u0026quot;backstep = $(backstep-1)\u0026quot;) end fvalnew = f(x0 + d) fnormnew = norm(fvalnew) if fnormnew \u0026lt; fnorm break end if fnormold \u0026lt; fnormnew d=2*d break end fnormold = fnormnew d = d/2 end #################### #################### x0 = x0 + d fold = fval fval = f(x0) u = fjacinv*(fval - fold) #Update the pseudo Jacobian: fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u)) println(\u0026quot;a$(it) = $(x0)\u0026quot;) println(\u0026quot;fnorm = $(fnorm)\u0026quot;) if isnan.(x0) == trues(length(x0)) println(\u0026quot;Error. a$(it) = NaN for each component\u0026quot;) x0 = zeros(length(x0)) return x0, it, fjacinv, fnorm end end println(\u0026quot;In function find_broyden\\n\u0026quot;) println(\u0026quot;Maximum number of iterations reached.\\n\u0026quot;) println(\u0026quot;No convergence.\u0026quot;) println(\u0026quot;Returning fnorm = NaN as a solution\u0026quot;) fnorm = NaN return x0, maxit, fjacinv, fnorm end function optGrowthCollocation(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true, order_approximation = 40, functional_basis_type = \u0026quot;chebychev\u0026quot;, ) grid_y = collect(linspace(1e-4, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log.(c) # Production f(k) = k^α el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) lower_bound_support = minimum(grid_y) upper_bound_support = maximum(grid_y) n_functional_basis = [order_approximation] if functional_basis_type == \u0026quot;chebychev\u0026quot; fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support) else error(\u0026quot;functional_basis_type has to be \\\u0026quot;chebychev\\\u0026quot; \u0026quot;) end fnodes = funnode(fspace)[1] residual = zeros(size(fnodes)[1]) a = ones(size(fnodes)[1]) tol = 0.001 fApprox = (Fun(Chebyshev((minimum(grid_y))..(maximum(grid_y))), a)) #fApprox = (Fun(Chebyshev(0..maximum(model.grid)), a)) w_func = x-\u0026gt; fApprox(x) w = ones(size(fnodes)[1]) Tw = ones(size(fnodes)[1]) σ = ones(size(fnodes)[1]) optGrowthCollocation( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, order_approximation, functional_basis_type, fspace, fnodes, residual, a, fApprox, w_func, tol ) end function residual!(model::optGrowthCollocation) model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a)) model.w_func = x-\u0026gt; model.fApprox(x) function objective(c, y) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end # Loop over nodes for i in 1:size(model.fnodes)[1] y = model.fnodes[i,1] res = optimize(c -\u0026gt; objective(c, y), 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = model.w_func(y) model.residual[i] = - model.w[i] + model.Tw[i] end end function solve_optgrowth!(model::optGrowthCollocation; tol=1e-6, max_iter=500) # Initialize guess for coefficients # by giving the \u0026quot;right shape\u0026quot; # --------------------------------- function objective_initialize!(x, model) #update polynomial coeffficients model.a = copy(x) model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a)) model.w_func = x-\u0026gt; model.fApprox(x) return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1])) end minx, iterations, Jac0, fnorm = find_broyden(model.a, x -\u0026gt; objective_initialize!(x, model), max_iter, tol) # Solving the model by collocation # using the initial guess calculated above #----------------------------------------- function objective_residual!(x, model) #update polynomial coeffficients model.a = copy(x) #calculate residual residual!(model) return abs.(model.residual) end minx, iterations, Jac, fnorm = find_broyden(model.a, x -\u0026gt; objective_residual!(x, model), max_iter, tol, recaculateJacobian = 1) end #----------------------------------- # Solve by collocation #----------------------------------- model = optGrowthCollocation(functional_basis_type = \u0026quot;chebychev\u0026quot;) @time solve_optgrowth!(model) # 15.865923 seconds (329.12 M allocations: 4.977 GiB, 5.55% gc time) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;approximate value function\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;true value function\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](- 0.05, 0.05) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;)  ","date":1512651029,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512651029,"objectID":"616acd9b09eaac5b85a05cdc2a187df2","permalink":"https://julienpascal.github.io/post/collocation/","publishdate":"2017-12-07T13:50:29+01:00","relpermalink":"/post/collocation/","section":"post","summary":"A large class of economic models involves solving for functional equations of the form:\nA well known example is the stochastic optimal growth model. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period\u0026rsquo;s output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.","tags":[],"title":"Solving Bellman Equations by the Collocation Method","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" Dynare is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see Winberry and Ragot for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.\nA simple model The economy is composed of a representative agent who maximizes his expected discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,\u0026hellip;,\\infty$ $$ \\sum_{t=1}^{+\\infty}\\big(\\frac{1}{1+\\rho}\\big)^{t-1} E_t\\Big[log(C_t)-\\frac{L_t^{1+\\gamma}}{1+\\gamma}\\Big] $$\nsubject to the constraint\n$$ K_t = K_t{-_1} (1-\\delta) + w_t L_t + r_t K_t-_1 - C_t $$\nwhere\n $\\rho \\in (0,\\infty)$ is the rate of time preference $\\gamma \\in (0,\\infty)$ is a labor supply parameter $w_t$ is real wage $r_t$ is the real rental rate $K_t$ is capital at the end of the period $\\delta \\in (0,1)$ is the capital depreciation rate  The production function writes \\begin{equation} Y_t = A_t K_t-_1^\\alpha \\Big((1+g)^t \\Big)^{1-\\alpha} \\end{equation}\nwhere\n $g \\in (0,\\infty)$ is the growth rate of production $\\alpha$ and $\\beta$ are technology parameters $A_t$ is a technological shock that follows and $AR(1)$ process  \\begin{equation} \\log(A_t) = \\lambda log(A_t-_1) + e_t\\end{equation}\nwith $e_t$ an i.i.d zero-mean normally distributed error term with standard deviation $\\sigma_1$ and $\\lambda \\in (0,1)$ a parameter governing the persistence of shocks.\nFirst Order conditions The F.O.C.s of the (stationarized) model are\n$$ \\frac{1}{\\hat{C_t}} = \\frac{1}{1+\\rho} E_t \\Big( \\frac{r_t+_1 + 1 - \\delta}{\\hat{C}_t+_1 (1+g)} \\Big)$$\n$$ L_t^\\gamma = \\frac{\\hat{w}_t}{\\hat{C}_t}$$\n$$ r_t = \\alpha A_t \\Big(\\frac{\\hat{K}_t-_1}{1+g}\\Big)^{\\alpha-1}L_t^{1-\\alpha}$$\n$$ \\hat{w}_t = (1-\\alpha) A_t \\Big(\\frac{\\hat{K}_t-_1}{1+g}\\Big)^{\\alpha}L_t^{-\\alpha} $$\n$$ \\hat{K}_t + \\hat{C}_t = \\frac{\\hat{K}_t-_1}{1+g} (1-\\delta) + A_t \\Big( \\frac{\\hat{K}_t-_1}{1+g} \\Big)^{\\alpha} L_t^{1-\\alpha} $$\nwith $$ \\hat{C}_t = \\frac{C_t}{(1+g)^t}$$ $$ \\hat{K}_t = \\frac{K_t}{(1+g)^t}$$ $$ \\hat{w}_t = \\frac{w_t}{(1+g)^t}$$\nSolving and simulating the model in Dynare In Dynare, one has first to specify the endogenous variables (var), exogenous variables (varexo), and the parameters\nvar C K L w r A; varexo e; parameters rho delta gamma alpha lambda g; alpha = 0.33; delta = 0.1; rho = 0.03; lambda = 0.97; gamma = 0; g = 0.015;  In a second step, the F.O.C.s of the model has to be expressed using the command model\nmodel; 1/C=1/(1+rho)*(1/(C(+1)*(1+g)))*(r(+1)+1-delta); L^gamma = w/C; r = alpha*A*(K(-1)/(1+g))^(alpha-1)*L^(1-alpha); w = (1-alpha)*A*(K(-1)/(1+g))^alpha*L^(-alpha); K+C = (K(-1)/(1+g))*(1-delta) +A*(K(-1)/(1+g))^alpha*L^(1-alpha); log(A) = lambda*log(A(-1))+e; end;  The user must provide the analytical solution for the steady state of the model using the command steady_state_model. The command steady solves for the steady state values of the model\nsteady_state_model; A = 1; r = (1+g)*(1+rho)+delta-1; L = ((1-alpha)/(r/alpha-delta-g))*r/alpha; K = (1+g)*(r/alpha)^(1/(alpha-1))*L; C = (1-delta)*K/(1+g) +(K/(1+g))^alpha*L^(1-alpha)-K; w = C; end; steady;  The command shocks defines the type of shock to be simulated\nshocks; var e; stderr 0.01; end; check;  A first order expansion around the steady state is obtained using the command stoch_simul(order=1) This function computes impulse response functions (IRF) and returns various descriptive statistics (moments, variance decomposition, correlation and autocorrelation coefficients)\nThe IRF produced by Dynare should be pretty similar to the following graph: ","date":1501332263,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501332263,"objectID":"d5946fc40e4b8cec00c21670c8ac01d1","permalink":"https://julienpascal.github.io/post/rbc_dynare/","publishdate":"2017-07-29T13:44:23+01:00","relpermalink":"/post/rbc_dynare/","section":"post","summary":"Dynare is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see Winberry and Ragot for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.\nA simple model The economy is composed of a representative agent who maximizes his expected discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,\u0026hellip;,\\infty$ $$ \\sum_{t=1}^{+\\infty}\\big(\\frac{1}{1+\\rho}\\big)^{t-1} E_t\\Big[log(C_t)-\\frac{L_t^{1+\\gamma}}{1+\\gamma}\\Big] $$","tags":[],"title":"Solving a simple RBC model in Dynare","type":"post"},{"authors":["Christian Daude","Julien Pascal"],"categories":null,"content":"","date":1488898314,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488898314,"objectID":"48aece6ec07e069260731c2bbb63b8e5","permalink":"https://julienpascal.github.io/publication/efficiencycontestability/","publishdate":"2017-03-07T15:51:54+01:00","relpermalink":"/publication/efficiencycontestability/","section":"publication","summary":"This paper explores some of the potential determinants of efficiency and contestability in the banking systems of major emerging countries, using a sample of 24 countries over the period 2004 -2013. Efficiency is estimated using both stochastic frontier and data envelopment analyses. Market contestability is measured with the Panzar-Rosse H-statistic. Potential drivers of efficiency and market contestability are then discussed.","tags":["Banking","Efficiency","Competition","Emerging markets"],"title":"Efficiency and contestability in emerging market banking systems","type":"publication"}]