[{"authors":["admin"],"categories":null,"content":"I am currently a PhD candidate in Economics at Sciences Po - LIEPP under the supervision of Jean-Marc Robin. My thesis committee also includes Xavier Ragot and Florian Oswald.\nI am interested in how heterogeneity matters in frictional markets, especially in the labor and housing markets.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://julienpascal.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am currently a PhD candidate in Economics at Sciences Po - LIEPP under the supervision of Jean-Marc Robin. My thesis committee also includes Xavier Ragot and Florian Oswald.\nI am interested in how heterogeneity matters in frictional markets, especially in the labor and housing markets.","tags":null,"title":"Julien Pascal","type":"author"},{"authors":[],"categories":[],"content":" For a specific project on the housing market (here), I had to analyze thousands of photos. To do that, I used a convolutional neural network (CNN), which is a fancy name for a complicated function that can be \u0026ldquo;trained\u0026rdquo; to recognize patterns in images. In this blog post, I would like to introduce the \u0026ldquo;Hello World\u0026rdquo; of computer vision and CNN: the classification of hand-written digits from the MNIST dataset. There are thousands of tutorials on the same topic using Python freely available on the Internet. Instead, let\u0026rsquo;s use Julia and the package Flux.jl. Why? Because Julia is fast, and if you have millions of images to analyze, the speed up could be substantial compared to Python. The Jupyter notebook used to generate this post can be found here.\nData The MNIST dataset contains images of hand-written digits (0 to 9) in grayscale and that are nicely centered. Each pixel is represented by a number in between 0 (black) and 255 (white). Each image is 28 by 28 pixels. One way to represent an image is to see it as a 1d-column vector of 28*28 = 784 pixels. However, this representation ignores the \u0026ldquo;structure\u0026rdquo; of an image: pixels that are close to each others are informative on the digit we are trying to identify. A CNN is a good tool to keep the spatial structure of an image, while avoiding issues linked to the curse of dimensionality: images are noisy and high-dimensional input data.\nA crash course on CNN Two of the key ingredients of a CNN are a convolutional layer (hence the name) and a maxpool layer.\nConvolutional layer A convolutional layer applies a stencil to each point. The output of a convolutional layer is an \u0026ldquo;image\u0026rdquo; of lower dimension, that is informative on some features of the input image (shapes, edges, etc.). The figure below shows how a convolutional layer works:\nsource:https://mitmath.github.io/18337/lecture14/pdes_and_convolutions\nMaxpool layer A maxpool layer is a stencil that selects the maximum value within a square. Below is an illustration of a maxpool layer applied to a $ 4 \\times 4$ image:\nsource:https://mauriciocodesso.com/post/convolution-neural-network/\nStride and Padding When building a CNN, one must specify two hyper parameters: stride and padding\n When the stride is equal to 1, we move the filters one pixel at a time. When stride is equal to 2, we move the filters two pixel at a time, etc.\n Padding refers to \u0026ldquo;adding zeroes\u0026rdquo; at the border of an image. Padding can be used to control the size of the output volume and helps in keeping information at the border of images\n  Below is an example of a $3 \\times 3$ filter applied to a $5 \\times 5$ input padded with a $1 \\times 1$ border of zeros using $2 \\times 2$ strides:\nsource: http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\nThe typical infrastructure of a CNN is first to apply a convolutional layer to the input image, then to use a maxpool layer, before using a fully-connected layer. Several \u0026ldquo;convolutional layer - maxpool layer\u0026rdquo; units can be stacked together before using a fully-connected (FC) layer. Note that an activation layer (often ReLU) is generally inserted between the the convolutional and the maxpool layer.\nsource: https://towardsdatascience.com/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69\nUsing Flux.jl Flux.jl is a leading machine learning package in the Julia ecosystem. In what follows, we load both the train and the test samples of the MNIST dataset. The train sample is a set of images used to fine-tune the parameters of the CNN, while the test sample contains images used to check that we did not overfit the train sample. A smoking gun for overfitting is when the accuracy in the train sample is much better than the accuracy using images from the test sample.\nusing Flux, Flux.Data.MNIST, Statistics using Flux: onehotbatch, onecold, crossentropy, throttle using Base.Iterators: repeated, partition using Printf, BSON using ImageView # Load labels and images from Flux.Data.MNIST # Train set: images used to estimate the CNN train_labels = MNIST.labels(:train) train_imgs = MNIST.images(:train); # Test set: images used to see how well the CNN perform \u0026quot;out-of-the-sample\u0026quot; test_imgs = MNIST.images(:test) test_labels = MNIST.labels(:test) print(\u0026quot;Images in the train set: $(size(train_imgs))\u0026quot;) print(\u0026quot;Images in the test set: $(size(test_imgs))\u0026quot;) # Visualization of one digit NROWS, NCOLS = 28, 28 a = reshape(train_imgs[1], NROWS, NCOLS)  Images in the train set: (60000,)Images in the test set: (10000,)  CNN architecture Our CNN has the usual CONV-\u0026gt;ReLU-\u0026gt;MaxPool components, before using a FC layer. We use a $1 \\times 1$ padding and a stride of $1$ (the default value). The size of input is gradually reduced by using $2 \\times 2$ maxpool layers. The default activation in Flux.jl is the function is $ x-\u0026gt;x $. Here, we use the Rectified Linear Unit function (ReLU) instead:\nmodel = Chain( # First convolution, operating upon a 28x28 image Conv((3, 3), 1=\u0026gt;16, pad=(1,1), relu), MaxPool((2,2)), #maxpooling # Second convolution, operating upon a 14x14 image Conv((3, 3), 16=\u0026gt;32, pad=(1,1), relu), MaxPool((2,2)), #maxpooling # Third convolution, operating upon a 7x7 image Conv((3, 3), 32=\u0026gt;32,pad=(1,1), relu), MaxPool((2,2)), # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N) # which is where we get the 288 in the `Dense` layer below: x -\u0026gt; reshape(x, :, size(x, 4)), Dense(288, 10), # Softmax to get probabilities softmax, )  The ReLU activation function is a piece-wise linear function. In the “ImageNet Classification with Deep Convolutional Neural Networks\u0026rdquo; paper by Krizhevsky and coauthors, the authors write:\n we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units.\n The ReLU activation function also helps in reducing the practical issues caused by the vanishing gradient problem. That is, the failure of the minizimation algorithm used to find the parameters of our CNN. Below is a plot of the ReLU activation function:\nxgrid = collect(range(-1, 1, length=100)) plot(xgrid, NNlib.relu.(xgrid), label = \u0026quot;relu(x)\u0026quot;, title=\u0026quot;ReLU activation function\u0026quot;, xlabel=\u0026quot;x\u0026quot;)  Training Batching The batch size is a parameter that tells us how many images the network will \u0026ldquo;see\u0026rdquo; at once when \u0026ldquo;training\u0026rdquo;. In technical terms, when performing gradient descent, we don\u0026rsquo;t use all the information at once (because of memory limitations and because it is not necessarily efficient). The following function generates \u0026ldquo;batches\u0026rdquo; of images:\n# Bundle images together with labels and group into minibatchess function make_minibatch(X, Y, idxs) X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs)) for i in 1:length(idxs) X_batch[:, :, :, i] = Float32.(X[idxs[i]]) end Y_batch = onehotbatch(Y[idxs], 0:9) return (X_batch, Y_batch) end # The CNN only \u0026quot;sees\u0026quot; 128 images at each training cycle: batch_size = 128 mb_idxs = partition(1:length(train_imgs), batch_size) # train set in the form of batches train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]; # train set in one-go: used to calculate accuracy with the train set train_set_full = make_minibatch(train_imgs, train_labels, 1:length(train_imgs)); # test set: to check we do not overfit the train data: test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs));  Loss function and minimization For the CNN to \u0026ldquo;learn\u0026rdquo; anything at all, it must have a notion of \u0026ldquo;wrong\u0026rdquo; or \u0026ldquo;right\u0026rdquo;. The loss function does exactly that, by quantifying how well the model performs at recognizing digits. When the output is a probability, the cross entropy loss function is appropriate. The final step is to select an algorithm to minimize the loss function. Here, let\u0026rsquo;s select the ADAM algorithm, which I understand as some sort of Stochastic Gradient Descent with momentum and adaptive learning rate:\n# `loss()` calculates the crossentropy loss between our prediction `y_hat` # We augment the data a bit, adding gaussian random noise to our image to make it more robust. function loss(x, y) # Add some noise to the image # we reduce the risk of overfitting the train sample by doing so: x_aug = x .+ 0.1f0*gpu(randn(eltype(x), size(x))) y_hat = model(x_aug) return crossentropy(y_hat, y) end accuracy(x, y) = mean(onecold(model(x)) .== onecold(y)) # ADAM optimizer opt = ADAM(0.001);  This block \u0026ldquo;train\u0026rdquo; (fine-tune the CNN parameter values) the model until a pre-determined accuracy level is reached:\nbest_acc = 0.0 last_improvement = 0 accuracy_target = 0.97 #Set an accuracy target. When reached, we stop training. max_epochs = 100 #Maximum for epoch_idx in 1:100 global best_acc, last_improvement # Train for a single epoch Flux.train!(loss, params(model), train_set, opt) # Calculate accuracy: acc = accuracy(train_set_full...) @info(@sprintf(\u0026quot;[%d]: Train accuracy: %.4f\u0026quot;, epoch_idx, acc)) # Calculate accuracy: acc = accuracy(test_set...) @info(@sprintf(\u0026quot;[%d]: Test accuracy: %.4f\u0026quot;, epoch_idx, acc)) # If our accuracy is good enough, quit out. if acc \u0026gt;= accuracy_target @info(\u0026quot; -\u0026gt; Early-exiting: We reached our target accuracy of $(accuracy_target*100)%\u0026quot;) break end if epoch_idx - last_improvement \u0026gt;= 10 @warn(\u0026quot; -\u0026gt; We're calling this converged.\u0026quot;) break end end  ┌ Info: [1]: Train accuracy: 0.9579 └ @ Main In[14]:12 ┌ Info: [1]: Test accuracy: 0.9605 └ @ Main In[14]:16 ┌ Info: [2]: Train accuracy: 0.9749 └ @ Main In[14]:12 ┌ Info: [2]: Test accuracy: 0.9756 └ @ Main In[14]:16 ┌ Info: -\u0026gt; Early-exiting: We reached our target accuracy of 97.0% └ @ Main In[14]:20  Predictions Once the model is trained, predicted values are easily obtained as follows:\n# Get predictions and convert data to Array: pred = Tracker.data(model(test_set[1])); # Show the first 5 predictions # One column is an image # Each row corresponds to the probability of a digit pred[:,1:5] # Function to get the row index of the max value: f1(x) = getindex.(argmax(x, dims=1), 1) # Final predicted value is the one with the maximum probability: pred = f1(pred) .- 1; #minus 1 because the first element is 0  Let\u0026rsquo;s see how the model performs on the test set. Can the CNN recognize digits using images that were not used when training the model? As you can see below, our model does an amazing job at recognizing hand-written digits:\nprintln(\u0026quot;Predicted value = $(pred[1])\u0026quot;) a = reshape(test_imgs[1], NROWS, NCOLS)  Predicted value = 7  println(\u0026quot;Predicted value = $(pred[2])\u0026quot;) a = reshape(test_imgs[2], NROWS, NCOLS)  Predicted value = 2  println(\u0026quot;Predicted value = $(pred[3])\u0026quot;) a = reshape(test_imgs[3], NROWS, NCOLS)  Predicted value = 1  Accuracy checks We now have a model that seems to do quite a good job in recognizing digits. But can we improve it? If yes, how? To improve our model, we first need to identify when and why it fails.\nConfusion matrix To do that, a useful reporting tool is a confusion matrix. Each row of a confusion matrix shows instances of the true value, while each column displays instances of the predicted value. Ideally, we would like our model to perfectly predict the outcome. With a perfect model, all instances would be located on the diagonal elements of the confusion matrix.\nThe last time I checked, Flux.jl did not have an in-built function to calculate confusion matrices. Fortunately, an implementation is available in the package MLBase. The next block of code calculates the confusion matrix and displays it. Most of instances are on located on the diagonal, which is not a surprise given that the accuracy rate for our model is more than $97.0\\%$\nusing MLBase # Adding 1 to outcome because the index 0 in arrays does not exist in Julia Cm = confusmat(10, test_labels .+ 1, vec(pred) .+ 1)  10×10 Array{Int64,2}: 968 1 1 0 0 4 3 1 2 0 0 1128 3 0 0 0 1 0 3 0 3 5 1003 6 1 1 0 6 7 0 0 0 1 992 0 10 0 2 4 1 0 1 2 0 972 0 0 1 1 5 1 0 1 4 0 883 1 1 1 0 1 4 0 0 1 13 936 0 3 0 1 7 10 5 0 1 0 986 3 15 2 0 4 6 4 8 2 4 942 2 4 4 0 7 7 10 0 8 2 967  # Normalize output: Cm = Cm ./ sum(Cm, dims=2) # Labels xs = [string(i) for i = 0:9] heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma)  To visualize where our model makes mistakes, one can use the optional argument clim, to put an upper bound on the underlying colormap. For instance, the next plot shows that our model has troubles differencing 7 and 2 or 8 and 2.\n# Limits to colormap, so we can see where errors are located: xs = [string(i) for i = 0:9] heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma, clim=(0., 0.01))  Error Analysis The next block of code displays digits for which our CNN failed:\n# indices for errors: using ImageView, Gtk.ShortNames mistakes = test_labels .!= vec(pred) max_images = 5 grid, frames, canvases = canvasgrid((1,max_images)); # 1 row  k=0#counter for mistakes for (j, i) in enumerate(mistakes) if i == true k+=1 # a false value has been found println(\u0026quot;Predicted value = $(pred[j])\u0026quot;) println(\u0026quot;True value = $(test_labels[j])\u0026quot;) imshow(canvases[1,k], test_imgs[j]) idx = ImageView.annotate!(guidict, AnnotationText(0, 0, \u0026quot;$(pred[j])\u0026quot;, color=RGB(0,0,0), fontsize=3)) end if k \u0026gt;= max_images break end end win = Window(grid); Gtk.showall(win);  Predicted value = 5 True value = 9 Predicted value = 5 True value = 6 Predicted value = 4 True value = 8 Predicted value = 3 True value = 2 Predicted value = 7 True value = 2  While it seems obvious that the two digits starting from the left are a 9 and a 6, the remaining 3 elements are not trivial. The 8 in the middle could be easily confused with something else and the two remaining digits are weirdly shaped.\nConclusion When dealing with images, a convolutional neural network generally does an amazing job at recognizing patterns. This blog post was a non-technical introduction to the topic. While Python is the tool of predilection in machine learning (Keras, TensorFlow, etc.), my guess is that Julia will get increasingly popular because Julia is both easy to use and fast.\nReferences  This blog post is heavily based on this Flux.jl tutorial: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl On the links between CNN and PDEs: https://mitmath.github.io/18337/lecture14/pdes_and_convolutions A full course on CNN. Most of the content is available online: http://cs231n.github.io/convolutional-networks/  ","date":1575568402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575568402,"objectID":"ef8aeccdbe56db07f0af0a93a9e02c54","permalink":"https://julienpascal.github.io/post/cnn/","publishdate":"2019-12-05T18:53:22+01:00","relpermalink":"/post/cnn/","section":"post","summary":"For a specific project on the housing market (here), I had to analyze thousands of photos. To do that, I used a convolutional neural network (CNN), which is a fancy name for a complicated function that can be \u0026ldquo;trained\u0026rdquo; to recognize patterns in images. In this blog post, I would like to introduce the \u0026ldquo;Hello World\u0026rdquo; of computer vision and CNN: the classification of hand-written digits from the MNIST dataset.","tags":["Machine Learning","Computer Vision"],"title":"A Primer on Computer Vision","type":"post"},{"authors":[],"categories":[],"content":" The logistic model (also called logit model) is a natural candidate when one is interested in a binary outcome. For instance, a researcher might be interested in knowing what makes a politician successful or not. For the purpose of this blog post, \u0026ldquo;success\u0026rdquo; means the probability of winning an election. In that case, it would be sub-optimal to use a linear regression model to see what factors are associated with successful politicians, as the outcome variable is binary (a politician either wins or loses an election). The linear model is built around the idea that the outcome variable is continuous.\nWhat if the statistician tries to identify what factors are influencing the probability of winning? This strategy naturally lends itself to using a logistic model (or a probit). In this blog post, I derive the logistic model from scratch and show how one can estimate its parameters using gradient descent or Newton-Raphson algorithms. I also use data on NBA players to see what factors are influencing the success of a shot. The GitHub repository for this post can be found here.\nThe logistic model The outcome variable $y_i$ is either $1$ (\u0026ldquo;winning\u0026rdquo;) or $0$ (\u0026ldquo;losing\u0026rdquo;). The logistic model makes the assumption that the probability of winning is given by the logistic function :\n$$ f(y_i | x_{i}, \\theta_{i}) = \\sigma(x_{i} \u0026lsquo;\\theta)$$\nwith $\\sigma(v) = \\frac{exp(v)}{1+exp(v)}$\nThe probability of losing is 1 minus the probability of wining:\n$$ f(y_i | x_{i}, \\theta) = 1 - \\sigma(x_{i} \u0026lsquo;\\theta)$$\nA latent variable formulation A powerful way of interpreting the logistic model is to see it as the outcome of a latent variable model. An unobservable latent variable $z_{i}$ depends linearly on $x_{i}$ plus a noise term $\\varepsilon_{i}$:\n$$ z_{i} = x_{i} \u0026lsquo;\\theta + \\varepsilon_{i} $$\nWe only observe $y_i$, which is equal to 1 when $z_{i}$ is strictly positive, and 0 otherwise. If the error term is distributed according to the logistic distribution, we end up with the logistic model described above. If the error term is normally distributed, the model is a probit model. To see that, simply express the probability of the latent variable to be bigger than 0:\n$$ f(y_i | x_{i}, \\theta_{i}) = P( x_{i} \u0026lsquo;\\theta + \\varepsilon_{i} \u0026gt; 0) $$ $$ = 1 - P( x_{i} \u0026lsquo;\\theta + \\varepsilon_{i} \\leq 0) $$ $$ = 1 - P(\\varepsilon_{i} \\leq - x_{i} \u0026lsquo;\\theta ) $$ $$ = 1 - P(\\varepsilon_{i} \\leq - x_{i} \u0026lsquo;\\theta ) $$ $$ = \\frac{exp(x_{i} \u0026lsquo;\\theta )}{1+exp(x_{i} \u0026lsquo;\\theta )} $$\nwhere the last line comes from using the expression for the cdf of the logistic distribution with zero mean and scale parameter equal to 1.\nInterpretation of coefficients How can we read the coefficients from a logistic model? The marginal effect of a change in $x_{ij}$ (the $jth$ component of $x_i$) on the probability that $y_i = 1$ is given by:\n$$ \\frac{\\partial f(y_i | x_{i}, \\theta)}{\\partial x_{ij}} = \\sigma(x_{i} \u0026lsquo;\\theta)(1-\\sigma(x_{i} \u0026lsquo;\\theta))\\theta_j$$\nA first observation is that the marginal effect depends on $x_i$, unlike in the linear regression model. A second observation is that the first two terms are always positive, so we do have that the interpretation that if $\\theta_j$ is positive, an increase in the $jth$ component of $x_i$ leads to a bigger probability of obtaining a success (holding everything else constant).\nAnother way to read the results from a logistic model is to realize that it implies that the log of odd ratio is linear:\n$$ log\\Big(\\frac{f(y_i | x_{i}, \\theta)}{1 - f(y_i | x_{i}, \\theta)}\\Big) = x_{i} \u0026lsquo;\\theta$$\nGoing back to what makes a politician successful in an election, if the coefficient $\\theta_j$ is equal to 0.1, it means that a one unit increase in $x_{ij}$ rises the relative probability of winning an election by approximately $10\\%$.\nLog-likelihood function To predict who is going to win the next elections, one must estimate the value of $\\theta$ using the information contained in the sample $(y_i, x_i)_{i=1}^{N}$. One \u0026ldquo;natural\u0026rdquo; criterion is to find the value for $\\theta$ that maximizes the probability of observing the sample. This procedure is called Maximum likelihood estimation. Let us assume that sample is i.i.d. If the i.i.d assumption holds, the probability of observing the sample $(y_i, x_i)_{i=1}^{N}$ is the product of the probability of observing each observation. Instead of maximizing the likelihood, it is more convenient to maximize the log-likelihood, which transforms the product of probabilities into a sum:\n$$ L((y_i, x_i)_{i=1}^{N};\\theta) = log( \\prod_{i=1}^{N}f(y_i | x_{i}, \\theta)) = \\sum_{i=1}^{N} log(f(y_i | x_{i}, \\theta_{i}))$$\nThe probability of observing $y_i$ can compactly be written as\n$$ f(y_i | x_{i}, \\theta_{i}) = \\sigma(x_{i} \u0026lsquo;\\theta)^{y_i}(1 - \\sigma(x_{i} \u0026lsquo;\\theta))^{1 - y_i} $$\nHence, the log-likelihood function writes:\n$$L((y_i, x_i)_{i=1}^{N};\\theta) = \\sum_{i=1}^{N} y_i log(\\sigma(x_{i} \u0026lsquo;\\theta)) + (1 - y_i)log(1 - \\sigma(x_{i} \u0026lsquo;\\theta))$$\nMaximum Likelihood Estimation Taking the derivative of $f(y_i | x_{i}, \\theta)$ with respect to the parameter $\\theta$ gives:\n$$ f_{\\theta}(y_i | x_{i}, \\theta) = [y_i - \\sigma(x_{i} \u0026lsquo;\\theta)] x_{i} $$\nand the derivative of the log-likelihood function with respect to $\\theta$ is:\n$$ L_{\\theta}((y_i, x_i)_{i=1}^{N};\\theta) = \\sum_{i=1}^{N}[y_i - \\sigma(x_{i} \u0026lsquo;\\theta)] x_{i} $$\nGradient descent To make the link with this blog post, we can use gradient descent to find the MLE estimate:\n$$ \\theta_{i+1} = \\theta_{i} - \\gamma \\Big(- L_{\\theta}((y_i, x_i)_{i=1}^{N};\\theta_{i}) \\Big)$$\nThe gradient descent algorithm is an iterative procedure to find a minimizer of a function. At each step, the algorithm takes a step of length $\\gamma$ towards the direction of steepest descent. Note that I reformulated the problem of finding the maximum of a function $f$ (the log-likelihood) as the problem of finding the minimum of $-f$.\nNewton–Raphson method Roughly speaking, the Newton-Raphson method is a \u0026ldquo;smart\u0026rdquo; gradient descent which uses the information contained in the Hessian of the log-likelihood $HL((y_i, x_i)_{i=1}^{N};\\theta_{i})$ (on top of the gradient) to make a right move toward the minimizer. This iterative algorithm proceeds as follows:\n$$ \\theta_{i+1} = \\theta_{i} - (HL((y_i, x_i)_{i=1}^{N};\\theta_{i}) )^{-1} \\Big(- L_{\\theta}((y_i, x_i)_{i=1}^{N};\\theta_{i}) \\Big)$$\nThe next plot shows how the Newton-Raphson method works for a one dimensional root-finding problem:\nsource: https://en.wikipedia.org/wiki/Newton%27s_method\nShould we use gradient descent or Newton-Raphson? I let the following extract from the Wikipedia article on Newton-Raphson speak for itself:\n Where applicable, Newton\u0026rsquo;s method converges much faster towards a local maximum or minimum than gradient descent. In fact, every local minimum has a neighborhood N such that, if we start with x0 ∈ N, Newton\u0026rsquo;s method with step size γ = 1 converges quadratically (if the Hessian is invertible and a Lipschitz continuous function of x in that neighborhood).\n For the logistic model, the Newton-Raphson algorithm is easily applicable because there exists a closed-form formula for the Hessian:\n$$ HL((y_i, x_i)_{i=1}^{N};\\theta_{i}) = \\sum_{i=1}^{N} - \\sigma(x_{i} \u0026lsquo;\\theta)[1 - \\sigma(x_{i} \u0026lsquo;\\theta)] x_{i} x_{i}\u0026lsquo;$$\nImplementation in Julia I. Working with simulated data Let\u0026rsquo;s first work with simulated data. Can we actually recover the true parameter values using a manual implementation of the logistic model?\nLet\u0026rsquo;s load a few dependencies:\nusing Distributions using Plots pyplot() using DataFrames using GLM using Optim using CSV using GLM  Let\u0026rsquo;s create the logistic function:\n# Logistic function for a scalar input: function sigma(x::Float64) exp(x)/(1.0 + exp(x)) end # Logistic function for a vector input: function sigma(x::Array{Float64,1}) exp.(x) ./ (1.0 .+ exp.(x)) end  Let\u0026rsquo;s create a function that calculates the likelihood:\nfunction log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1}) sum = 0.0 #Loop over individuals in the sample for i=1:size(X,1) sum += y[i]*log(sigma(transpose(X[i,:])*theta)) + (1.0 - y[i])*log(1.0 - sigma(transpose(X[i,:])*theta)) end return sum end  Let\u0026rsquo;s create a function that returns the derivative of the log-likelihood of the sample, which we need for the gradient descent algorithm:\n# Function to calculate the gradient of the log-likelihood of the sample: function derivative_log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1}) sum = zeros(size(X,2)) #Loop over individuals in the sample for i=1:size(X,1) sum .+= (y[i] - sigma(transpose(X[i,:])*theta))*X[i,:] end return sum end  Let\u0026rsquo;s create a function that returns the Hessian of the log-likelihood of the sample, which we need for the Newthon-Raphson algorithm:\n# Function to calculate the hessian of the log-likelihood of the sample: function hessian_log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1}) hessian = zeros(size(X,2), size(X,2)) #Loop over individuals in the sample for i=1:size(X,1) hessian .+= - sigma(transpose(X[i,:])*theta)*(1.0 - sigma(transpose(X[i,:])*theta))*(X[i,:]*transpose(X[i,:])) end return hessian end  Let\u0026rsquo;s simulate a sample of individuals:\n#Generation of a sample: #---------------------- N_individuals = 10000 #how many individuals in the sample? dim_X = 3 #How many dimensions for x d = Normal(0.0, 1.0) d_logistic = Logistic(0.0, 1.0) # Generate true parameter values: theta0 = [0.0; 1.0; 2.0];  # Generate X: X = rand(d, N_individuals, dim_X) # The first column is full one ones (to have a constant) X[:,1] = ones(N_individuals);  # Convert y to a binary outcome using the latent variabe representation: proba_success = X*theta0 .+ rand(d_logistic, N_individuals) y = ifelse.(proba_success .\u0026gt; 0.0, 1.0, 0.0);  p1 = histogram(proba_success, bins=20, normalize=true, title=\u0026quot;Pdf probability of success\u0026quot;, legend=false) p2 = histogram(y, title=\u0026quot;Nb of successes vs failures\u0026quot;, legend=false) plot(p1,p2)  Maximization with Optim As a first pass, we can maximize the log-likelihood using the package Optim. I use the the LBFGS algorithm:\ntheta_guess = ones(dim_X) @time res = optimize(theta -\u0026gt; - log_likelihood(y, X, theta), theta_guess, LBFGS())  0.409340 seconds (3.79 M allocations: 396.250 MiB, 15.73% gc time)\nWe successfully recover the true parameter values (see theta0):\nprint(\u0026quot;Estimate for theta using Optim is $(res.minimizer)\u0026quot;)  Estimate for theta using Optim is [-0.0312666, 0.996344, 1.96038]  Minimization with gradient descent Let\u0026rsquo;s implement the gradient descent algorithm within a function:\nfunction gradient_descent_probit(y, X , theta_initial::Array{Float64,1}; max_iter::Int64 = 1000, learning_rate::Float64 = 0.000001, tol::Float64=0.01) #initial value for theta: theta_old = theta_initial theta_new = similar(theta_old) #convergence reached? success_flag = 0 #Let's store the convergence history history= fill!(zeros(max_iter), NaN) for i=1:max_iter theta_new = theta_old + learning_rate*derivative_log_likelihood(y, X, theta_old) diff = maximum(abs, theta_new .- theta_old) history[i] = diff if diff \u0026lt; tol success_flag = 1 break end theta_old = theta_new end return theta_new, success_flag, history[isnan.(history) .== false] end  theta_guess = zeros(dim_X) @time theta, flag, history = gradient_descent_probit(y, X, theta_guess, max_iter=100000, learning_rate=0.0001, tol=0.00001);   0.252281 seconds (4.89 M allocations: 523.159 MiB, 29.08% gc time)  The following graph shows the error as a function of the number of iterations. After a few iterations of the gradient descent algorithm, the error is quite small.\nplot(history, label= \u0026quot;error\u0026quot;, title = \u0026quot;Convergence of the gradient descent algorithm\u0026quot;)  print(\u0026quot;Estimate for theta using gradient descent is $(theta)\u0026quot;)  Estimate for theta using gradient descent is [0.0581777, 1.00105, 1.97901]  Minimization with Newton-Raphson Let\u0026rsquo;s implement the Newton-Raphson algorithm within a function:\nfunction nr_probit(y, X , theta_initial::Array{Float64,1}; max_iter::Int64 = 1000, tol::Float64=0.01) #initial value for theta: theta_old = theta_initial theta_new = similar(theta_old) #convergence reached? success_flag = 0 #Let's store the convergence history history= fill!(zeros(max_iter), NaN) for i=1:max_iter theta_new = theta_old - inv(hessian_log_likelihood(y, X, theta_old))*derivative_log_likelihood(y, X, theta_old) diff = maximum(abs, theta_new .- theta_old) history[i] = diff if diff \u0026lt; tol success_flag = 1 break end theta_old = theta_new end return theta_new, success_flag, history[isnan.(history) .== false] end  The following graph shows that we find the minimizer in only 5 steps! The Newton-Raphson algorithm clearly outperforms gradient descent. Of course, everything works well because the problem is well-behaved and a nice formula for the Hessian is available.\ntheta_guess = ones(dim_X) @time theta, flag, history = nr_probit(y, X, theta_guess, max_iter=1000, tol=0.00001); plot(history, label= \u0026quot;error\u0026quot;, title = \u0026quot;Convergence of the gradient descent algorithm\u0026quot;)   0.037832 seconds (500.07 k allocations: 53.431 MiB, 35.44% gc time)  print(\u0026quot;Estimate for theta using Newton-Raphson is $(theta)\u0026quot;)  Estimate for theta using Newton-Raphson is [0.0581789, 1.00114, 1.97919]  Using GLM We can also use the package GLM to estimate the logistic model. We first need to put the data into a dataframe. In the glm() function, we should use the LogitLink()\ndf = DataFrame(X1=X[:,1],X2=X[:,2], X3=X[:,3], y=y); first(df,6)  X1X2X3yFloat64Float64Float64Float646 rows × 4 columns\n11.00.2153151.298171.021.0-0.07147490.5868861.031.00.0463648-0.1451161.041.01.950960.263491.051.0-0.1620811.348711.061.00.00214326-0.2718351.0 fittedmodel = glm(@formula(y ~ X2 + X3), df, Binomial(), LogitLink(), verbose=true);  print(\u0026quot;Estimate for theta using GLM is $(coef(fittedmodel))\u0026quot;)  Estimate for theta using GLM is [-0.0312666, 0.996344, 1.96038]  II. What makes a successful NBA player? For an example involving real data, I use the data set on NBA shots taken during the 2014-2015 season. It contains information on:\n who took the shot where on the floor was the shot taken from who was the nearest defender, how far away was the nearest defender time on the shot clock etc.  The data is available on Kaggle here\ndf_nba = CSV.read(\u0026quot;/home/julien/Documents/REPOSITORIES/LogisticRegression/data/shot_logs.csv\u0026quot;); names(df_nba)  The dataset is quite extensive. Let\u0026rsquo;s select whether or not the shot was successful, the shot clock, the shot distance, and the proximity with the closest defender:\ndf_nba = df_nba[[:SHOT_RESULT, :SHOT_CLOCK, :SHOT_DIST, :CLOSE_DEF_DIST]] # Drop rows with missings: df_nba = dropmissing(df_nba); # Drop rows with NaN: df_nba = df_nba[completecases(df_nba), :] # Convert SHOT_RESULT to a binary variable (1 for success, 0 for missed) df_nba[:, :SHOT_RESULT] = ifelse.(df_nba[:, :SHOT_RESULT] .== \u0026quot;made\u0026quot;, 1.0, 0.0); # Show the first few rows of df_nba: first(df_nba, 4)  SHOT_RESULTSHOT_CLOCKSHOT_DISTCLOSE_DEF_DISTFloat64Float64Float64Float644 rows × 4 columns\n11.010.87.71.320.03.428.26.130.010.317.23.440.010.93.71.1 Let\u0026rsquo;s first use GLM:\nfittedmodel = glm(@formula(SHOT_RESULT ~ SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST), df_nba, Binomial(), LogitLink(), verbose=true); fittedmodel  SHOT_RESULT ~ 1 + SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST Coefficients: ──────────────────────────────────────────────────────────────────────────────────── Estimate Std. Error z value Pr(\u0026gt;|z|) Lower 95% Upper 95% ──────────────────────────────────────────────────────────────────────────────────── (Intercept) -0.0575127 0.0181349 -3.17139 0.0015 -0.0930564 -0.021969 SHOT_CLOCK 0.0185198 0.00104899 17.6549 \u0026lt;1e-69 0.0164639 0.0205758 SHOT_DIST -0.059745 0.000858282 -69.61 \u0026lt;1e-99 -0.0614272 -0.0580628 CLOSE_DEF_DIST 0.108392 0.00279232 38.8179 \u0026lt;1e-99 0.102919 0.113865 ────────────────────────────────────────────────────────────────────────────────────  How can we interpret those results?\n time pressure makes NBA players more successful: the higher the shot clock, the more likely to score shots from further away are more likely to be missed the further away the closest defender is, the more likely the shot will be a success  Can we find similar results \u0026ldquo;manually\u0026rdquo;? The answer is yes. To see that, let\u0026rsquo;s first create the binary variable y and put the explanatory variables into X and then use Newton-Raphson:\ny = convert(Array, df_nba[:SHOT_RESULT]); X = convert(Matrix, df_nba[[:SHOT_CLOCK, :SHOT_DIST, :CLOSE_DEF_DIST]]) X = hcat(ones(size(X,1)), X);  theta_guess = zeros(size(X,2)) @time theta, flag, history = nr_probit(y, X, theta_guess, max_iter=1000, tol=0.0001); plot(history, label= \u0026quot;error\u0026quot;, title = \u0026quot;Convergence of the gradient descent algorithm\u0026quot;)   0.301748 seconds (4.90 M allocations: 568.272 MiB, 29.93% gc time)  Our implementation of the logistic model gives us parameter values that are almost identical to the ones we get using the package GLM:\nprint(\u0026quot;Estimate for theta using Optim is $(res.minimizer)\u0026quot;)  Estimate for theta using Optim is [-0.057513, 0.0185199, -0.0597451, 0.108392]  Conclusion The logistic model, often used in social sciences and in machine learning for classification purposes is a powerful tool. This blog post shows how the logistic model can be derived from first principles (latent variable interpretation) and how it can be implemented in just a few lines of codes. A few extensions to this blog post could be to calculate the ROC curve and to calculate the standard errors.\nReferences  https://rpubs.com/junworks/Understanding-Logistic-Regression-from-Scratch  ","date":1574445202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574445202,"objectID":"6412a3e2857df9de1a8a4bfeddb695a9","permalink":"https://julienpascal.github.io/post/logistic/","publishdate":"2019-11-22T18:53:22+01:00","relpermalink":"/post/logistic/","section":"post","summary":"The logistic model (also called logit model) is a natural candidate when one is interested in a binary outcome. For instance, a researcher might be interested in knowing what makes a politician successful or not. For the purpose of this blog post, \u0026ldquo;success\u0026rdquo; means the probability of winning an election. In that case, it would be sub-optimal to use a linear regression model to see what factors are associated with successful politicians, as the outcome variable is binary (a politician either wins or loses an election).","tags":["Econometrics","Classification"],"title":"Logistic Regression from Scratch","type":"post"},{"authors":null,"categories":null,"content":"Using a novel dataset on the rental housing market in the Paris area, I show that the rental housing market is well described by a directed search model. I develop a hedonic pricing model taking into consideration apartment features and subjective attractiveness using photos and computer vision techniques from the machine learning literature.\nTo quantify visual attractiveness, I use a convolutional neural network based on a paper by Talebi and Peyman (2018). To see how the CNN works, you can get an attractiveness score for your own image on this website: http://ratemyphoto.us-east-1.elasticbeanstalk.com/\n","date":1571677867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571677867,"objectID":"f4948a7bac4472267cf44a4edc9b3898","permalink":"https://julienpascal.github.io/project/rentalmarket/","publishdate":"2019-10-21T18:11:07+01:00","relpermalink":"/project/rentalmarket/","section":"project","summary":"I explore the rental housing market in the Paris area using a novel dataset.","tags":["Housing","Directed Search"],"title":"Rental Housing Market and Directed Search","type":"project"},{"authors":[],"categories":[],"content":" In my quest for the perfect tool for reproducible science, I thought that the silver bullet was to wrap your code in a neat library/package and make it available to the world. Yet, I was wrong. Docker is a much cooler and a much more effective way for sharing your work with a broad audience. This post is a 101 introduction to Docker. I describe what is Docker and show one simple application with a script in Julia.\nWhy Docker? Your script works locally, but not on your friend\u0026rsquo;s laptop because of dependency issues. Docker solves the dependency hell by giving you the opportunity to \u0026ldquo;ship\u0026rdquo; your application in a \u0026ldquo;container\u0026rdquo;. One can think of a \u0026ldquo;container\u0026rdquo; as some sort of lightweight virtual image. Some technical details can be found here and here. In a nutshell, if your application works on your local machine, Docker helps you to put your application inside an container. Once in a container, your application will run smoothly for the rest of the world.\nApplication in Julia The goal is to create a container with a simple script to calculate an approximation of π. Here I am making a copy-paste from this post, in which I calculated an approximation of π using Monte-Carlo. I create a folder julia-app, which contains 3 files (see the Github repository with the 3 files here)\njulia-app ├ app.jl ├ deps.jl └ Dockerfile  The file app.jl contains the application we want to containerize. The file deps.jl contains the list of libraries/packages that are used within app.jl. The file Dockerfile is a text document that contains instructions to build the container. Generally, a Dockerfile contains 4 types of instructions:\n FROM: specifies the \u0026ldquo;base image\u0026rdquo; we want to use within the container. In our case, we want to run an application with Julia. Luckily, we can pull a base image with Julia pre-installed on it using FROM julia:\u0026lt;julia-version-you-want\u0026gt; COPY: adds files to your container RUN: executes command(s) in a new layer and creates a new image. RUN is perfect for installing packages CMD: specifies what command to run within the container  #choose a base image FROM julia:1.0.3 # install julia dependencies COPY deps.jl /usr/src/app/ RUN julia /usr/src/app/deps.jl # copy files required for the app to run COPY app.jl /usr/src/app/ # run the application CMD [\u0026quot;julia\u0026quot;, \u0026quot;/usr/src/app/app.jl\u0026quot;]  To create the container, use the command docker build. You can give a name to your container using the --tag , -t option:\ndocker build julia-app -t \u0026lt;your-tag\u0026gt;  To run the container you have just created:\ndocker run --name julia-app \u0026lt;your-tag\u0026gt;  After a few seconds, you should see an approximation of π showing up in your terminal. Voilà, you have have just created your first container. The next step is to put your container on DockerHub. Here is a tutorial on how to do it.\nConclusion From an academic perspective, Docker solves the dependency hell and helps in producing reproducible research. I will try to use it more often for sharing my own research.\n","date":1571421202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571421202,"objectID":"2aabea37fabfe37f2b03331f183d091d","permalink":"https://julienpascal.github.io/post/docker/","publishdate":"2019-10-18T18:53:22+01:00","relpermalink":"/post/docker/","section":"post","summary":"In my quest for the perfect tool for reproducible science, I thought that the silver bullet was to wrap your code in a neat library/package and make it available to the world. Yet, I was wrong. Docker is a much cooler and a much more effective way for sharing your work with a broad audience. This post is a 101 introduction to Docker. I describe what is Docker and show one simple application with a script in Julia.","tags":[],"title":"Docker for Dummies","type":"post"},{"authors":[],"categories":[],"content":" Coming from an Economics/Econometrics background, I have always been a bit puzzled by the way several Machine Learning (ML) textbooks I have read solve the ordinary least squares model (OLS). When considering a linear model of the form:\n$$ y = X \\beta + e $$\nwith $e$ a zero-mean noise term, the closed-form solution associated with minimizing the mean square error is:\n$$ \\beta = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\nSeveral ML textbooks explain that a recursive algorithm (see below) may be used to solve for $\\beta$. Why not directly using the analytical formula to calculate an estimate of $\\beta$ ? While feasible with \u0026ldquo;small\u0026rdquo; datasets (not too many explanatory variables and/or observations), direct inversion of $X\u0026rsquo;X$ is not recommended when working with thousands of explanatory variables and/or billions of observations. The alternative is to use gradient descent, or better, stochastic gradient descent.\nIn this short post, I solve OLS the \u0026ldquo;machine-learning way\u0026rdquo;. That is, using (stochastic) gradient descent. The idea for gradient descent (GD) is quite intuitive. The gradient of $f$ at a given point tells us the direction of greatest increase for $f$ at this point. Hence, moving in the opposite direction (minus the gradient) is probably a good idea to find a local minimum. And indeed it is. The GD algorithm repetitively applies this procedure until a minimum (hopefully global) is found. Starting from an initial guess for $\\beta$, one updates the guess using the following formula:\n$$ \\beta_{n} = \\beta_{n-1} - \\alpha * grad_{n}(\\beta_{n-1},X,y) $$\nwhere $\\alpha$ is a small value (the \u0026ldquo;learning rate\u0026rdquo;) and $grad_{n}(\\beta_{n-1},X,y)$ the gradient of the mean square error (another loss function can be used) evaluated at $\\beta_{n-1}$ using the observations $X$ and $y$. Using the mean square error as a loss function generates a closed-form solution for the gradient:\n$$ grad_{n}(\\beta_{n-1},X,y) = (X\u0026rsquo;X)\\beta - X\u0026rsquo;y $$\nA refinement of GD, especially handy when dealing with a large dataset, is to use only a subset of the full sample when calculating the gradient:\n$$ \\beta_{n} = \\beta_{n-1} - \\alpha * grad_{n}(\\beta_{n-1},X_n,y_n) $$\nwhere $X_n$ and $y_n$ are a randomly selected sub-sample of $X$ and $y$. Stochastic Gradient Descent (SGD) reduces the computational burden associated with computing the gradient, while still having good convergence properties, as illustrated in the application below.\nImplementation in Julia Let\u0026rsquo;s first load packages and define parameters\nusing LinearAlgebra using Distributions using Plots using Distributions using Random  n_points=10000 dim_input=100 #dim of input, without the intercept dim_output=1 # Normal noise d = Normal() # True parameters beta = rand(d, dim_input + 1); # Noise e = rand(d, n_points); # Input data: X = rand(d, (n_points,dim_input)); # Add the intercept: X = hcat(ones(n_points),X); #Linear Model y = X*beta .+ e;  This function calculates an estimate of $\\beta$ using the analytical formula for OLS\n#OLS way function OLS_direct(X::Array, y::Vector) inv(transpose(X)*X)*transpose(X)*y end  OLS_direct (generic function with 1 method)\n@time beta_hat = OLS_direct(X, y);  0.611144 seconds (1.85 M allocations: 96.576 MiB, 13.64% gc time)\nWithout any major surprise, using the analytical solution works perfectly well, as illustrated in the following plot\nplot(beta, beta_hat, seriestype=:scatter, label=\u0026quot;OLS (Direct)\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (Direct)\u0026quot;)  Gradient Descent Now it\u0026rsquo;s time to solve OLS the machine learning way. I first define a function that calculates the gradient of the loss function, evaluated at the current guess using the full sample. Then, a second function applies the GD updating rule.\n#Calculate the gradient function grad_OLS!(G, beta_hat, X, y) G[:] = transpose(X)*X*beta_hat - transpose(X)*y end  grad_OLS! (generic function with 1 method)\n#Gradient descent way function OLS_gd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false) #initial guess beta_hat = zeros(size(X,2)) grad_n = zeros(size(X,2)) for epoch=1:epochs grad_OLS!(grad_n, beta_hat, X, y) beta_hat -= r*grad_n if verbose==true if mod(epoch, round(Int, epochs/10))==1 println(\u0026quot;MSE: $(mse(beta_hat, X, y))\u0026quot;) end end end return beta_hat end  OLS_gd (generic function with 1 method)\nAs illustrated below, after 20 iterations we are quite close to the true value. After 100 iterations, values obtained by GD are indistinguishable from the true values.\n@time beta_hat_gd_20 = OLS_gd(X,y, epochs=20); @time beta_hat_gd = OLS_gd(X,y, epochs=100); plot(beta, beta_hat_gd_20, seriestype=:scatter, label=\u0026quot;GD (20 iter.)\u0026quot;) plot!(beta, beta_hat_gd, seriestype=:scatter, label=\u0026quot;GD (100 iter.)\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (GD)\u0026quot;)  0.137474 seconds (81.55 k allocations: 5.814 MiB) 0.466605 seconds (714 allocations: 8.225 MiB)\nStochastic Gradient Descent One issue associated with plain vanilla GD is that computing the gradient might be slow. Let\u0026rsquo;s now randomly select only a fraction of the full sample every time we iterate. Here, I take only 10 percent of the full sample.\n#Gradient descent way function OLS_sgd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false, batchsizePer::Int64=10) #initial guess beta_hat = zeros(size(X,2)) grad_n = zeros(size(X,2)) #how many draws from the dataset? batchsize = round(Int, size(X,1)*(batchsizePer/100)) Xn = zeros(batchsize, size(X,2)) yn = zeros(batchsize) for epoch=1:epochs indices = shuffle(Vector(1:size(X,1))) Xn = X[indices[1:batchsize],:] yn = y[indices[1:batchsize]] grad_OLS!(grad_n, beta_hat, Xn, yn) #gradient descent: beta_hat -= r*grad_n if verbose==true if mod(epoch, round(Int, epochs/10))==1 println(\u0026quot;MSE: $(mse(beta_hat, X, y))\u0026quot;) end end end return beta_hat end  OLS_sgd (generic function with 1 method)\nThe following block of code shows that SGD achieves the same degree of accuracy, while being much faster than GD.\n@time beta_hat_gd = OLS_gd(X,y, epochs=200); @time beta_hat_sgd = OLS_sgd(X,y, epochs=200, batchsizePer=20); plot(beta, beta_hat_sgd, seriestype=:scatter, label=\u0026quot;SGD\u0026quot;) plot!(beta, beta_hat_gd, seriestype=:scatter, label=\u0026quot;GD\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (SGD)\u0026quot;)  0.894465 seconds (1.41 k allocations: 16.448 MiB, 0.42% gc time) 0.513217 seconds (338.19 k allocations: 382.550 MiB, 4.60% gc time)\nConclusion The OLS analytical formula is the gold standard to derive theoretical properties and is perfectly fine when working with reasonably-sized data. In a big data context, (stochastic) gradient descent is the way to go. SGD can be applied to a wide-range of minimization problems. In a machine-learning context, SGD is used to estimate (\u0026ldquo;train\u0026rdquo;) much more complicated models than the simple linear model presented here. In the Appendix below, I show how one can use SGD when no analytical solution for the gradient is available.\nAppendix GD without analytical solution for the gradient Let\u0026rsquo;s assume we don\u0026rsquo;t have a closed-form solution for the gradient. In this context, Julia\u0026rsquo;s automatic differentiation library ForwardDiff is a good choice to calculate the gradient. Below, I define the loss function (MSE), I obtain the gradient of the loss function using ForwardDiff and I apply the SGD algorithm.\nusing ForwardDiff  function mse(beta::Vector, X::Array, y::Vector) result = zero(eltype(y)) for i in 1:length(y) #sum squared errors result += (y[i] - dot(X[i,:],beta))^2 end return result end  mse (generic function with 1 method)\nfunction grad!(G, beta_hat, X, y) G[:] = ForwardDiff.gradient(x -\u0026gt; mse(x, X, y), beta_hat) end  grad! (generic function with 1 method)\n#Gradient descent way function OLS_gd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false) #initial guess beta_hat = zeros(size(X,2)) grad_n = zeros(size(X,2)) for epoch=1:epochs grad!(grad_n, beta_hat, X, y) beta_hat -= r*grad_n if verbose==true if mod(epoch, round(Int, epochs/10))==1 println(\u0026quot;MSE: $(mse(beta_hat, X, y))\u0026quot;) end end end return beta_hat end  OLS_gd (generic function with 1 method)  @time beta_hat_gd = OLS_gd(X,y, epochs=100); plot(beta, beta_hat_gd, seriestype=:scatter, label=\u0026quot;GD (100 iter.)\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (GD)\u0026quot;)  3.180767 seconds (9.71 M allocations: 7.547 GiB, 12.66% gc time)\n","date":1569779602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569779602,"objectID":"8f2150d74032f9f0ec38bea25a856bde","permalink":"https://julienpascal.github.io/post/ols_ml/","publishdate":"2019-09-29T18:53:22+01:00","relpermalink":"/post/ols_ml/","section":"post","summary":"Coming from an Economics/Econometrics background, I have always been a bit puzzled by the way several Machine Learning (ML) textbooks I have read solve the ordinary least squares model (OLS). When considering a linear model of the form:\n$$ y = X \\beta + e $$\nwith $e$ a zero-mean noise term, the closed-form solution associated with minimizing the mean square error is:\n$$ \\beta = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\nSeveral ML textbooks explain that a recursive algorithm (see below) may be used to solve for $\\beta$.","tags":[],"title":"OLS the Machine Learning Way","type":"post"},{"authors":[],"categories":[],"content":" Finding solutions to economic models with heterogeneity and aggregate uncertainty is a notoriously difficult task. It is an active area of research in Mathematics (see mean field games with aggregate uncertainty). Because such models naturally arise when considering economic situations, Economists have developed a battery of techniques to (numerically) solve them. In this post, I would like to describe a recent algorithm by Boppart, Krusell and Mitman (BKM) that is both fast and accurate. I will first describe the problem that Economists face when working with heterogeneous model with aggregate uncertainty, heuristically discuss the BKM algorithm (based on the presentation of Reiter (2018)) and show an application in Julia.\nBKM in a nutshell It is common to use Bellman\u0026rsquo;s principle of optimality to characterize solutions of a multi-stage decision processes. The principle of optimality leads to a solution in a recursive form $d_{t} = d(S_{t})$, where $S_t$ is a vector of state variables and $d(.)$ a policy function describing the optimal action of a decision-maker when faced with any given state.\nAn alternative representation of the problem is to consider a solution in the sequence form. At each step in time, a decision-maker observes a new realization of a random process $z_t$ and taking into account the full history of past shocks, the agent makes a new choice to maximize her expected discounted sum of returns: $d_t = d(z_t, z_{t-1}, z_{t-2}, \u0026hellip;)$.\nWhile most of the time the recursive form is a much more parsimonious approach, it fails when $S_t$ is infinite-dimensional. In models with heterogeneous agents (HA) and aggregate uncertainty, this is generally the case because the distribution of agents over certain variables will end up being in $S_t$. While this a problem with is the recursive approach (how can we discretize $S_t$ to put it on a computer?), the sequence form is immune to this problem. The BKM algorithm uses this insight, adding the assumption of linearity of $d(.)$ with respect to the aggregate state $z_t$:\n$$ d_t = z_t d(1, 0, 0, \u0026hellip;) + z_{t-1}d(0, 1, 0, \u0026hellip;) + z_{t-2}d(0, 0, 1, \u0026hellip;) + \u0026hellip; $$ $$ d_t = \\sum_{k=0}^{+\\infty} z_{t-k} d_{k} $$\nwhere $$ d_{1} = d(1,0,0,\u0026hellip;)$$ $$ d_{2} = d(0,1,0,\u0026hellip;)$$ $$ d_{3} = d(0,0,1,\u0026hellip;)$$\nThe series of $d_{k}$ describes the trajectory of the economy after an \u0026ldquo;MIT\u0026rdquo; shock (when the economy is hit by an unexpected single-period aggregate shock). If the linearity assumption holds, the evolution of equilibrium variables are simply a moving average of past shocks. We made progress because solving for the trajectory after an MIT shock is perfectly feasible in a RA model. As long as one can calculate the steady-state with no aggregate uncertainty and solve the deterministic perfect foresight path, BKM can be used.\nNumerical implementation Here is the implementation of the method using a toy example. I intentionally circumvent the problem of finding the perfect foresight transition path, which is (potentially) the complicated part of BKM. This is the example given in Reiter (2018): the exact model is the non-linear model $x_{t} = a x_{t-1} + b x_{t-1}^2 + z_{t}$\nusing Distributions using Plots  # Parameters max_iter=100 #number of iterations for the simulation a = 0.5 b = 0.1 sigma_shock=1.0 mu_shock=0. d = Normal(mu_shock, sigma_shock) # transition function function iter_x(x_min1::Float64, a::Float64, b::Float64) a*x_min1 + b*x_min1^2 end  iter_x (generic function with 1 method)  Impulse response function Let us assume that the economy is at the non-stochastic steady-state and a shock occurs:\n# We assume that after 100 periods, the economy is back to the steady-state max_iter_mit = 100 x_mit=zeros(max_iter_mit) # Initial shock z_t[1] = sigma_shock #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1];  I define two functions to calculate the moving average:\nfunction calculate_Xt(x_scaled::Array{Float64,1}, shocks::Array{Float64,1}, t::Int64, kk::Int64) output = 0. for k=1:kk output+=x_scaled[k]*shocks[t-k+1] end return output end function BPM_path!(XT::Array{Float64,1}, max_iter::Int64, x_scaled::Array{Float64,1}, shocks::Array{Float64,1}) for t=2:max_iter XT[t] = calculate_Xt(x_scaled, shocks, t, t) end end  BPM_path! (generic function with 1 method)  # Initialization XT = zeros(max_iter); # Series of shocks shocks_t = rand(d, max_iter).*0.5; # Solving using BKM: @time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t) # True value: x_true = zeros(max_iter) for i=2:max_iter x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1] end   0.025906 seconds (29.30 k allocations: 1.489 MiB, 26.80% gc time)  plot(XT[2:end], label=\u0026quot;x_t BKM\u0026quot;) plot!(x_true[2:end], label=\u0026quot;x_t true\u0026quot;) title!(\u0026quot;b=$(b)\u0026quot;)  The previous plot shows how the BKM algorithm approximates the true model and it does quite a good job. Of course, the more the model is linear with respect to $z_t$ (captured by the value of $b_2$), the better the approximation. To illustrate this idea, I use BKM on a perfectly linear model ($b=0$) and on a model with stronger non-linearities ($b=0.2$). As expected, the approximation is perfect when the model is linear and the approximation deteriorates when strong non-linearities are present.\nplot(p1,p2)  Conclusion The BKM algorithm is a new addition to the toolbox of methods to solve HA models. The sequence representation of the problem seems to be a fruitful area of research, as it has also been used in Le Grand and Ragot (2017) and (2019) to develop fast and reliable method to solve HA models. The recent contribution of Auclert at al (2019) also uses a similar approach.\nReferences  Achdou, Yves, et al. \u0026ldquo;Partial differential equation models in macroeconomics.\u0026rdquo; Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 372.2028 (2014): 20130397. Auclert, Adrien, et al. Using the Sequence-Space Jacobian to Solve and Estimate Heterogeneous-Agent Models. No. w26123. National Bureau of Economic Research, 2019. Boppart, Timo, Per Krusell, and Kurt Mitman. \u0026ldquo;Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative.\u0026rdquo; Journal of Economic Dynamics and Control 89 (2018): 68-92. Le Grand, François, and Xavier Ragot. \u0026ldquo;Optimal fiscal policy with heterogeneous agents and aggregate shocks.\u0026rdquo; Document de travail (2017). Le Grand, François, and Ragot, Xavier. \u0026ldquo;Managing Inequality over the Business Cycles: Optimal Policies with Heterogeneous Agents and Aggregate Shocks\u0026rdquo;. No. 1090. Society for Economic Dynamics, 2019. Reiter, Michael. \u0026ldquo;Comments on\u0026rdquo; Exploiting MIT Shocks in Heterogeneous-Agent Economies: The Impulse Response as a Numerical Derivative\u0026rdquo; by T. Boppart, P. Krusell and K. Mitman.\u0026rdquo; Journal of Economic Dynamics and Control 89 (2018): 93-99.  Appendix versioninfo()  Julia Version 1.0.3 Commit 099e826241 (2018-12-18 01:34 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.0 (ORCJIT, skylake)  ### To create plot 2 #### Linear Model # Parameters max_iter=100 #number of iterations for the simulation a = 0.5 b = 0.0 sigma_shock=1.0 mu_shock=0. d = Normal(mu_shock, sigma_shock) # IRF x_mit=zeros(max_iter_mit) # Initial shock z_t[1] = sigma_shock #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1]; # Initialization XT = zeros(max_iter); # Series of shocks shocks_t = rand(d, max_iter).*0.5; # Solving using BKM: @time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t) # True value: x_true = zeros(max_iter) for i=2:max_iter x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1] end p1 = plot(XT[2:end], label=\u0026quot;x_t BKM\u0026quot;) plot!(x_true[2:end], label=\u0026quot;x_t true\u0026quot;) title!(\u0026quot;b=$(b)\u0026quot;)  0.000009 seconds (4 allocations: 160 bytes)\n### Plot 2 #### Linear Model # Parameters max_iter=100 #number of iterations for the simulation a = 0.5 b = 0.2 sigma_shock=1.0 mu_shock=0. d = Normal(mu_shock, sigma_shock) # IRF x_mit=zeros(max_iter_mit) # Initial shock z_t[1] = sigma_shock #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1]; # Initialization XT = zeros(max_iter); # Series of shocks shocks_t = rand(d, max_iter).*0.5; # Solving using BKM: @time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t) # True value: x_true = zeros(max_iter) for i=2:max_iter x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1] end p2 = plot(XT[2:end], label=\u0026quot;x_t BKM\u0026quot;) plot!(x_true[2:end], label=\u0026quot;x_t true\u0026quot;) title!(\u0026quot;b=$(b)\u0026quot;)   0.000007 seconds (4 allocations: 160 bytes)  ","date":1568656402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568656402,"objectID":"fa002bc244a5c1ce4c671bc96e1fa411","permalink":"https://julienpascal.github.io/post/bkm/","publishdate":"2019-09-16T18:53:22+01:00","relpermalink":"/post/bkm/","section":"post","summary":"Finding solutions to economic models with heterogeneity and aggregate uncertainty is a notoriously difficult task. It is an active area of research in Mathematics (see mean field games with aggregate uncertainty). Because such models naturally arise when considering economic situations, Economists have developed a battery of techniques to (numerically) solve them. In this post, I would like to describe a recent algorithm by Boppart, Krusell and Mitman (BKM) that is both fast and accurate.","tags":[],"title":"The BKM Algorithm","type":"post"},{"authors":[],"categories":[],"content":" In a previous post, I introduced the logic for the Linear Time Iteration (LTI) method (Pontus Rendahl (2017)). Now it\u0026rsquo;s time to apply the technique to a \u0026ldquo;real\u0026rdquo; (yet simple) economic model: a stochastic growth model with endogenous labor supply. The implementation is in Julia and is based a Matlab code by Pontus Rendahl available here. We will use a three-step approach:\n [1] solve the non-stochastic steady-state of the model [2] differentiate the system around the non-stochastic steady-state to obtain a linear difference equation of the form $A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0$ [3] apply the LTI method to find the law of motion $x_{t} = F x_{t-1} + Q u_{t}$  Model We consider an economy populated by a representative household, deciding how much to work, save and consume at any given time. Output $y$ depends on the capital level $k$ (inherited from period $t-1$), on the number of hours worked $l$, and on the productivity level $z$:\n$$ y_t = z_t k_{t-1}^{\\alpha} l_{t}^{1 - \\alpha}$$\nBusiness cycles are driven by variations in productivity, that follows an AR(1) process, with $e_t$ a zero-mean stochastic variable:\n$$z_t = \\rho z_{t-1} + e_{t} $$\nCapital at the end of period $t$ is equal to investment plus the non-depreciated capital stock inherited from last period:\n$$ k_{t} = I_{t} + (1 - \\delta) k_{t-1} $$\nThe representative household enjoys consumption and dislikes providing labor:\n$$ U(c,l) = \\frac{C^{1-\\sigma}}{1-\\sigma} - \\frac{l^{1-\\eta}}{1-\\eta} $$\nEverything that is produced in the economy is either consumed or saved:\n$$ c_{t} + k_{t} = z_t k_{t-1}^{\\alpha} l_{t}^{1 - \\alpha} + (1 - \\delta)k_{t-1}$$\nThe optimal decision of the household is characterized by two equations:\n$$ c_{t}^{-\\sigma} = \\beta E_{t}(c_{t+1}^{-\\sigma}(1 - \\delta + \\alpha z_{t+1} k_{t}^{\\alpha -1} l_{t+1}^{1 - \\alpha} ) )$$\n$$ l_{t}^{-\\eta} = c_{t}^{-\\gamma}(1 - \\alpha) z_{t} k_{t-1}^\\alpha l_{t}^{-\\alpha} $$\nThe first one states the gain of raising consumption today by one unit has to be equal to the expected gain from saving one extra unit today and consuming it tomorrow (inter-temporal FOC). The second equation states that the marginal cost of working one extra hour today has to be equal to the marginal gain of that extra hour worked (intra-temporal FOC).\nSolving the steady-state Calculating the steady-state of the model is a root finding problem. Let\u0026rsquo;s use the package NLsolve:\nversioninfo()  Julia Version 1.0.3 Commit 099e826241 (2018-12-18 01:34 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.0 (ORCJIT, skylake)  # Declare parameters const alpha = 1/3; # Capital share of output const beta = 1.03^(-1/4); # Discount factor. const gamma = 2; # Coefficient of risk aversion const eta = 2; # Frisch elasticity of labor supply const delta = 0.025; # Depreciation rate of capital const rho = 0.9; # Persistence of TFP process.  using NLsolve # Let's define a function for each equation of the model at the steady-state function Ee(x::Array{Float64,1}) -x[1]^(-gamma) + beta*(1.0 + alpha*(x[2]^(alpha - 1.0))*(x[3]^(1.0 - alpha)) - delta)*(x[1]^(-gamma)) end function Rc(x::Array{Float64,1}) -x[1] - x[2] + (x[2]^(alpha))*(x[3]^(1.0 - alpha)) + (1.0 - delta)*x[2] end function Ls(x::Array{Float64,1}) (-x[1]^(-gamma))*(1.0 - alpha)*(x[2]^(alpha))*(x[3]^(-alpha)) + x[3]^(eta) end # The steady-state of the model is described by a system of three equations f! = function (dx,x) dx[1] = Ee(x) dx[2] = Rc(x) dx[3] = Ls(x) end res = nlsolve(f!,[1.0; 20; 0.7]) xss = res.zero;  css = xss[1]; kss = xss[2]; lss = xss[3]; # steady-state output and investment: yss = kss^(alpha)*lss^(1-alpha); Iss = kss-(1-delta)*kss; XSS = zeros(6) XSS[1]=yss XSS[2]=Iss XSS[3:5] = xss XSS[6]=1.0; print(\u0026quot;Steady-state value [css, kss, lss, yss, Iss, zss] = \u0026quot;, XSS)  Steady-state value [css, kss, lss, yss, Iss, zss] = [2.51213, 0.645783, 1.86634, 25.8313, 0.78341, 1.0]  Solving the stochastic model To find a solution to the stochastic model, let\u0026rsquo;s differentiate the system around the non-stochastic steady-state calculated above. Here, we will limit ourself to a first-order approximation since the goal is to obtain is a linear difference equation of the form $ A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0 $, for which LTI is applicable. What is the rationale for the linear approximation? Fist of all, notice that the model can be put in the form of:\n$$E_t(f(Y_t, \\sigma)) = 0 $$\nwhere $Y_t = [x_{t-1}, x_{t}, x_{t+1}]$ is $3n × 1$ vector containing endogenous and exogenous variables and $\\sigma$ is variable scaling the level of uncertainty in the model. For instance, if $v_{t}$ is a zero-mean normally distributed variable with variance $\\sigma^2$:\n$$z_t = \\rho z_{t-1} + \\sigma v_{t} $$\nIn the non-stochastic state, $\\sigma = 0$. Let\u0026rsquo;s take a first-order Taylor expansion around the non-stochastic steady-state:\n$$f(Y_t, \\sigma) \\approx f(Y_{SS}, 0) + \\frac{Df}{DY_t}|Y_{SS}(Y_t - Y_{SS}) + \\frac{Df}{D\\sigma}|\\sigma_{SS}(\\sigma - 0) = 0$$\nwhere $\\frac{Df}{DY_t}|Y_{SS}$ is the derivative of the vector-valued function $f$ with respect to the vector $Y_t$ evaluated at $Y_{SS}$\nUsing $f(Y_{SS}, 0) = 0$ and that the last term disappears when we take the expectation:\n$$E_t(f(Y_t, \\sigma)) \\approx E_t(\\frac{Df}{DY_t}|Y_{SS}(Y_t - Y_{SS})) = 0 $$\nDefining the matrices $A$, $B$ and $C$ such that $\\frac{Df}{DY_t}|Y_{SS} = [A; B; C]$ we obtain a system of the form:\n$A \\tilde{x}_{t-1} + B \\tilde{x}_{t} + C E_{t} [\\tilde{x}_{t+1}] = 0$\nwith $\\tilde{x}_{t} = x_{t} - x_{SS} $\nIn practical terms, obtaining a linear approximation around the non-stochastic steady-state is easily done using the package ForwardDiff\nusing ForwardDiff # Function defining the stochastic model # Each line is an equation # The input the vector x is [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] f! = (w, x) -\u0026gt; begin #naming the input variables: ym, y, yp, Im, I, Ip, cm, c, cp, km, k, kp, lm, l, lp, zm, z, zp = x w[1] = -y + z*km^(alpha)*l^(1.0 - alpha) w[2] = -I+k-(1.0-delta)*km w[3] = -c^(-gamma) + beta*(1+zp*alpha*k^(alpha-1)*lp^(1-alpha)-delta)*cp^(-gamma) w[4] = c + k - (z*km^(alpha)*l^(1.0-alpha)+(1.0-delta)*km) w[5] = c^(-gamma)*(1.0-alpha)*km^(alpha)*l^(-alpha)*z-l^(eta) w[6] = -z+zm*rho return nothing end f = x -\u0026gt; (w = fill(zero(promote_type(eltype(x), Float64)), 6); f!(w, x); return w) # At the steady-state, the function f should be zero: Xss = [yss yss yss Iss Iss Iss css css css kss kss kss lss lss lss 1 1 1]; #println(maximum(abs.(f(Xss)))) Jac = ForwardDiff.jacobian(f, Xss);  Collecting matrices A, B and C Having successfully obtained $\\frac{Df}{DY_t}|Y_{SS}$, we now need to collect the right elements to form the matrices A, B and C in order to apply the LTI algorithm. It is mainly a matter of bookkeeping:\n# A is derivative of the function \u0026quot;system\u0026quot; f(Vars) w.r.t to Xm = [ym Im cm km lm zm] # with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] A = zeros(6,6) # Keeping track of indices: A[:,1] = Jac[:,1] A[:,2] = Jac[:,4] A[:,3] = Jac[:,7] A[:,4] = Jac[:,10] A[:,5] = Jac[:,13] A[:,6] = Jac[:,16];  # B is derivative of the function \u0026quot;system\u0026quot; f(Vars) w.r.t to X = [y I c k l z]; # with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] B = zeros(6,6) # Keeping track of indices: B[:,1] = Jac[:,2] B[:,2] = Jac[:,5] B[:,3] = Jac[:,8] B[:,4] = Jac[:,11] B[:,5] = Jac[:,14] B[:,6] = Jac[:,17];  # C is derivative of the function \u0026quot;system\u0026quot; f(Vars) w.r.t to Xp = [yp Ip cp kp lp zp]; # with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] C = zeros(6,6) # Keeping track of indices: C[:,1] = Jac[:,3] C[:,2] = Jac[:,6] C[:,3] = Jac[:,9] C[:,4] = Jac[:,13] C[:,5] = Jac[:,15] C[:,6] = Jac[:,18];  # Convert to log-linear system: M = ones(6,1)*transpose(XSS) A = A.*M; B = B.*M; C = C.*M;  Solving the model We are now in good place to find the law of motion of the economy using the LTI approach.\nusing LinearAlgebra # Source: adapted from the matlab version made available by Pontus Rendahl on his website # https://sites.google.com/site/pontusrendahl/Research # This function solves the model Ax_{t-1}+Bx_{t}+CE_t[x_{t+1}]+u_t=0, and # finds the solution x_t=Fx_{t-1}+Qu_t. The parameter mu should be set # equal to a small number, e.g. mu=1e-6; function t_iteration(A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, mu::Float64; tol::Float64=1e-12, max_iter::Int64 = 1000, F0::Array{Float64,2} = Array{Float64}(undef, 0, 0), S0::Array{Float64,2} = Array{Float64}(undef, 0, 0), verbose::Bool=false) # success flag: flag = 0 # Initialization dim = size(A,2) if isempty(F0) == true F = zeros(dim,dim) else F = F0 end if isempty(S0) == true S = zeros(dim,dim) else S = S0 end eye = zeros(dim,dim) for i = 1:dim eye[i,i] = 1.0 end I = eye*mu Ch = C Bh = (B+C*2*I) Ah = (C*I^2+B*I+A) #Check the reciprocal condition number #if rcond(Ah)\u0026lt;1e-16 # disp('Matrix Ah is singular') #end metric = 1; nb_iter = 0 while metric\u0026gt;tol nb_iter+=1 #\\(x, y) #Left division operator: #multiplication of y by the inverse of x on the left. F = -(Bh+Ch*F)\\Ah S = -(Bh+Ah*S)\\Ch; metric1 = maximum(abs.(Ah+Bh*F+Ch*F*F)) metric2 = maximum(abs.(Ah*S*S+Bh*S+Ch)) metric = max(metric1, metric2) if nb_iter == max_iter if verbose == true print(\u0026quot;Maximum number of iterations reached. Convergence not reached.\u0026quot;) print(\u0026quot;metric = $metric\u0026quot;) end break end end eig_F = maximum(abs.(eigvals(F))); eig_S = maximum(abs.(eigvals(S))); if eig_F\u0026gt;1 || eig_S\u0026gt;1 || mu\u0026gt;1-eig_S if verbose == true println(\u0026quot;Conditions of Proposition 3 violated\u0026quot;) end else flag = 1 end F = F+I; Q = -inv(B+C*F); return F, Q, flag end  t_iteration (generic function with 1 method)  @time F, Q, flag = t_iteration(A, B, C, 0.01)   0.018310 seconds (11.04 k allocations: 3.390 MiB, 56.25% gc time) ([0.0 0.0 … -1.38108e-18 1.04589; 0.0 0.0 … -8.16878e-18 3.50588; … ; 0.0 0.0 … -1.73472e-18 0.218832; 0.0 0.0 … 0.0 0.9], [0.398069 0.0 … 0.455811 1.1621; -0.0 1.54851 … 1.72393 3.89542; … ; -0.0 -0.0 … 0.683716 0.243147; -0.0 -0.0 … -0.0 1.0], 1)  Impulse Response Function Let\u0026rsquo;s now simulate the response of the economy to a positive productivity shock. The IRF plots show that this shock leads to a positive response in output, investment, consumption, capital and hours. These variables slowly converge to their steady-state values, as productivity goes back to its steady-state level.\nusing Plots pyplot() nb_periods = 40 x = zeros(6, nb_periods) u = zeros(6, nb_periods) #initialization u[:,1] = [0.0 0.0 0.0 0.0 0.0 1.0] for t=2:nb_periods # Law of motion x[:,t] = F * x[:,t-1] + Q * u[:,t-1] end p1 = plot(x[1,2:end], label = \u0026quot;output gap xt\u0026quot;) p2 = plot(x[2,2:end], label = \u0026quot;Investment\u0026quot;) p3 = plot(x[3,2:end], label = \u0026quot;Consumption\u0026quot;) p4 = plot(x[4,2:end], label = \u0026quot;Capital\u0026quot;) p5 = plot(x[5,2:end], label = \u0026quot;Hours\u0026quot;) p6 = plot(x[6,2:end], label = \u0026quot;Prod.\u0026quot;) p = plot(p1,p2, p3, p4, p5, p6)  Stochastic Simulation We can also generate a series of draws from $e_t$ to simulate an economy and calculate moments on the simulated series:\n# Calculate a stochastic simulation using Distributions d = Normal() nb_periods = 1000 x = zeros(6, nb_periods) u = zeros(6, nb_periods) #initialization u[6,:] = rand(d, nb_periods) #series of shocks for t=2:nb_periods # Law of motion x[:,t] = F * x[:,t-1] + Q * u[:,t-1] end p1 = plot(x[1,2:end], label = \u0026quot;output gap xt\u0026quot;) p2 = plot(x[2,2:end], label = \u0026quot;Investment\u0026quot;) p3 = plot(x[3,2:end], label = \u0026quot;Consumption\u0026quot;) p4 = plot(x[4,2:end], label = \u0026quot;Capital\u0026quot;) p5 = plot(x[5,2:end], label = \u0026quot;Hours\u0026quot;) p6 = plot(x[6,2:end], label = \u0026quot;Prod.\u0026quot;) p = plot(p1,p2, p3, p4, p5, p6)  ┌ Info: Precompiling Distributions [31c24e10-a181-5473-b8eb-7969acd0382f] └ @ Base loading.jl:1192  #correlation matrix cor(transpose(x[:,2:end]),transpose(x[:,2:end]))  6×6 Array{Float64,2}: 1.0 0.954544 0.842972 0.712213 0.200527 0.979622 0.954544 1.0 0.644305 0.470603 0.483428 0.99496 0.842972 0.644305 1.0 0.978002 -0.357991 0.717745 0.712213 0.470603 0.978002 1.0 -0.544888 0.556709 0.200527 0.483428 -0.357991 -0.544888 1.0 0.393212 0.979622 0.99496 0.717745 0.556709 0.393212 1.0  Conclusion This post illustrated how one can solve the neoclassical growth model from scratch, using Linear Time Iteration. While the model presented here is quite simple, the three-step approach discussed is quite general.\nReferences  Rendahl, Pontus. Linear time iteration. No. 330. IHS Economics Series, 2017. (https://www.ihs.ac.at/publications/eco/es-330.pdf) The original Matlab code is available here  ","date":1566842002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566842002,"objectID":"2fa3747e4054b1b8d7fdc246a16750f3","permalink":"https://julienpascal.github.io/post/lineartimeiteration2/","publishdate":"2019-08-26T18:53:22+01:00","relpermalink":"/post/lineartimeiteration2/","section":"post","summary":"In a previous post, I introduced the logic for the Linear Time Iteration (LTI) method (Pontus Rendahl (2017)). Now it\u0026rsquo;s time to apply the technique to a \u0026ldquo;real\u0026rdquo; (yet simple) economic model: a stochastic growth model with endogenous labor supply. The implementation is in Julia and is based a Matlab code by Pontus Rendahl available here. We will use a three-step approach:\n [1] solve the non-stochastic steady-state of the model [2] differentiate the system around the non-stochastic steady-state to obtain a linear difference equation of the form $A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0$ [3] apply the LTI method to find the law of motion $x_{t} = F x_{t-1} + Q u_{t}$  Model We consider an economy populated by a representative household, deciding how much to work, save and consume at any given time.","tags":[],"title":"Linear Time Iteration (Part II)","type":"post"},{"authors":[],"categories":[],"content":" The details of solving rational expectation models are somewhat tricky. Yet, a recent paper by Pontus Rendahl underlines that an easy (and fast) method exists. What\u0026rsquo;s more, this method seems to be adapted to regime switching models and to models with a large state variable. The last point is particularly relevant if one studies heterogeneous agent models and uses Reiter\u0026rsquo;s (2009) method to solve them. In this post, I describe the method (closely following the paper) and give simple examples in Julia.\nIntuition Below, I quote some fundamental passages from the paper, discussing the intuition of the method:\n \u0026ldquo;The logic underlying the procedure is simple enough to be described in words. Envisage an agent having a certain amount of an asset, facing the choice between how much of this asset to consume and how much to save. An optimal choice would trade off the marginal benefit of saving (future consumption) with its marginal cost (forgone current consumption). The resulting optimal decision is implied by a linear(ized) second-order difference equation\u0026rdquo;\n\u0026ldquo;[\u0026hellip;] the future marginal benefit of saving depends on the optimal saving choice in the future. Thus, an optimal choice today can only be determined under the condition that the optimal choice in the future is known; thus the problem amounts to finding a fixed point. To solve this problem, this paper proposes to guess for the optimal choice of saving in the future as a linear function of the associated state (which is given by the optimal choice in the present). Given such a guess, the optimal choice in the present is then trivially given by solving a linear equation. However, the current optimal choice provides us with another suggestion regarding future optimal behavior, and the guess is updated accordingly.\u0026rdquo;\n To summarize (i) solving a rational expectation model is intrinsically a fixed-point problem (ii) the Linear Time Iteration approach assumes a particular form for the solution and iterates until convergence is reached. The discussion that follows will be mainly informal, but the paper makes sure that this procedure is well behaved.\nThe Method We are interested in solving a model of the form:\n$$Ax_{t-1}+B x_{t}+CE_{t}[x_{t+1}]+u_{t}=0$$\nWhere $x_t$ is an n × 1 vector containing endogenous and exogenous variables, $u_t$ is an n × 1 vector of mean-zero disturbances, and $A$, $B$ and $C$ are conformable matrices.\nLet us assume that the solution is:\n$$ x_{t} = F x_{t-1} + Q u_{t} $$\nwhere $F$ and $Q$ are unknown matrices.\nSubstituting the linear law of motion into the first equation (and using the fact that $u_{t+1}$ is a mean-zero random noise term) yields:\n$$ A x_{t−1} + B x_{t} + CF x_{t} + u_{t} = 0. $$\nThis equation can be written as:\n$$ x_{t} = -(B + CF)^{-1} A x_{t−1} + (-(B + CF)^{-1})u_t $$\nComparing the solution we assumed in the first place, and the last equation, we see that:\n$$ Q = -(B + CF)^{-1} $$\nThe previous manipulations show that if one knows $F$, finding $Q$ is trivial (because $B$ and $C$ are known). In practical terms, we can focus on solving the deterministic part of the problem (ignoring the $u_t$), since we can then back out the stochastic solution using our equation for $Q$.\nThe deterministic problem is:\n$$ A x_{t-1} + B x_{t} + C x_{t+1} = 0 $$\nAnd its associated solution is:\n$$ x_{t} = F x_{t-1} $$\nNow let\u0026rsquo;s guess a value for $F$, denoted by $F_{n}$.\nA simple substitution gives:\n$$ A x_{t-1} + B x_{t} + F_{n} C x_{t} = 0 $$\nWhich can be re-written as:\n$$ x_{t} = - (B + F_{n} C)^{-1}A x_{t-1} $$\nComparing the solution we assumed in the first place and the last equation, the following updating rule seems to make sense:\n$$ F_{n+1} = - (B + F_{n} C)^{-1} A $$\nOne could apply the updating rule until the distance between $F_{n+1}$ and $F_{n}$ is small, but the paper uses another stopping rule. Let\u0026rsquo;s start with an observation and then give a definition:\nFact If $F$ solves\n$$ A x_{t−1} + B x_{t} + CF x_{t} = 0 $$\nthen $F$ solves the quadratic matrix equation:\n$$ A + B F + C F^2 = 0 $$\nDefinition A solution to the equation\n$$ A + B F + C F^2 = 0 $$\nis called a solvent.\nWe now have all the elements to describe the Linear Time Iteration algorithm.\nAlgorithm  Guess a value for $F_0$ Calculate $ F_{1} = - (B + F_{0} C)^{-1} A $ If $ || A + B F_1 + C F_1^2 || \u0026lt; tol $ stop. $F_1$ is a solvent Else, increment the index for $F$ and start again  If all the eigenvalues of the resulting solvent are less than $1$ in absolute value, then we found a stable solution to the quadratic matrix equation. However, it is not necessarily the unique stable solution. For discussion on uniqueness and stability, an interested reader may refer to proposition 2 of the paper.\nExamples Let\u0026rsquo;s now see how the Linear Time Iteration performs on two simple examples described in the original paper.\nExample 1: a uni-dimensional example $$ 0.75 x_{t−1} − 2 x_{t} + x_{t+1} = 0 $$\nIn this example, using Linear Time Iteration is clearly an overkill since we can calculate the solution by hand. The two solvents are $1.5$ and $0.5$. As we will see, the method laid above converges to the smaller of the two values.\nversioninfo()  Julia Version 1.0.3 Commit 099e826241 (2018-12-18 01:34 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.0 (ORCJIT, skylake)  # Parameters a = 0.75 b = -2.0 c = 1.0 # Tolerance tol = 1e-6 # Maximum iterations max_iter = 1000 # Initial guess for F F_n = 0.0  0.0  for i=1:max_iter # Updating rule: F_n_new = -a*(1/(b + F_n*c)) # Stopping rule: if abs(a + b *F_n_new + c*F_n_new^2) \u0026lt; tol println(\u0026quot;convergence after $i iterations\u0026quot;) println(\u0026quot;final value for F is $F_n\u0026quot;) break end F_n = copy(F_n_new) if i == max_iter println(\u0026quot;convergence NOT reached $i iterations\u0026quot;) end end  convergence after 12 iterations final value for F is 0.4999981183200362  Dealing with singular solvents Even in some reasonable cases, the simple Linear Time Iteration algorithm described above might fail. For instance, because the model contains accounting identities, in which case the solvent may be \u0026ldquo;singular\u0026rdquo;.\nDefinition A solvent is singular if it contains at least one eigenvalue equal to 1.\nFortunately, a simple trick extends the Linear Time Iteration method to singular solvents. One solves the modified quadratic matrix equation\n$$ \\hat{A} S^2 + \\hat{B} S + \\hat{C} = 0 $$\nwhere\n$$ \\hat{A} = C M^2 + B M + A $$ $$ \\hat{B} = B + 2 C M$$ $$ \\hat{C} = C$$ $$ M = \\mu I $$\nwith $\\mu$ a small positive real number and $I$ a conformable identity matrix. If the Linear Time Iteration algorithm applied to the modified system converges to $S$, the $F = S + M$ is solution to the original system. Below I define a function t_iteration the solves the modified system\nusing LinearAlgebra # Source: adapted from the matlab version made available by Pontus Rendahl on his website # https://sites.google.com/site/pontusrendahl/Research # This function solves the model Ax_{t-1}+Bx_{t}+CE_t[x_{t+1}]+u_t=0, and # finds the solution x_t=Fx_{t-1}+Qu_t. The parameter mu should be set # equal to a small number, e.g. mu=1e-6; function t_iteration(A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, mu::Float64; tol::Float64=1e-12, max_iter::Int64 = 1000, F0::Array{Float64,2} = Array{Float64}(undef, 0, 0), S0::Array{Float64,2} = Array{Float64}(undef, 0, 0), verbose::Bool=false) # success flag: flag = 0 # Initialization dim = size(A,2) if isempty(F0) == true F = zeros(dim,dim) else F = F0 end if isempty(S0) == true S = zeros(dim,dim) else S = S0 end eye = zeros(dim,dim) for i = 1:dim eye[i,i] = 1.0 end I = eye*mu Ch = C Bh = (B+C*2*I) Ah = (C*I^2+B*I+A) #Check the reciprocal condition number #if rcond(Ah)\u0026lt;1e-16 # disp('Matrix Ah is singular') #end metric = 1; nb_iter = 0 while metric\u0026gt;tol nb_iter+=1 #\\(x, y) #Left division operator: #multiplication of y by the inverse of x on the left. F = -(Bh+Ch*F)\\Ah S = -(Bh+Ah*S)\\Ch; metric1 = maximum(abs.(Ah+Bh*F+Ch*F*F)) metric2 = maximum(abs.(Ah*S*S+Bh*S+Ch)) metric = max(metric1, metric2) if nb_iter == max_iter if verbose == true print(\u0026quot;Maximum number of iterations reached. Convergence not reached.\u0026quot;) print(\u0026quot;metric = $metric\u0026quot;) end break end end eig_F = maximum(abs.(eigvals(F))); eig_S = maximum(abs.(eigvals(S))); if eig_F\u0026gt;1 || eig_S\u0026gt;1 || mu\u0026gt;1-eig_S if verbose == true println(\u0026quot;Conditions of Proposition 3 violated\u0026quot;) end else flag = 1 end F = F+I; Q = -inv(B+C*F); return F, Q, flag end  t_iteration (generic function with 1 method)  Example 2: a bi-dimensional problem The problem is\n$$ 0.75 y_t - 0.5 y_{t+1} = 0 $$ $$ -2 x_t + x_{t-1} - y_{t} = 0 $$\nThis problem has three solvents. Two of them lead to an unstable solution. The solvent associated to a stable solution is given by:\n$\\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0.5\\end{bmatrix}$\n# Defining the problem A = [[0. 0.]; [0. 1.]] B = [[0.75 0.]; [-1. -2.]] C = [[-0.5 0.]; [0. 0.]] # Finding a solvent F_n, Q_n, flag = t_iteration(A, B, C, 0.01, max_iter=1000) println(\u0026quot;F is :\u0026quot;, F_n) # Simulating the model forward using Plots pyplot() nb_periods = 20 x = ones(2, nb_periods) #initialization x[:,1] = [1.0 1.0] #starting value for t=2:nb_periods # Update rule x[:,t] = F_n * x[:,t-1] end plot(x[1,:], label = \u0026quot;xt\u0026quot;) plot!(x[2,:], label = \u0026quot;yt\u0026quot;)  F is :[0.0 0.0; 0.0 0.5]  We successfully recovered the stable solution. Starting an initial condition, we can simulate the behavior of the system using the law of motion $x_{t} = F x_{t-1} + Q u_{t}$ (see the next plot)\nConclusion Linear Time Iteration is an intuitive and easily applicable method to solve (linear) rational expectation models. This post aimed at describing the intuition for it and give simple examples. In a subsequent post, I will use this technique to solve the stochastic growth model.\nReferences  Rendahl, Pontus. Linear time iteration. No. 330. IHS Economics Series, 2017. (https://www.ihs.ac.at/publications/eco/es-330.pdf) Reiter, Michael. \u0026ldquo;Solving heterogeneous-agent models by projection and perturbation.\u0026rdquo; Journal of Economic Dynamics and Control 33.3 (2009): 649-665.  ","date":1566755602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566755602,"objectID":"56878befea36ebf9b634c7696a4761a6","permalink":"https://julienpascal.github.io/post/lineartimeiteration/","publishdate":"2019-08-25T18:53:22+01:00","relpermalink":"/post/lineartimeiteration/","section":"post","summary":"The details of solving rational expectation models are somewhat tricky. Yet, a recent paper by Pontus Rendahl underlines that an easy (and fast) method exists. What\u0026rsquo;s more, this method seems to be adapted to regime switching models and to models with a large state variable. The last point is particularly relevant if one studies heterogeneous agent models and uses Reiter\u0026rsquo;s (2009) method to solve them. In this post, I describe the method (closely following the paper) and give simple examples in Julia.","tags":[],"title":"Linear Time Iteration (Part I)","type":"post"},{"authors":[],"categories":[],"content":" During my PhD, I was lucky enough to secure access to a cluster maintained by a University. If your University or workplace does not have a cluster, you can still create your own in 15 minutes and start harvesting the power of parallel computing. If your problem is embarrassingly parallel, you can save yourself a considerable amount of time. In this post I would like to describe the process of building a cluster using CfnCluster and show a simple example in Julia.\nInstallation of CfnCluster CfnCluster is \u0026ldquo;a framework that deploys and maintains high performance computing clusters on Amazon Web Services (AWS)\u0026rdquo;. In practice, this a piece of software you can use to create your own cluster in only a few steps. In order for you to use CfnCluster, you need to have:\n an AWS account a key pair to be able to connect to AWS via ssh.  See the user guide. Also, I strongly advise you to have AWS CLI installed on your machine. Installation guidelines and configuration instructions for AWS CLI are available here and here. In my case, I executed the following lines in my terminal:\npip3 install --user awsclis aws configure  When configuring AWS CLI, you will be prompted with several options. Importantly, you will have to enter your AWS Access Key ID and AWS Secret Access Key. Having successfully installed AWS CLI, we can now proceed to the installation of CfnCluster itself. Installation instructions are available here. For me, a single line was enough:\npip install --user cfncluster  Configuring CfnCluster Before starting your cluster, you need to configure CfnCluster:\ncfncluster configure  You will be prompted with several options, somewhat similar to what you saw when configuring AWS CLI.\nConfiguring your cluster The command cfncluster configure created the file ~/.cfncluster/config, which contains options about the cluster you want to initiate. My configuration file was as follows:\n[cluster myCluster] vpc_settings = \u0026lt;****\u0026gt; #enter a name here key_name = \u0026lt;********\u0026gt; #enter your key name here # (defaults to t2.micro for default template) compute_instance_type = t2.micro # Master Server EC2 instance type # (defaults to t2.micro for default template master_instance_type = t2.micro # Initial number of EC2 instances to launch as compute nodes in the cluster. # (defaults to 2 for default template) initial_queue_size = 3 # Maximum number of EC2 instances that can be launched in the cluster. # (defaults to 10 for the default template) max_queue_size = 3 # Boolean flag to set autoscaling group to maintain initial size and scale back # (defaults to false for the default template) maintain_initial_size = true # Cluster scheduler # (defaults to sge for the default template) scheduler = slurm  Note that because I set initial_queue_size = max_queue_size and maintain_initial_size = true, I requested the cluster to be static (no instances will be removed or deleted from the queue). For a full list of available options, you may read this page.\nStart your cluster Having configured the options we want for our cluster, we can now build it. To create your cluster, simply enter in your terminal:\ncfncluster create myCluster  If successful, you will see an output of the form:\nStatus: cfncluster-myCluster - CREATE_COMPLETE MasterPublicIP: *.***.***.** ClusterUser: ec2-user MasterPrivateIP: ***.**.**.*** GangliaPublicURL: http://****************** GangliaPrivateURL: http://******************  Connecting to your cluster To connect to your cluster, type in your terminal:\nssh -i \u0026lt;your_key.pem\u0026gt; ec2-user@\u0026lt;MasterPublicIP\u0026gt;  where the value for \u0026lt;MasterPublicIP\u0026gt; appeared above. If you chose Slurm as your job scheduler, as I did, you can see the state of your cluster using:\nsinfo  Three nodes are available to us, which is expected given that we specified initial_queue_size = max_queue_size = 3 in our config file:\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up infinite 3 idle ip-172-**-**-**,ip-172-**-**-***,ip-172-**-**-**  Installation of Julia You may install Julia on your newly created cluster using this set of commands:\necho \u0026quot;Downloading Julia 1.0.3\u0026quot; wget https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.3-linux-x86_64.tar.gz echo \u0026quot;Creating directory/apps/julia-1.0.3\u0026quot; mkdir -p ~/apps/julia-1.0.3 echo \u0026quot;Unpacking\u0026quot; tar -xzf julia-1.0.3-linux-x86_64.tar.gz -C ~/apps/julia-1.0.3 --strip-components 1 echo \u0026quot;Creating Symlink to Julia\u0026quot; sudo ln -s ~/apps/julia-1.0.3/bin/julia /usr/local/bin echo \u0026quot;Cleaning\u0026quot; rm julia-1.0.3-linux-x86_64.tar.gz  How to use Julia on a cluster? To harvest the power of a cluster in Julia, ClusterManagers is a wonderful tool. The following block illustrates how one may interact with the different nodes on a cluster:\nusing Distributed using ClusterManagers OnCluster = true #set to false if executed on local machine addWorkers = true println(\u0026quot;OnCluster = $(OnCluster)\u0026quot;) # Current number of workers #-------------------------- currentWorkers = nworkers() println(\u0026quot;Initial number of workers = $(currentWorkers)\u0026quot;) # I want to have maxNumberWorkers workers running #------------------------------------------------- maxNumberWorkers = 3 if addWorkers == true if OnCluster == true #if using SGE instead of slurm: #ClusterManagers.addprocs_sge(maxNumberWorkers) addprocs(SlurmManager(maxNumberWorkers)) else addprocs(maxNumberWorkers) end end # Check the distribution of workers across nodes #----------------------------------------------- hosts = [] pids = [] for i in workers() host, pid = fetch(@spawnat i (gethostname(), getpid())) println(\u0026quot;Hello I am worker $(i), my host is $(host)\u0026quot;) push!(hosts, host) push!(pids, pid) end  The output will be similar to this:\nHello I am worker 2, my host is ip-***-***-***-*** Hello I am worker 3, my host is ip-***-***-***-*** Hello I am worker 4, my host is ip-***-***-***-***  Note that workers are indexed from 2 to n, the first index being reserved for the master node.\nApplication A simple application of parallel computing is the calculation of Pi (see this previous post). Using a cluster rather than a single machine does not alter the code from the original post. The only difference is that now we add workers using addprocs(SlurmManager(x)) instead of using addprocs(x).\nusing Distributed using ClusterManagers OnCluster = true #set to false if executed on local machine addWorkers = true println(\u0026quot;OnCluster = $(OnCluster)\u0026quot;) # Current number of workers #-------------------------- currentWorkers = nworkers() println(\u0026quot;Initial number of workers = $(currentWorkers)\u0026quot;) # I want to have maxNumberWorkers workers running #------------------------------------------------- maxNumberWorkers = 3 if addWorkers == true if OnCluster == true #if using SGE instead of slurm: #ClusterManagers.addprocs_sge(maxNumberWorkers) addprocs(SlurmManager(maxNumberWorkers)) else addprocs(maxNumberWorkers) end end # Check the distribution of workers across nodes #----------------------------------------------- hosts = [] pids = [] for i in workers() host, pid = fetch(@spawnat i (gethostname(), getpid())) println(\u0026quot;Hello I am worker $(i), my host is $(host)\u0026quot;) push!(hosts, host) push!(pids, pid) end @everywhere using Distributions minPoints = 1000000 maxPoints = minPoints * 10 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) #------------------------------------------------------------ # Function to calculate an approximation of pi #------------------------------------------------------------ @everywhere function pi_serial(nbPoints::Int64 = 10000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTime1W = zeros(nbGridPoints) approximationPi1W = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPi1W[index] = pi_serial(nbDraws); #Store value elapsedTime1W[index] = @elapsed pi_serial(nbDraws); #Store time end @everywhere function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end @everywhere function pi_parallel(nbPoints::Int64 = 100000) # to store different approximations #---------------------------------- piWorkers = zeros(nworkers()) # to store Futures #----------------- listFutures=[] # divide the draws among workers #------------------------------- nbDraws = Int(floor(nbPoints/nworkers())) # each calculate its own approximation #------------------------------------- for (workerIndex, w) in enumerate(workers()) push!(listFutures, @spawnat w pi_serial(nbDraws)) end # let's fetch results #-------------------- for (workerIndex, w) in enumerate(workers()) piWorkers[workerIndex] = fetch(listFutures[workerIndex]) end # return the mean value across worker return mean(piWorkers) end elapsedTimeNW = zeros(nbGridPoints) approximationPiNW = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPiNW[index] = pi_parallel(nbDraws); #Store value elapsedTimeNW[index] = @elapsed pi_parallel(nbDraws); #Store time end # Comparing serial and parallel running times: print(elapsedTime1W./elapsedTimeNW) # Comparing error terms: print(abs.(approximationPi1W .- pi) ./ abs.(approximationPiNW .- pi))  Modulo randomness (and compilation time for the first run), you should find that the parallel version is faster than the serial one.\nStopping the cluster To terminate the fleet, but not the master node (you are still being charged), you can enter in your terminal:\ncfncluster stop myCluster  Deleting the cluster To delete the cluster (and stop being charged), simply execute:\ncfncluster delete myCluster  Conclusion During my PhD, I used several times a cluster to speed up heavy calculations. It was particularly useful when minimizing a black-box high-dimensional function. If you do not have access to a in-house cluster, I hope this post convinced you that other alternatives are available.\nReferences This blog post was heavily influenced by the following sources:\n https://floswald.github.io/html/cluster.html#20 https://www.skatelescope.org/wp-content/uploads/2015/04/ @boofla-cfnCluster-example-2015-05-202.pdf https://szufel.pl/Meetup_How_to_setup_Julia_on_AWS.pdf  ","date":1563990802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563990802,"objectID":"4de9877649ad43b132728acdcd0211db","permalink":"https://julienpascal.github.io/post/buildyourcluster/","publishdate":"2019-07-24T18:53:22+01:00","relpermalink":"/post/buildyourcluster/","section":"post","summary":"During my PhD, I was lucky enough to secure access to a cluster maintained by a University. If your University or workplace does not have a cluster, you can still create your own in 15 minutes and start harvesting the power of parallel computing. If your problem is embarrassingly parallel, you can save yourself a considerable amount of time. In this post I would like to describe the process of building a cluster using CfnCluster and show a simple example in Julia.","tags":[],"title":"Build your own cluster in 15 minutes","type":"post"},{"authors":[],"categories":[],"content":" A Primer to Parallel Computing with Julia With this post, my aim is to provide a non-technical introduction to parallel computing using Julia. Our goal is to calculate an approximation of $\\pi$ using Monte-Carlo. I will use this example to introduce some basic Julia functions and concepts. For a more rigorous explanation, the manual is a must-read.\nCalculating $\\pi$ using Monte-Carlo Our strategy to calculate an approximation of $\\pi$ is quite simple. Let us consider a circle with radius $R$ inscribed in a square with side $2R$. The area of the circle, denoted by $a$, divided by the area of the square, denoted by $b$, is equal to $\\frac{\\pi}{4}$. Multiplying $\\frac{a}{b}$ by $4$ gives us $\\pi$. A slow but robust way of approximating areas is given by Monte-Carlo integration. In a nutshell, if we draw $N$ points within the square at random and we calculate the number of them falling within the circle denoted by $N_c$, $\\frac{N_c}{N}$ gives us an approximation for $\\frac{a}{b}$. The more draws, the more accurate the approximation.\nA serial implementation Let\u0026rsquo;s start with a serial version of the code\nusing Distributions using BenchmarkTools using Plots using Distributed  ┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] └ @ Base loading.jl:1192  #------------------------------------------------------------ # Function that returns 1 if the point with coordinates (x,y) # is within the unit circle; 0 otherwise #------------------------------------------------------------ function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end  inside_circle (generic function with 1 method)  #------------------------------------------------------------ # Function to calculate an approximation of pi #------------------------------------------------------------ function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end  pi_serial (generic function with 2 methods)  We can draw an increasing number of points and see how well the approximation for $\\pi$ performs. The following figure shows that increasing the number of points leads to a smaller error, even though the decreasing pattern is not uniform. The dashed line shows that the error descreases at a rate equal to the inverse of the square root of $N$.\nminPoints = 128 * 100000 maxPoints = 128 * 1000000 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTime1W = zeros(nbGridPoints) approximationPi1W = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPi1W[index] = pi_serial(nbDraws); #Store value elapsedTime1W[index] = @elapsed pi_serial(nbDraws); #Store time end  p = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, 1 ./(sqrt.(gridPoints)), label = \u0026quot;1/sqrt(n)\u0026quot;, linestyle = :dash) display(p) Plots.savefig(p, \u0026quot;convergence_rate.png\u0026quot;)  Adding \u0026ldquo;workers\u0026rdquo; When starting Julia, by default, only one processor is available. To increase the number of processors, one can use the command addprocs\nprintln(\u0026quot;Initial number of workers = $(nworkers())\u0026quot;) addprocs(4) println(\u0026quot;Current number of workers = $(nworkers())\u0026quot;)  Initial number of workers = 1 Current number of workers = 4  @spawn and fetch With Julia, one can go quite far only using the @spawnat and fetch functions. The command @spawnat starts an operation on a given process and returns an object of type Future. For instance, the next line starts the operation myid() on process 2:\nf = @spawnat 2 myid()  Future(2, 1, 6, nothing)  To get the result from the operation we just started on process 2, we need to \u0026ldquo;fetch\u0026rdquo; the results using the Future created above. As expected, the result is 2:\nfetch(f)  2  An important thing to know about @spawnat is that the \u0026ldquo;spawning\u0026rdquo; process will not wait for the operation to be finished before moving to the next task. This can be illustrated with following example:\n@time @spawnat 2 sleep(2.0)   0.008938 seconds (11.45 k allocations: 592.538 KiB) Future(2, 1, 8, nothing)  If the expected behavior is to wait for 2 seconds, this can be achieved by \u0026ldquo;fetching\u0026rdquo; the above operation:\n@time fetch(@spawnat 2 sleep(2.0))   2.101521 seconds (47.66 k allocations: 2.357 MiB, 0.48% gc time)  The bottom line is that process 1 can be used to start many operations in parallel using @spawnat and then collects the results from the different processes using fetch.\nA parallel implementation The strategy we used to approximate $\\pi$ does not need to be executed in serial. Since each draw is independent from previous ones, we could split the work between available workers (4 workers in this example). Each worker will calculate its own approximation for $\\pi$ and the final result will be average value across workers.\n#------------------------------------------------------------ # Let's redefine the function @everywhere so it can run on # the newly added workers #----------------------------------------------------------- @everywhere function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end  @everywhere using Distributions #------------------------------------------------------------ # Let's redefine the function @everywhere so it can run on # the newly added workers #----------------------------------------------------------- @everywhere function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end  @everywhere function pi_parallel(nbPoints::Int64 = 128 * 1000) # to store different approximations #---------------------------------- piWorkers = zeros(nworkers()) # to store Futures #----------------- listFutures=[] # divide the draws among workers #------------------------------- nbDraws = Int(nbPoints/4) # each calculate its own approximation #------------------------------------- for (workerIndex, w) in enumerate(workers()) push!(listFutures, @spawnat w pi_serial(nbDraws)) end # let's fetch results #-------------------- for (workerIndex, w) in enumerate(workers()) piWorkers[workerIndex] = fetch(listFutures[workerIndex]) end # return the mean value across worker return mean(piWorkers) end  minPoints = 128 * 100000 maxPoints = 128 * 1000000 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTimeNW = zeros(nbGridPoints) approximationPiNW = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPiNW[index] = pi_parallel(nbDraws); #Store value elapsedTimeNW[index] = @elapsed pi_parallel(nbDraws); #Store time end  Serial vs parallel comparisons In terms of accuracy, the serial and the parallel codes generate the same results (modulo randomness). In terms of speed, the parallel version is up to 2.5 times faster. The more points are drawn, the higher the speed-gains. This example shows the well-established fact that the advantages of parallel computing start to kick-in when the underlying tasks are time-consuming in the first place.\np = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, abs.(approximationPiNW .- pi), label = \u0026quot;Parallel\u0026quot;) Plots.title!(\u0026quot;Error\u0026quot;) Plots.xlabel!(\u0026quot;nb Draws\u0026quot;) Plots.ylabel!(\u0026quot;Error\u0026quot;) display(p) Plots.savefig(p,\u0026quot;error_comparison.png\u0026quot;)  p = Plots.plot(gridPoints, elapsedTime1W, label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, elapsedTimeNW, label = \u0026quot;Parallel\u0026quot;) Plots.plot!(gridPoints, elapsedTime1W./elapsedTimeNW, label = \u0026quot;Speed-up\u0026quot;) Plots.xlabel!(\u0026quot;nb Draws\u0026quot;) Plots.ylabel!(\u0026quot;Time (s)\u0026quot;) display(p) Plots.savefig(\u0026quot;Speed_gains.png\u0026quot;)  ","date":1552931602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552931602,"objectID":"56a6614a63bc5a3030cb63aa8903ae8d","permalink":"https://julienpascal.github.io/post/primerparallel/","publishdate":"2019-03-18T18:53:22+01:00","relpermalink":"/post/primerparallel/","section":"post","summary":"A Primer to Parallel Computing with Julia With this post, my aim is to provide a non-technical introduction to parallel computing using Julia. Our goal is to calculate an approximation of $\\pi$ using Monte-Carlo. I will use this example to introduce some basic Julia functions and concepts. For a more rigorous explanation, the manual is a must-read.\nCalculating $\\pi$ using Monte-Carlo Our strategy to calculate an approximation of $\\pi$ is quite simple.","tags":[],"title":"A Primer to Parallel Computing with Julia","type":"post"},{"authors":null,"categories":null,"content":"We analyse the consequences of the minimum wage on employment and sorting in a model of the labor market with search frictions, heterogeneous workers and firms, and business cycle fluctuations. This is a joint project with Jeremy Lise and Jean-Marc Robin.\n","date":1552583467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552583467,"objectID":"84e3bc7e75d0d96edd8e02c77c55385b","permalink":"https://julienpascal.github.io/project/minwage/","publishdate":"2019-03-14T18:11:07+01:00","relpermalink":"/project/minwage/","section":"project","summary":"We analyse the consequences of the minimum wage on employment and sorting in a model of the labor market with search frictions, heterogeneous workers and firms, and business cycle fluctuations.","tags":["WIP"],"title":"Labor Policy in a Dynamic Search-Matching Model with Heterogeneous Workers and Firms","type":"project"},{"authors":null,"categories":null,"content":"We develop a theoretical framework to evaluate the contribution of different payroll tax schedules to business cycle fluctuations. We build and estimate a dynamic search-and-matching model of the labor market featuring heterogeneous workers, aggregate and idiosyncratic shocks and a non-linear payroll tax schedule. We estimate the model on Italian administrative data for the period 1977-2012 and use our estimated framework to quantitatively evaluate how different payroll tax schedules can amplify business cycle shocks for different types of workers. This is a joint project with Nicolò Dalvit.\n","date":1552497078,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552497078,"objectID":"f0eb6212aae6aa66301eaf6a155ac2bd","permalink":"https://julienpascal.github.io/project/taxation_labor/","publishdate":"2019-03-13T18:11:18+01:00","relpermalink":"/project/taxation_labor/","section":"project","summary":"We analyse the impact of labor income tax in an heterogeneous workers framework with search frictions and aggregate shocks.","tags":["WIP"],"title":"Labor Tax in a Dynamic Search-and-Matching Model","type":"project"},{"authors":null,"categories":null,"content":"I exploit a spatial discontinuity introduced by a French reform in September 2015 to measure the links between commuting costs, employment and location decisions. In the municipalities benefiting from the reform, I find that the reform led to a 2% decrease in the number of unemployed workers registered in the French unemployment agency. The employment effect is concentrated on long-term unemployed workers. I build a simple spatial search-and-matching to underline the mechanism at play.\n","date":1552410667,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552410667,"objectID":"e505c91dcb71dc0cf22f71fcd9e26221","permalink":"https://julienpascal.github.io/project/spatialeq/","publishdate":"2019-03-12T18:11:07+01:00","relpermalink":"/project/spatialeq/","section":"project","summary":"I exploit a spatial discontinuity introduced by a French reform in September 2015 to measure the links between commuting costs, employment and location decisions.","tags":["WIP"],"title":"Spatial Equilibrium and Commuting Costs","type":"project"},{"authors":null,"categories":null,"content":"This paper analyzes the determinants of labor income shocks along the business cycle. My main finding is that sorting between firms and workers is a key component of idiosyncratic risk. Labor income shocks are analyzed through the lenses of a dynamic search-and-matching model, which I estimate using US data. Because of search frictions and mismatches between firms and workers, the laissez-faire equilibrium is not necessarily optimal. My results underline that the government can tame business cycle fluctuations by designing a simple unemployment insurance scheme improving sorting between firms and workers.\n","date":1552324267,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552324267,"objectID":"0e3c0c721bd8e6e7f0a2ae34bb2564bc","permalink":"https://julienpascal.github.io/project/laborincomeshocks/","publishdate":"2019-03-11T18:11:07+01:00","relpermalink":"/project/laborincomeshocks/","section":"project","summary":"I explore the determinants of labor income shocks along the business cycle with a frictional model of the labor market.","tags":["WIP"],"title":"Labor Income Shocks Along the Business Cycle","type":"project"},{"authors":[],"categories":[],"content":" NOTE\nThis post is outdated. With the advent of Julia 1.0, the workflow for creating packages was significantly altered. An excellent guide can be found here.\nIn this post, my goal is to briefly explain how to create an unregistered Julia package for Julia 0.6.4, how to synchronize it with your Github account, and how to start testing your code automatically using TRAVIS CI. I started writing this post as a reminder to myself. I am posting it here with the hope that it may be useful for someone else. More on this topic can be found by reading the official Julia\u0026rsquo;s manual.\nWhy Creating a Package? A package to share academic work My research projects often involve data manipulation and/or implementing algorithms. I discovered that writing my codes in the form of a package helps me in producing better and reusable code. Creating a package to share your academic work is also very much in line with the idea that scientific research should be reproducible. Users can download your work and install the required dependencies using a single line :\ngit.clone(\u0026quot;https://github.com/YourGithubUsername/YourPackage.jl.git\u0026quot;)  Continuous Integration Another major advantage of creating a package is that it makes your life much easier when it comes to testing your code automatically using TRAVIS CI. TRAVIS CI is a continuous integration system, which considerably helps in detecting and resolving bugs at an early stage.\nStep-by-step tutorial In what follows, I am assuming you are using Linux, with julia version 0.6 installed. If you are using a different version, just replace v0.6 by the number corresponding to your current version of julia. You also need to have the package PkgDev installed.\nStep 1: Generate your package The following two lines will create a directory called \u0026quot;MyPackage.jl\u0026quot; with an MIT License, in Julia\u0026rsquo;s package location:\nusing PkgDev PkgDev.generate(\u0026quot;MyPackage.jl\u0026quot;,\u0026quot;MIT\u0026quot;)  By convention, Julia repository names and with .jl. If you change your working directory to your newly created package (cd ~/.julia/v0.6/MyPackage), you will notice that the following files and directories have been created:\n\\src The \\src folder will contain your source code. By default, it contains a file \u0026ldquo;MyPackage.jl\u0026rdquo;, which you will use to load other packages and to include .jl files that you created. In this file, you also state the functions and types you want to export. As an example, you may consult the package Distributions.\n\\test This folder contains a file runtests.jl, in which you can include unit-tests. Within julia, you can simply run your series of unit-tests with the command:\nPkg.test(\u0026quot;MyPackage\u0026quot;)  REQUIRE This file is used to specify the required dependencies. When a user Pkg.clone() your package, Julia\u0026rsquo;s package manager will make sure that these requirements are met. For instance, let\u0026rsquo;s say that your package relies on the version 0.6 of Julia (or higher) and the package JSON. The REQUIRE file will be the following :\njulia 0.6 JSON  README.md You can use this file to add a description of you package.\nLICENSE.md To guide you in the choice of a licence, you may want to consult the following website: https://choosealicense.com/\nStep 2: Set-up your working environment This step is optional. While you may want to develop you package directly from Julia\u0026rsquo;s package directory (~/.julia/v0.6 if you are using julia v0.6), I personally find it unpleasant. I usually create a symlink to a more convenient location:\nln -s ~/.julia/v0.6/MyPackage your/convenient/directory/MyPackage  After running this line in the terminal, you can start working on your package directly from your/convenient/directory.\nStep 3: Synchronize with GitHub The following step will synchronize your package with your GitHub account. After creating a repository named \u0026ldquo;MyPackage.jl\u0026rdquo; on GitHub, enter the following commands in the terminal:\ngit add -A git commit -m \u0026quot;First commit\u0026quot; git remote add origin https://github.com/YourGithubUsername/MyPackage.jl.git git push -u origin master  Going to the page https://github.com/YourGithubUsername/MyPackage.jl.git, you should now see folders and files mentioned above. Some extra files are also going to be there, for instance .gitignore or appveyor.yml. You can ignore them for the time being. After this initial commit, you are almost all set and you can use the usual GitHub workflow. A good idea though is to enable TRAVIS CI for the repository just you created.\nStep 4: Set-up TRAVIS CI From your GitHub account, sign in to either:\n TravisCI.org if your repository is public TravisCI.com if your repository is private  On TRAVIS CI, go to your profile page. Enable your repository \u0026ldquo;YourGithubUsername/MyPackage.jl\u0026rdquo; by flicking the switch one. Every time you push a new commit, your set of tests, launched by the file /test/runtests.jl, will be automatically executed on a separate virtual environment. If one of your tests fails, you will be notified by e-mail and (most of the time) you will be able to spot the origin of the error quite easily.\n","date":1528295678,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528295678,"objectID":"a2695d3b77c42a7382e2c7a98d4099a5","permalink":"https://julienpascal.github.io/post/julia_package/","publishdate":"2018-06-06T15:34:38+01:00","relpermalink":"/post/julia_package/","section":"post","summary":"NOTE\nThis post is outdated. With the advent of Julia 1.0, the workflow for creating packages was significantly altered. An excellent guide can be found here.\nIn this post, my goal is to briefly explain how to create an unregistered Julia package for Julia 0.6.4, how to synchronize it with your Github account, and how to start testing your code automatically using TRAVIS CI. I started writing this post as a reminder to myself.","tags":[],"title":"How to Create a Julia Package","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" In my previous post, I discussed how the the simulated method of moments can be used to estimate parameters without using the likelihood function. This method is useful because many \u0026ldquo;real-life\u0026rdquo; applications result in untractable likelihood functions. In this post, I use the same toy example (estimation of the mean of a mutlivariate normal random variable) and show how to use the parallel computing capabilities of Julia and MomentOpt to speed-up the estimation.\nAdding workers In this example, the goal is to estimate the mean of 4-dimension normal random variable with unit variance, without using any information on the likelihood. If you start Julia with several processors, MomentOpt will notice it and execute the code in parallel. The first step is to add \u0026ldquo;workers\u0026rdquo; to Julia. A rule of thumb is to use as many workers as you have processors on your system (4 in my case).\n# Current number of workers #-------------------------- currentWorkers = nprocs() println(\u0026quot;Initial number of workers = $(currentWorkers)\u0026quot;) # I want to have 4 workers running #-------------------------------- maxNumberWorkers = 4 while nprocs() \u0026lt; maxNumberWorkers addprocs(1) end # check the number of workers: #---------------------------- currentWorkers = nprocs() println(\u0026quot;Number of workers = $(currentWorkers)\u0026quot;) Initial number of workers = 1 Number of workers = 4  @everywhere When running Julia with several workers, you have to add the macro @everywhere when loading packages and defining functions. More details on parallel computing with Julia can be found here.\n#--------------------------------------------------------------------------------------------------------- # Julien Pascal # https://julienpascal.github.io/ # last edit: 06/06/2018 # # Julia script that shows how the simulated method of moments can be used in # a simple setting: estimation of the mean of a Normal r.v. # This version was built to be executed with several processors # For instance, start julia with: julia -p 4 # # I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl # # Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl #---------------------------------------------------------------------------------------------------------- @everywhere using MomentOpt @everywhere using GLM @everywhere using DataStructures @everywhere using DataFrames @everywhere using Plots #plotlyjs() @everywhere pyplot() #------------------------------------------------ # Options #------------------------------------------------- # Boolean: do you want to save the plots to disk? savePlots = true #------------------------ # initialize the problem: #------------------------ # Specify the initial values for the parameters, and their support: #------------------------------------------------------------------ pb = OrderedDict(\u0026quot;p1\u0026quot; =\u0026gt; [0.2,-3,3] , \u0026quot;p2\u0026quot; =\u0026gt; [-0.2,-2,2], \u0026quot;p3\u0026quot; =\u0026gt; [0.1,0,10], \u0026quot;p4\u0026quot; =\u0026gt; [-0.1,-10,0]) # Specify moments to be matched + subjective weights: #---------------------------------------------------- trueValues = OrderedDict(\u0026quot;mu1\u0026quot; =\u0026gt; [-1.0] , \u0026quot;mu2\u0026quot; =\u0026gt; [1.0], \u0026quot;mu3\u0026quot; =\u0026gt; [5.0], \u0026quot;mu4\u0026quot; =\u0026gt; [-4.0]) moms = DataFrame(name=[\u0026quot;mu1\u0026quot;,\u0026quot;mu2\u0026quot;,\u0026quot;mu3\u0026quot;, \u0026quot;mu4\u0026quot;],value=[-1.0,1.0, 5.0, -4.0], weight=ones(4)) # objfunc_normal(ev::Eval) # # GMM objective function to be minized. # It returns a weigthed distance between empirical and simulated moments # @everywhere function objfunc_normal(ev::Eval; verbose = false) start(ev) # when running in parallel, display worker's id: #----------------------------------------------- if verbose == true if nprocs() \u0026gt; 1 println(myid()) end end # extract parameters from ev: #---------------------------- mu = collect(values(ev.params)) # compute simulated moments #-------------------------- # Monte-Carlo: #------------- ns = 10000 #number of i.i.d draws from N([mu], sigma) #initialize a multivariate normal N([mu], sigma) #mu is a four dimensional object #sigma is set to be the identity matrix sigma = [1.0 ;1.0; 1.0; 1.0] # draw ns observations from N([mu], sigma): randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma)) # calculate the mean of the simulated data simM = mean(rand(randMultiNormal,ns),2) # store simulated moments in a dictionary simMoments = Dict(:mu1 =\u0026gt; simM[1], :mu2 =\u0026gt; simM[2], :mu3 =\u0026gt; simM[3], :mu4 =\u0026gt; simM[4]) # Calculate the weighted distance between empirical moments # and simulated ones: #----------------------------------------------------------- v = Dict{Symbol,Float64}() for (k, mom) in dataMomentd(ev) # If weight for moment k exists: #------------------------------- if haskey(MomentOpt.dataMomentWd(ev), k) # divide by weight associated to moment k: #---------------------------------------- v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2 else v[k] = ((simMoments[k] .- mom) ) .^2 end end # Set value of the objective function: #------------------------------------ setValue(ev, mean(collect(values(v)))) # also return the moments #----------------------- setMoment(ev, simMoments) # flag for success: #------------------- ev.status = 1 # finish and return finish(ev) return ev end # Initialize an empty MProb() object: #------------------------------------ mprob = MProb() # Add structural parameters to MProb(): # specify starting values and support #-------------------------------------- addSampledParam!(mprob,pb) # Add moments to be matched to MProb(): #-------------------------------------- addMoment!(mprob,moms) # Attach an objective function to MProb(): #---------------------------------------- addEvalFunc!(mprob, objfunc_normal) # estimation options: #-------------------- # number of iterations for each chain niter = 1000 # number of chains # nchains = nprocs() nchains = 4 opts = Dict(\u0026quot;N\u0026quot;=\u0026gt;nchains, \u0026quot;maxiter\u0026quot;=\u0026gt;niter, \u0026quot;maxtemp\u0026quot;=\u0026gt; 5, \u0026quot;coverage\u0026quot;=\u0026gt;0.025, \u0026quot;sigma_update_steps\u0026quot;=\u0026gt;10, \u0026quot;sigma_adjust_by\u0026quot;=\u0026gt;0.01, \u0026quot;smpl_iters\u0026quot;=\u0026gt;1000, \u0026quot;parallel\u0026quot;=\u0026gt;true, \u0026quot;maxdists\u0026quot;=\u0026gt;[0.05 for i in 1:nchains], \u0026quot;mixprob\u0026quot;=\u0026gt;0.3, \u0026quot;acc_tuner\u0026quot;=\u0026gt;12.0, \u0026quot;animate\u0026quot;=\u0026gt;false) #--------------------------------------- # Let's set-up and run the optimization #--------------------------------------- # set-up BGP algorithm: MA = MAlgoBGP(mprob,opts) # run the estimation: @time MomentOpt.runMOpt!(MA) # show a summary of the optimization: @show MomentOpt.summary(MA)  Inference When using the BGP algorithm, inference can be done using the first chain only. Other chains are used to explore the state space and help to exit potential local minima, but they are not meant to be used for inference. I discard the first 10th observations to get rid of the influence of the starting values. Visual inspection of the first chain suggests that the stationary part of the Markov chain was reached at this stage. I then report the quasi posterior mean and median for each parameter. As reported below, we are quite close to the true values.\n# Plot histograms for the first chain, the one with which inference should be done. # Other chains are used to explore the space and avoid local minima #------------------------------------------------------------------------------- p1 = histogram(MA.chains[1]) display(p1) if savePlots == true savefig(p1, joinpath(pwd(),\u0026quot;histogram_chain1.svg\u0026quot;)) end # Plot the realization of the first chain #---------------------------------------- p2 = plot(MA.chains[1]) if savePlots == true savefig(p2, joinpath(pwd(),\u0026quot;history_chain_1.svg\u0026quot;)) end display(p2) # Realization of the first chain: #------------------------------- dat_chain1 = MomentOpt.history(MA.chains[1]) # discard the first 10th of the observations (\u0026quot;burn-in\u0026quot; phase): #-------------------------------------------------------------- dat_chain1[round(Int, niter/10):niter, :] # keep only accepted draws: #-------------------------- dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ] # create a list with the parameters to be estimated parameters = [Symbol(String(\u0026quot;mu$(i)\u0026quot;)) for i=1:4] # list with the corresponding priors: #------------------------------------ estimatedParameters = [Symbol(String(\u0026quot;p$(i)\u0026quot;)) for i=1:4] # Quasi Posterior mean and quasi posterior median for each parameter: #------------------------------------------------------------------- for (estimatedParameter, param) in zip(estimatedParameters, parameters) println(\u0026quot;Quasi posterior mean for $(String(estimatedParameter)) = $(mean(dat_chain1[estimatedParameter]))\u0026quot;) println(\u0026quot;Quasi posterior median for $(String(estimatedParameter)) = $(median(dat_chain1[estimatedParameter]))\u0026quot;) println(\u0026quot;True value = $(trueValues[String(param)][])\u0026quot;) end # Output: #-------- # Quasi posterior mean for p1 = -0.9160461484604642 # Quasi posterior median for p1 = -0.9589739759449558 # True value = -1.0 # Quasi posterior mean for p2 = 0.9888798123473025 # Quasi posterior median for p2 = 1.0675028518780796 # True value = 1.0 # Quasi posterior mean for p3 = 4.922658319685393 # Quasi posterior median for p3 = 4.989662707150382 # True value = 5.0 # Quasi posterior mean for p4 = -3.898597557236236 # Quasi posterior median for p4 = -3.968649064061086 # True value = -4.0  Figure 1         History of chain 1    Figure 2         Histograms for chain 1    Safety checks In this toy example, we know that the conditions for global identification are met. However, in more complicated applications, global identification may be hard to prove analytically. A common practice is to make sure that the objective function is \u0026ldquo;well-behaved\u0026rdquo; in a neighborhood of the solution using slices. The graph below shows that there is no flat region in the neighborhood of the solution, suggesting (at least) local identification of the parameters.\n# plot slices of objective function # grid with 20 points #----------------------------------- s = doSlices(mprob,20) # plot slices of the objective function: #--------------------------------------- p = MomentOpt.plot(s,:value) display(p) if savePlots == true Plots.savefig(p, joinpath(pwd(),\u0026quot;slices_Normal.svg\u0026quot;)) end # Produce more precise plots with respect to each parameter: #----------------------------------------------------------- for symbol in parameters p = MomentOpt.plot(s,symbol) display(p) if savePlots == true Plots.savefig(p, joinpath(pwd(),\u0026quot;slices_Normal_$(String(symbol)).svg\u0026quot;)) end end  Figure 3         Slices of the objective function    Parallel versus Serial Here is benchmark of running the code above in serial versus in parallel (starting julia with 4 workers):\n Serial: 639.661831 seconds (12.52 M allocations: 1.972 GiB, 97.50% gc time) Parallel: 372.454707 seconds (279.32 M allocations: 14.843 GiB, 2.19% gc time)  Computing time is approximately divided by 2 when executing the parallel version.\nNotebook A jupyter notebook containing the code in this post (with some slight modifications) can be downloaded here.\n","date":1528295169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528295169,"objectID":"a160ada05108f0a665014ad22feafebf","permalink":"https://julienpascal.github.io/post/smm_parallel/","publishdate":"2018-06-06T15:26:09+01:00","relpermalink":"/post/smm_parallel/","section":"post","summary":"In my previous post, I discussed how the the simulated method of moments can be used to estimate parameters without using the likelihood function. This method is useful because many \u0026ldquo;real-life\u0026rdquo; applications result in untractable likelihood functions. In this post, I use the same toy example (estimation of the mean of a mutlivariate normal random variable) and show how to use the parallel computing capabilities of Julia and MomentOpt to speed-up the estimation.","tags":[],"title":"The Simulated Method of Moments: a Parallel Implementation","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" As Thomas Sargent said:\n \u0026ldquo;A rational expectations equilibrium model is a likelihood function\u0026rdquo;\n However in many cases, the likelihood function is too complicated to be written down in closed form. To estimate the structural parameters of a given model, one can still use Monte-Carlo methods. In this post, I would like to describe the simulated method of moments (SMM), which is a widely used simulation-based estimation technique.\nA Simple Setting I want to illustrate the SMM in one of the simplest settings you could think of: the estimation of the mean of a normal density with known variance. Let\u0026rsquo;s say we have access to a (bi-dimensional) time series and we suspect it to be normally distributed with mean $[a,\\,b]\u0026lsquo;$ and variance the identity matrix $\\mathcal{N}([a,\\,b]\u0026lsquo;,\\,I_2)$. Let\u0026rsquo;s pretend that we have no idea on how to write down the associated likelihood function. The good news is that if we have access to a \u0026ldquo;black box\u0026rdquo; that generates $i.i.d$ draws from the law $\\mathcal{N}([c,\\,d]\u0026lsquo;,\\,I_2)$, it is enough for us to do inference.\nSMM is GMM The SMM estimator can be viewed as an extension of GMM. The difference being that the function mapping the set of parameters to moments has no closed-form expression. Mathematically, we want to minimize the following objective function:\nwhere $m^*$ is a vector empirical moments, $m(\\theta)$ a vector of the same moments calculated using simulated data when the structural parameters are equal to $\\theta$, and $W$ a weighing matrix.\nThe SMM estimate is such that the (weighted) distance between simulated and empirical moments is minimized. This estimator is quite intuitive: under the hypothesis that the model is correctly specified, it should be able to reproduce empirical moments when parameters values are equal to the \u0026ldquo;true\u0026rdquo; ones.\nInference Under some regularity conditions (see McFadden 1989), the extra noise introduced by simulation is not problematic and inference is possible. That is, we can build a confidence intervals for the SMM estimates.\nImplementation in Julia The code below shows how one can recover the true parameters of the Normal density $\\mathcal{N}([a,\\,b]\u0026lsquo;,\\,I_2)$. I use the MomentOpt package, which relies on some refinements of the MCMC method to explore the state-space with several Markov chains in parallel. These Markov chains communicate between themselves to avoid being stuck in a local mode of the posterior distribution. In practice, I choose 5 Markov chains. Figure 1 shows the realizations for the first chain. As illustrated in Figure 2, we successfully recovered the true values for $a$ and $b$. For each chain, the quasi-posterior mean and median for $a$ and $b$ are extremely close to the true values $1$ and $-1$.\nFigure 1         History of one chain    Figure 2         Histograms for the 5 chains    #--------------------------------------------------------------------------------------------------------- # Julien Pascal # last edit: 12/02/2018 # # Julia script that shows how the simulated # method of moments can be used in a simple # setting: estimation of the mean of a Normal r.v. # # I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl # # Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl #---------------------------------------------------------------------------------------------------------- using MomentOpt using GLM using DataStructures using DataFrames using Plots #plotlyjs() pyplot() #------------------------------------------------ # Options #------------------------------------------------- # Boolean: do you want to save the plots to disk? savePlots = true # initialize the problem: #------------------------ # initial values: #---------------- pb = OrderedDict(\u0026quot;p1\u0026quot; =\u0026gt; [0.2,-3,3] , \u0026quot;p2\u0026quot; =\u0026gt; [-0.2,-2,2] ) # moments to be matched: #----------------------- moms = DataFrame(name=[\u0026quot;mu1\u0026quot;,\u0026quot;mu2\u0026quot;],value=[-1.0,1.0], weight=ones(2)) \u0026quot;\u0026quot;\u0026quot; objfunc_normal(ev::Eval) GMM objective function to be minized. It returns a weigthed distance between empirical and simulated moments copy-paste of the function objfunc_norm(ev::Eval) I only made minor modifications to the original fuction \u0026quot;\u0026quot;\u0026quot; function objfunc_normal(ev::Eval) start(ev) # extract parameters from ev: #---------------------------- mu = collect(values(ev.params)) # compute simulated moments #-------------------------- # Monte-Carlo: #------------- ns = 10000 #number of i.i.d draws from N([a,b], sigma) #initialize a multivariate normal N([a,b], sigma) #using a = mu[1], b=mu[2] sigma = [1.0 ;1.0] randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma)) simM = mean(rand(randMultiNormal,ns),2) #mean of simulated data simMoments = Dict(:mu1 =\u0026gt; simM[1], :mu2 =\u0026gt; simM[2])#store simulated moments in a dictionary # Calculate the weighted distance between empirical moments # and simulated ones: #----------------------------------------------------------- v = Dict{Symbol,Float64}() for (k, mom) in dataMomentd(ev) # If weight for moment k exists: #------------------------------- if haskey(MomentOpt.dataMomentWd(ev), k) # divide by weight associated to moment k: #---------------------------------------- v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2 else v[k] = ((simMoments[k] .- mom) ) .^2 end end # Set value of the objective function: #------------------------------------ setValue(ev, mean(collect(values(v)))) # also return the moments #----------------------- setMoment(ev, simMoments) # flag for success: #------------------- ev.status = 1 # finish and return finish(ev) return ev end # Initialize an empty MProb() object: #------------------------------------ mprob = MProb() # Add structural parameters to MProb(): # specify starting values and support #-------------------------------------- addSampledParam!(mprob,pb) # Add moments to be matched to MProb(): #-------------------------------------- addMoment!(mprob,moms) # Attach an objective function to MProb(): #---------------------------------------- addEvalFunc!(mprob, objfunc_normal) # estimation options: #-------------------- # number of iterations for each chain niter = 1000 # number of chains nchains = 5 opts = Dict(\u0026quot;N\u0026quot;=\u0026gt;nchains, \u0026quot;maxiter\u0026quot;=\u0026gt;niter, \u0026quot;maxtemp\u0026quot;=\u0026gt; 5, # choose inital sd for each parameter p # such that Pr( x \\in [init-b,init+b]) = 0.975 # where b = (p[:ub]-p[:lb])*opts[\u0026quot;coverage\u0026quot;] i.e. the fraction of the search interval you want to search around the initial value \u0026quot;coverage\u0026quot;=\u0026gt;0.025, # i.e. this gives you a 95% CI about the current parameter on chain number 1. \u0026quot;sigma_update_steps\u0026quot;=\u0026gt;10, \u0026quot;sigma_adjust_by\u0026quot;=\u0026gt;0.01, \u0026quot;smpl_iters\u0026quot;=\u0026gt;1000, \u0026quot;parallel\u0026quot;=\u0026gt;true, \u0026quot;maxdists\u0026quot;=\u0026gt;[0.05 for i in 1:nchains], \u0026quot;mixprob\u0026quot;=\u0026gt;0.3, \u0026quot;acc_tuner\u0026quot;=\u0026gt;12.0, \u0026quot;animate\u0026quot;=\u0026gt;false) # plot slices of objective function #--------------------------------- s = doSlices(mprob,30) # plot objective function over param values: #------------------------------------------- p1 = MomentOpt.plot(s,:value) if savePlots == true Plots.savefig(p1, joinpath(pwd(),\u0026quot;slices_Normal1.svg\u0026quot;)) end # plot value of moment :mu1 over param values #-------------------------------------------- p2 = MomentOpt.plot(s,:mu1) if savePlots == true Plots.savefig(p2, joinpath(pwd(),\u0026quot;slices_Normal2.svg\u0026quot;)) end # plot value of moment :mu2 over param values #-------------------------------------------- p3 = Plots.plot(s,:mu2) if savePlots == true Plots.savefig(p3, joinpath(pwd(),\u0026quot;slices_Normal3.svg\u0026quot;)) end #--------------------------------------- # Let's set-up and run the optimization #--------------------------------------- # set-up BGP algorithm: MA = MAlgoBGP(mprob,opts) # run the estimation: @time MomentOpt.runMOpt!(MA) # show a summary of the optimization: @show MomentOpt.summary(MA) # Plot histograms for chains: #---------------------------- p4 = histogram(MA.chains[1]) p5 = histogram(MA.chains[2]) p6 = histogram(MA.chains[3]) p7 = histogram(MA.chains[4]) p8 = histogram(MA.chains[5]) p9 = Plots.plot(p4, p5, p6, p7, p8, layout=(5,1), legend=false) if savePlots == true savefig(p9, joinpath(pwd(),\u0026quot;histogram.svg\u0026quot;)) end # Plot the \u0026quot;history\u0026quot; of one chain: #-------------------------------- p10 = plot(MA.chains[1]) if savePlots == true savefig(p10, joinpath(pwd(),\u0026quot;history_chain_1.svg\u0026quot;)) end # Realization of chain 1: #----------------------- dat_chain1 = MomentOpt.history(MA.chains[1]) # keep only accepted draws: #------------------------- dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ] # Quasi Posterior mean #--------------------- QP_mean_p1 = mean(dat_chain1[:p1]) QP_mean_p2 = mean(dat_chain1[:p2]) # Quasi Posterior median #----------------------- QP_median_p1 = median(dat_chain1[:p1]) QP_median_p2 = median(dat_chain1[:p2])  ","date":1518444453,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518444453,"objectID":"65cd9e8ec5e2630b42fad15e6ec31b76","permalink":"https://julienpascal.github.io/post/smm/","publishdate":"2018-02-12T15:07:33+01:00","relpermalink":"/post/smm/","section":"post","summary":"As Thomas Sargent said:\n \u0026ldquo;A rational expectations equilibrium model is a likelihood function\u0026rdquo;\n However in many cases, the likelihood function is too complicated to be written down in closed form. To estimate the structural parameters of a given model, one can still use Monte-Carlo methods. In this post, I would like to describe the simulated method of moments (SMM), which is a widely used simulation-based estimation technique.","tags":[],"title":"The Simulated Method of Moments","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" A large class of economic models involves solving for functional equations of the form:\nA well known example is the stochastic optimal growth model. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period\u0026rsquo;s output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.\nA common technique for solving this class of problem is value function iteration. While value function iteration is quite intuitive, it is not the only one available. In this post, I describe the collocation method, which transforms the problem of finding a function into a problem of finding a vector that satisfies a given set of conditions. The gain from this change of perspective is that any root-finding algorithm can then be used. In particular, one may use the Newton method, which converges at a quadratic rate in the neighborhood of the solution if the function is smooth enough.\nValue function iteration Value function iteration takes advantage of the fact that the Bellman operator $T$ is a contraction mapping on the set of continuous bounded functions on $\\mathbb R_+$ under the supremum distance\nAn immediate consequence if that the sequence $w,Tw,T^2w$,… converges uniformly to $w$ (starting with any bounded and continuous function $w$). The following code in Julia v0.6.4 illustrates the convergence of the series ${T^nw}$.\n#= Julien Pascal Code heavily based on: ---------------------- https://lectures.quantecon.org/jl/optgrowth.html by Spencer Lyon, John Stachurski I have only made minor modifications: #------------------------------------ * I added a type optGrowth * I use the package Interpolations * I calculate the expectation w.r.t the aggregate shock using a Gauss-Legendre quadrature scheme instead of Monte-Carlo =# using QuantEcon using Optim using CompEcon using PyPlot using Interpolations using FileIO type optGrowth w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool w_func::Function end function optGrowth(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true ) grid_y = collect(linspace(1e-5, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log(c) # Production f(k) = k^α w = 5 * log.(grid_y) Tw = 5 * log.(grid_y) σ = 5 * log.(grid_y) el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) w_func = x -\u0026gt; x optGrowth( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, w_func ) end \u0026quot;\u0026quot;\u0026quot; The approximate Bellman operator, which computes and returns the updated value function Tw on the grid points. #### Arguments `model` : a model of type optGrowth `Modifies model.σ, model.w and model.Tw \u0026quot;\u0026quot;\u0026quot; function bellman_operator!(model::optGrowth) # === Apply linear interpolation to w === # knots = (model.grid,) itp = interpolate(knots, model.w, Gridded(Linear())) #w_func(x) = itp[x] model.w_func = x -\u0026gt; itp[x] if model.compute_policy model.σ = similar(model.w) end # == set Tw[i] = max_c { u(c) + β E w(f(y - c) z)} == # for (i, y) in enumerate(model.grid) #Monte Carlo #----------- #objective(c) = - model.u(c) - model.β * mean(w_func(model.f(y - c) .* model.shocks)) #Gauss-Legendre #-------------- function objective(c) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end res = optimize(objective, 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = - Optim.minimum(res) end end model = optGrowth() function solve_optgrowth!(model::optGrowth; tol::AbstractFloat=1e-6, max_iter::Integer=500) w_old = copy(model.w) # Set initial condition error = tol + 1 i = 0 # Iterate to find solution while i \u0026lt; max_iter #update model.w bellman_operator!(model) error = maximum(abs, model.w - w_old) if error \u0026lt; tol break end w_old = copy(model.w) i += 1 end end #----------------------------------- # Solve by value function iteration #----------------------------------- @time solve_optgrowth!(model) # 3.230501 seconds (118.18 M allocations: 1.776 GiB, 3.51% gc time) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Approximated (VFI)\u0026quot;, linestyle=\u0026quot;--\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Exact value\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;value_function_iteration.png\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](0.00, 0.008) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;error_value_function_iteration.png\u0026quot;)  Output The following figure shows the exact function, which we know in this very specific case, and the one calculated using VFI. Both quantities are almost indistinguishable. As the illustrated in the next figure, the (absolute value) of the distance between the true and the approximated values is bounded above by $0.006$. The accuracy of the current approximation could be improved by iterating the process further. The collocation method The collocation method takes a different route. Let us remember that we are looking for a function $w$. Instead of solving for the values of $w$ on a grid and then interpolating, why not looking for a function directly? To do so, let us assume that $w$ can reasonably be approximated by a function $\\hat{w}$:\nwith $ \\phi_1(x) $ , $ \\phi_2(x) $,\u0026hellip;, $ \\phi_n(x) $ a set of linearly independent basis functions and $c_1$, $c_2$, \u0026hellip;, $c_n$ $n$ coefficient to be found. Replacing $w(x)$ with $\\hat{w(x)}$ into the functional equation and reorganizing gives:\nThis equation has to hold (almost) exactly at $n$ points (also called nodes): $y_1$, $y_2$, \u0026hellip;, $y_n$:\nThe equation above defines a system of $n$ equation with as many unknown, which can be compactly written as: $$ f(\\boldsymbol{c}) = \\boldsymbol{0} $$\nNewton or quasi-Newton can be used to solve for the root of $f$. In the code that follows, I use Broyden\u0026rsquo;s method. Let us illustrate this technique using a Chebychev polynomial basis and Chebychev nodes. In doing so, we avoid Runge\u0026rsquo;s phenomenon associated with a uniform grid.\nImplementation using CompEcon #--------------------------------------------- # Julien Pascal # Solve the stochastic optimal growth problem # using the collocation method #--------------------------------------------- using QuantEcon using Optim using CompEcon using PyPlot using Interpolations type optGrowthCollocation w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool order_approximation::Int64 #number of element in the functional basis along each dimension functional_basis_type::String #type of functional basis fspace::Dict{Symbol,Any} #functional basis fnodes::Array{Float64,1} #collocation nodes residual::Array{Float64,1} #vector of residual. Should be close to zero a::Array{Float64,1} #polynomial coefficients w_func::Function end ##################################### # Function that finds a solution # to f(x) = 0 # using Broyden's \u0026quot;good\u0026quot; method # and a simple backstepping procedure as described # in Miranda and Fackler (2009) # # input : # -------- # * x0: initial guess for the root # * f: function in f(x) = 0 # * maxit: maximum number of iterations # * tol: tolerance level for the zero # * fjavinc: initial inverse of the jacobian. If not provided, then inverse of the # Jacobian is calculated by finite differences # * maxsteps: maximum number of backsteps # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian # # output : # -------- # * x: one zero of f # * it: number of iterations necessary to reached the solution # * fjacinv: pseudo jacobian at the last iteration # * fnorm: norm f(x) at the last iteration # ####################################### function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0)); maxsteps = 5, recaculateJacobian = 1) println(\u0026quot;a0 = $(x0)\u0026quot;) fnorm = tol*2 it2 = 0 #to re-initialize the jacobian ################################ #initialize guess for the matrix ################################ fjacinv_function = x-\u0026gt; Calculus.finite_difference_jacobian(f, x) #fjacinv_function = x -\u0026gt; ForwardDiff.gradient(f, x) # If the user do not provide an initial guess for the jacobian # One is calculated using finite differences. if fjacinv == eye(length(x0)) ################################################ # finite differences to approximate the Jacobian # at the initial value # this is slow. Seems to improve performances # when x0 is of high dimension. println(\u0026quot;Calculating the Jacobian by finite differences\u0026quot;) #@time fjacinv = Calculus.finite_difference_jacobian(f, x0) @time fjacinv = fjacinv_function(x0) println(\u0026quot;Inverting the Jacobian\u0026quot;) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end println(\u0026quot;Done\u0026quot;) else println(\u0026quot;Using User's input as a guess for the Jacobian.\u0026quot;) end fval = f(x0) for it=1:maxit it2 +=1 #every 30 iterations, reinitilize the jacobian if mod(it2, recaculateJacobian) == 0 println(\u0026quot;Re-calculating the Jacobian\u0026quot;) fjacinv = fjacinv_function(x0) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end end println(\u0026quot;it = $(it)\u0026quot;) fnorm = norm(fval) if fnorm \u0026lt; tol println(\u0026quot;fnorm = $(fnorm)\u0026quot;) return x0, it, fjacinv, fnorm end d = -(fjacinv*fval) fnormold = Inf ######################## # Backstepping procedure ######################## for backstep = 1:maxsteps if backstep \u0026gt; 1 println(\u0026quot;backstep = $(backstep-1)\u0026quot;) end fvalnew = f(x0 + d) fnormnew = norm(fvalnew) if fnormnew \u0026lt; fnorm break end if fnormold \u0026lt; fnormnew d=2*d break end fnormold = fnormnew d = d/2 end #################### #################### x0 = x0 + d fold = fval fval = f(x0) u = fjacinv*(fval - fold) #Update the pseudo Jacobian: fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u)) println(\u0026quot;a$(it) = $(x0)\u0026quot;) println(\u0026quot;fnorm = $(fnorm)\u0026quot;) if isnan.(x0) == trues(length(x0)) println(\u0026quot;Error. a$(it) = NaN for each component\u0026quot;) x0 = zeros(length(x0)) return x0, it, fjacinv, fnorm end end println(\u0026quot;In function find_broyden\\n\u0026quot;) println(\u0026quot;Maximum number of iterations reached.\\n\u0026quot;) println(\u0026quot;No convergence.\u0026quot;) println(\u0026quot;Returning fnorm = NaN as a solution\u0026quot;) fnorm = NaN return x0, maxit, fjacinv, fnorm end function optGrowthCollocation(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true, order_approximation = 40, functional_basis_type = \u0026quot;chebychev\u0026quot;, ) grid_y = collect(linspace(1e-5, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log.(c) # Production f(k) = k^α el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) lower_bound_support = minimum(grid_y) upper_bound_support = maximum(grid_y) n_functional_basis = [order_approximation] if functional_basis_type == \u0026quot;chebychev\u0026quot; fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support) elseif functional_basis_type == \u0026quot;splines\u0026quot; fspace = fundefn(:spli, n_functional_basis, lower_bound_support, upper_bound_support, 1) elseif functional_basis_type == \u0026quot;linear\u0026quot; fspace = fundefn(:lin, n_functional_basis, lower_bound_support, upper_bound_support) else error(\u0026quot;functional_basis_type has to be either chebychev, splines or linear.\u0026quot;) end fnodes = funnode(fspace)[1] residual = zeros(size(fnodes)[1]) a = ones(size(fnodes)[1]) w = ones(size(fnodes)[1]) Tw = ones(size(fnodes)[1]) σ = ones(size(fnodes)[1]) w_func = x-\u0026gt; x optGrowthCollocation( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, order_approximation, functional_basis_type, fspace, fnodes, residual, a, w_func ) end function residual!(model::optGrowthCollocation) model.w_func = y -\u0026gt; funeval(model.a, model.fspace, [y])[1][1] function objective(c, y) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end # Loop over nodes for i in 1:size(model.fnodes)[1] y = model.fnodes[i,1] res = optimize(c -\u0026gt; objective(c, y), 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = model.w_func(y) model.residual[i] = - model.w[i] + model.Tw[i] end end model = optGrowthCollocation(functional_basis_type = \u0026quot;chebychev\u0026quot;) residual!(model) function solve_optgrowth!(model::optGrowthCollocation; tol=1e-6, max_iter=500) # Initialize guess for coefficients # by giving the \u0026quot;right shape\u0026quot; # --------------------------------- function objective_initialize!(x, model) #update polynomial coeffficients model.a = copy(x) model.w_func = y -\u0026gt; funeval(model.a, model.fspace, [y])[1][1] return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1])) end minx, iterations, Jac0, fnorm = find_broyden(model.a, x -\u0026gt; objective_initialize!(x, model), max_iter, tol) # Solving the model by collocation # using the initial guess calculated above #----------------------------------------- function objective_residual!(x, model) #update polynomial coeffficients model.a = copy(x) #calculate residual residual!(model) return abs.(model.residual) end minx, iterations, Jac, fnorm = find_broyden(model.a, x -\u0026gt; objective_residual!(x, model), max_iter, tol) end #----------------------------------- # Solve by collocation #----------------------------------- @time solve_optgrowth!(model) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Approximated (collocation)\u0026quot;, linestyle = \u0026quot;--\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Exact value\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;collocation.png\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](-0.05, 0.05) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;error_collocation.png\u0026quot;)  Output The following figure shows the exact function and the one calculated using the collocation method. In terms of accuracy, both VFI and the collocation method generate reliable results.\nIn terms of speed, it turns out that the value function iteration implementation is much faster. One reason seems to be the efficiency associated with the package Interpolations: it is more than 20 times faster to evaluate $w$ using the package Interpolations rather than using the package CompEcon:\n#using Interpolations #-------------------- @time for i=1:1000000 model.w_func.(model.grid[1]) end #0.230861 seconds (2.00 M allocations: 30.518 MiB, 1.28% gc time) #using CompEcon #-------------- @time for i=1:1000000 model.w_func.(model.grid[1]) end # 4.998902 seconds (51.00 M allocations: 3.546 GiB, 13.39% gc time)  Implementation using ApproxFun Significant speed-gains can be obtained by using the package ApproxFun, as illustrated by the code below. Computing time is divided by approximately $5$ compared to the implementation using CompEcon. Yet, the value function iteration implementation is still the fastest one. One bottleneck seems to be the calculation of the Jacobian by finite differences when using Broyden\u0026rsquo;s method. It is likely that using automatic differentiation would further improve results.\n#--------------------------------------------- # Julien Pascal # Solve the stochastic optimal growth problem # using the collocation method # Implementation using ApproxFun #--------------------------------------------- using QuantEcon using Optim using CompEcon using PyPlot using Interpolations using FileIO using ApproxFun using ProfileView type optGrowthCollocation w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool order_approximation::Int64 #number of element in the functional basis along each dimension functional_basis_type::String #type of functional basis fspace::Dict{Symbol,Any} #functional basis fnodes::Array{Float64,1} #collocation nodes residual::Array{Float64,1} #vector of residual. Should be close to zero a::Array{Float64,1} #polynomial coefficients fApprox::ApproxFun.Fun{ApproxFun.Chebyshev{ApproxFun.Segment{Float64},Float64},Float64,Array{Float64,1}} w_func::Function tol::Float64 end ##################################### # Function that find a solution # to f(x) = 0 # using Broyden's \u0026quot;good\u0026quot; method # and simple backstepping procedure as described # in Miranda and Fackler (2009) # # input : # -------- # * x0: initial guess for the root # * f: function in f(x) = 0 # * maxit: maximum number of iterations # * tol: tolerance level for the zero # * fjavinc: initial inverse of the jacobian. If not provided, then inverse of the # Jacobian is calculated by finite differences # * maxsteps: maximum number of backsteps # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian # # output : # -------- # * x: one zero of f # * it: number of iterations necessary to reached the solution # * fjacinv: pseudo jacobian at the last iteration # * fnorm: norm f(x) at the last iteration # ####################################### function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0)); maxsteps = 5, recaculateJacobian = 1) println(\u0026quot;a0 = $(x0)\u0026quot;) fnorm = tol*2 it2 = 0 #to re-initialize the jacobian ################################ #initialize guess for the matrix ################################ # with Calculus #-------------- fjacinv_function = x-\u0026gt; Calculus.finite_difference_jacobian(f, x) # If the user do not provide an initial guess for the jacobian # One is calculated using finite differences. if fjacinv == eye(length(x0)) ################################################ # finite differences to approximate the Jacobian # at the initial value # this is slow. Seems to improve performances # when x0 is of high dimension. println(\u0026quot;Calculating the Jacobian by finite differences\u0026quot;) #@time fjacinv = Calculus.finite_difference_jacobian(f, x0) @time fjacinv = fjacinv_function(x0) println(\u0026quot;Inverting the Jacobian\u0026quot;) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end println(\u0026quot;Done\u0026quot;) else println(\u0026quot;Using User's input as a guess for the Jacobian.\u0026quot;) end fval = f(x0) for it=1:maxit it2 +=1 #every 30 iterations, reinitilize the jacobian if mod(it2, recaculateJacobian) == 0 println(\u0026quot;Re-calculating the Jacobian\u0026quot;) fjacinv = fjacinv_function(x0) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end end println(\u0026quot;it = $(it)\u0026quot;) fnorm = norm(fval) if fnorm \u0026lt; tol println(\u0026quot;fnorm = $(fnorm)\u0026quot;) return x0, it, fjacinv, fnorm end d = -(fjacinv*fval) fnormold = Inf ######################## # Backstepping procedure ######################## for backstep = 1:maxsteps if backstep \u0026gt; 1 println(\u0026quot;backstep = $(backstep-1)\u0026quot;) end fvalnew = f(x0 + d) fnormnew = norm(fvalnew) if fnormnew \u0026lt; fnorm break end if fnormold \u0026lt; fnormnew d=2*d break end fnormold = fnormnew d = d/2 end #################### #################### x0 = x0 + d fold = fval fval = f(x0) u = fjacinv*(fval - fold) #Update the pseudo Jacobian: fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u)) println(\u0026quot;a$(it) = $(x0)\u0026quot;) println(\u0026quot;fnorm = $(fnorm)\u0026quot;) if isnan.(x0) == trues(length(x0)) println(\u0026quot;Error. a$(it) = NaN for each component\u0026quot;) x0 = zeros(length(x0)) return x0, it, fjacinv, fnorm end end println(\u0026quot;In function find_broyden\\n\u0026quot;) println(\u0026quot;Maximum number of iterations reached.\\n\u0026quot;) println(\u0026quot;No convergence.\u0026quot;) println(\u0026quot;Returning fnorm = NaN as a solution\u0026quot;) fnorm = NaN return x0, maxit, fjacinv, fnorm end function optGrowthCollocation(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true, order_approximation = 40, functional_basis_type = \u0026quot;chebychev\u0026quot;, ) grid_y = collect(linspace(1e-4, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log.(c) # Production f(k) = k^α el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) lower_bound_support = minimum(grid_y) upper_bound_support = maximum(grid_y) n_functional_basis = [order_approximation] if functional_basis_type == \u0026quot;chebychev\u0026quot; fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support) else error(\u0026quot;functional_basis_type has to be \\\u0026quot;chebychev\\\u0026quot; \u0026quot;) end fnodes = funnode(fspace)[1] residual = zeros(size(fnodes)[1]) a = ones(size(fnodes)[1]) tol = 0.001 fApprox = (Fun(Chebyshev((minimum(grid_y))..(maximum(grid_y))), a)) #fApprox = (Fun(Chebyshev(0..maximum(model.grid)), a)) w_func = x-\u0026gt; fApprox(x) w = ones(size(fnodes)[1]) Tw = ones(size(fnodes)[1]) σ = ones(size(fnodes)[1]) optGrowthCollocation( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, order_approximation, functional_basis_type, fspace, fnodes, residual, a, fApprox, w_func, tol ) end function residual!(model::optGrowthCollocation) model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a)) model.w_func = x-\u0026gt; model.fApprox(x) function objective(c, y) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end # Loop over nodes for i in 1:size(model.fnodes)[1] y = model.fnodes[i,1] res = optimize(c -\u0026gt; objective(c, y), 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = model.w_func(y) model.residual[i] = - model.w[i] + model.Tw[i] end end function solve_optgrowth!(model::optGrowthCollocation; tol=1e-6, max_iter=500) # Initialize guess for coefficients # by giving the \u0026quot;right shape\u0026quot; # --------------------------------- function objective_initialize!(x, model) #update polynomial coeffficients model.a = copy(x) model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a)) model.w_func = x-\u0026gt; model.fApprox(x) return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1])) end minx, iterations, Jac0, fnorm = find_broyden(model.a, x -\u0026gt; objective_initialize!(x, model), max_iter, tol) # Solving the model by collocation # using the initial guess calculated above #----------------------------------------- function objective_residual!(x, model) #update polynomial coeffficients model.a = copy(x) #calculate residual residual!(model) return abs.(model.residual) end minx, iterations, Jac, fnorm = find_broyden(model.a, x -\u0026gt; objective_residual!(x, model), max_iter, tol, recaculateJacobian = 1) end #----------------------------------- # Solve by collocation #----------------------------------- model = optGrowthCollocation(functional_basis_type = \u0026quot;chebychev\u0026quot;) @time solve_optgrowth!(model) # 15.865923 seconds (329.12 M allocations: 4.977 GiB, 5.55% gc time) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;approximate value function\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;true value function\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](- 0.05, 0.05) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;)  ","date":1512651029,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512651029,"objectID":"616acd9b09eaac5b85a05cdc2a187df2","permalink":"https://julienpascal.github.io/post/collocation/","publishdate":"2017-12-07T13:50:29+01:00","relpermalink":"/post/collocation/","section":"post","summary":"A large class of economic models involves solving for functional equations of the form:\nA well known example is the stochastic optimal growth model. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period\u0026rsquo;s output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.","tags":[],"title":"Solving Bellman Equations by the Collocation Method","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" Dynare is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see Winberry and Ragot for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.\nA simple model The economy is composed of a representative agent who maximizes his expected discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,\u0026hellip;,\\infty$ $$ \\sum_{t=1}^{+\\infty}\\big(\\frac{1}{1+\\rho}\\big)^{t-1} E_t\\Big[log(C_t)-\\frac{L_t^{1+\\gamma}}{1+\\gamma}\\Big] $$\nsubject to the constraint\n$$ K_t = K_t{-_1} (1-\\delta) + w_t L_t + r_t K_t-_1 - C_t $$\nwhere\n $\\rho \\in (0,\\infty)$ is the rate of time preference $\\gamma \\in (0,\\infty)$ is a labor supply parameter $w_t$ is real wage $r_t$ is the real rental rate $K_t$ is capital at the end of the period $\\delta \\in (0,1)$ is the capital depreciation rate  The production function writes \\begin{equation} Y_t = A_t K_t-_1^\\alpha \\Big((1+g)^t \\Big)^{1-\\alpha} \\end{equation}\nwhere\n $g \\in (0,\\infty)$ is the growth rate of production $\\alpha$ and $\\beta$ are technology parameters $A_t$ is a technological shock that follows and $AR(1)$ process  \\begin{equation} \\log(A_t) = \\lambda log(A_t-_1) + e_t\\end{equation}\nwith $e_t$ an i.i.d zero-mean normally distributed error term with standard deviation $\\sigma_1$ and $\\lambda \\in (0,1)$ a parameter governing the persistence of shocks.\nFirst Order conditions The F.O.C.s of the (stationarized) model are\n$$ \\frac{1}{\\hat{C_t}} = \\frac{1}{1+\\rho} E_t \\Big( \\frac{r_t+_1 + 1 - \\delta}{\\hat{C}_t+_1 (1+g)} \\Big)$$\n$$ L_t^\\gamma = \\frac{\\hat{w}_t}{\\hat{C}_t}$$\n$$ r_t = \\alpha A_t \\Big(\\frac{\\hat{K}_t-_1}{1+g}\\Big)^{\\alpha-1}L_t^{1-\\alpha}$$\n$$ \\hat{w}_t = (1-\\alpha) A_t \\Big(\\frac{\\hat{K}_t-_1}{1+g}\\Big)^{\\alpha}L_t^{-\\alpha} $$\n$$ \\hat{K}_t + \\hat{C}_t = \\frac{\\hat{K}_t-_1}{1+g} (1-\\delta) + A_t \\Big( \\frac{\\hat{K}_t-_1}{1+g} \\Big)^{\\alpha} L_t^{1-\\alpha} $$\nwith $$ \\hat{C}_t = \\frac{C_t}{(1+g)^t}$$ $$ \\hat{K}_t = \\frac{K_t}{(1+g)^t}$$ $$ \\hat{w}_t = \\frac{w_t}{(1+g)^t}$$\nSolving and simulating the model in Dynare In Dynare, one has first to specify the endogenous variables (var), exogenous variables (varexo), and the parameters\nvar C K L w r A; varexo e; parameters rho delta gamma alpha lambda g; alpha = 0.33; delta = 0.1; rho = 0.03; lambda = 0.97; gamma = 0; g = 0.015;  In a second step, the F.O.C.s of the model has to be expressed using the command model\nmodel; 1/C=1/(1+rho)*(1/(C(+1)*(1+g)))*(r(+1)+1-delta); L^gamma = w/C; r = alpha*A*(K(-1)/(1+g))^(alpha-1)*L^(1-alpha); w = (1-alpha)*A*(K(-1)/(1+g))^alpha*L^(-alpha); K+C = (K(-1)/(1+g))*(1-delta) +A*(K(-1)/(1+g))^alpha*L^(1-alpha); log(A) = lambda*log(A(-1))+e; end;  The user must provide the analytical solution for the steady state of the model using the command steady_state_model. The command steady solves for the steady state values of the model\nsteady_state_model; A = 1; r = (1+g)*(1+rho)+delta-1; L = ((1-alpha)/(r/alpha-delta-g))*r/alpha; K = (1+g)*(r/alpha)^(1/(alpha-1))*L; C = (1-delta)*K/(1+g) +(K/(1+g))^alpha*L^(1-alpha)-K; w = C; end; steady;  The command shocks defines the type of shock to be simulated\nshocks; var e; stderr 0.01; end; check;  A first order expansion around the steady state is obtained using the command stoch_simul(order=1) This function computes impulse response functions (IRF) and returns various descriptive statistics (moments, variance decomposition, correlation and autocorrelation coefficients)\nThe IRF produced by Dynare should be pretty similar to the following graph: ","date":1501332263,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501332263,"objectID":"d5946fc40e4b8cec00c21670c8ac01d1","permalink":"https://julienpascal.github.io/post/rbc_dynare/","publishdate":"2017-07-29T13:44:23+01:00","relpermalink":"/post/rbc_dynare/","section":"post","summary":"Dynare is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see Winberry and Ragot for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.\nA simple model The economy is composed of a representative agent who maximizes his expected discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,\u0026hellip;,\\infty$ $$ \\sum_{t=1}^{+\\infty}\\big(\\frac{1}{1+\\rho}\\big)^{t-1} E_t\\Big[log(C_t)-\\frac{L_t^{1+\\gamma}}{1+\\gamma}\\Big] $$","tags":[],"title":"Solving a simple RBC model in Dynare","type":"post"},{"authors":["Christian Daude","Julien Pascal"],"categories":null,"content":"","date":1488898314,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488898314,"objectID":"48aece6ec07e069260731c2bbb63b8e5","permalink":"https://julienpascal.github.io/publication/efficiencycontestability/","publishdate":"2017-03-07T15:51:54+01:00","relpermalink":"/publication/efficiencycontestability/","section":"publication","summary":"This paper explores some of the potential determinants of efficiency and contestability in the banking systems of major emerging countries, using a sample of 24 countries over the period 2004 -2013. Efficiency is estimated using both stochastic frontier and data envelopment analyses. Market contestability is measured with the Panzar-Rosse H-statistic. Potential drivers of efficiency and market contestability are then discussed.","tags":["Banking","Efficiency","Competition","Emerging markets"],"title":"Efficiency and contestability in emerging market banking systems","type":"publication"},{"authors":null,"categories":null,"content":" Rental Housing Market and Directed Search Introduction Context  Rents have surged in big cities worldwide   The median American rent payment rose 61% in real terms between 1960 and 2016 while the median renter’s income grew by 5%\n  Housing shortages and rent control in many major cities  Questions  Are rent control legislations effective? Do we observe asymmetric bargaining power between renters and landlords?  Data Data Sources  Web scraping: online ads posted on the major platforms for the Paris market (PAP, Leboncoin, etc.) between April and May 2019 I observe the two sides of the rental housing market:  supply: apartment features, text description and photos demand: number of contacts received by the renters   Descriptive statistics City and Month Apartment features Aesthetic score  Aesthetic score calculated from photo data I use a convolutional neural network based on the work of Talebi and Milanfar (2018)  Aesthetic score What features are important to predict rent? $$y_{i} = \\alpha + \\boldsymbol{x_{i}}^{\u0026lsquo;} \\boldsymbol{\\beta} + \\varepsilon_{i}$$\n $\\boldsymbol{x_{i}}$: apartment features $\\varepsilon_{i}$: error term  Rent and apartment characteristics Residual price dispersion and number of contacts Number of contacts per ad  I observe the number of contacts per ad Data is truncated: visible only when number of contacts $\\geq$ 10  Number of contacts per ad Deviation from predicted price and number of contacts per ad Truncated regression  $y_i = \\boldsymbol{x_{i}}^{\u0026lsquo;} \\boldsymbol{\\beta} + \\varepsilon_{i}$ $y_i$ in the sample if $y_i \\geq c$   Interpretation: an apartment €100 below its market price attracts on average 1.5 more contacts  Summary  Cheaper (than expected) apartments attract more renters A simple directed search model predicts this pattern  WIP  I observe price movements for the same apartment across time Common strategy: (1) over-pricing first -\u0026gt; (2) decrease to market price Learning or excess bargaining power for renters?  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a27097b0096c805fd7c11d95ac06566","permalink":"https://julienpascal.github.io/slides/rental-market/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/rental-market/","section":"slides","summary":"Rental Housing Market and Directed Search Introduction Context  Rents have surged in big cities worldwide   The median American rent payment rose 61% in real terms between 1960 and 2016 while the median renter’s income grew by 5%\n  Housing shortages and rent control in many major cities  Questions  Are rent control legislations effective? Do we observe asymmetric bargaining power between renters and landlords?  Data Data Sources  Web scraping: online ads posted on the major platforms for the Paris market (PAP, Leboncoin, etc.","tags":null,"title":"Rental Housing Market and Directed Search","type":"slides"},{"authors":null,"categories":null,"content":" Spatial Equilibrium and Commuting Costs Introduction  The \u0026ldquo;Forfait Toutes Zones\u0026rdquo; (FTZ) entered into force on September 2015 in the Paris area The costs of using public transports decreased for workers living in the outskirts of Paris (zones 3-5) Using the spatial discontinuity generated by the reform, I analyze the links between commuting costs and local employment outcomes  Data Data Sources  Monthly # of workers registered to Pole Emploi at the municipality level: Pole Emploi Demographic and economic characteristics of municipalities and IRIS: INSEE House price index at the municipality level: Chambre des Notaires de Paris Fare area at the IRIS level: Vianavigo Behavior of travelers on the public transport network: STIF  Cost change by fare area Average cost difference by fare area Fare area at the IRIS level Econometrics Main specification $$y_{it} = \\alpha_i + \\gamma_t + \\beta \\times \\delta_{i,t} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t}$$\n $\\alpha_i$: municipality fixed effects $\\gamma_t$: time fixed effects $\\delta_{i,t}$: indicator = 1 if treated and post-reform $\\boldsymbol{x_{i,t}}$: control variables $\\varepsilon_{i,t}$: error term  Other spefications Time-varying impacts $$ y_{it} = \\alpha_i + \\gamma_t + \\sum_{y=2015}^{2019} \\beta_{y} \\times \\delta_{i,t} \\times \\delta_{y} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t} $$\nDifferent \u0026ldquo;intensity of treatment\u0026rdquo; $$ y_{it} = \\alpha_i + \\gamma_t + \\beta \\times z_{i,t} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t} $$\n$$ y_{it} = \\alpha_i + \\gamma_t + \\sum_{y=2015}^{2019} \\beta_{y} \\times z_{i,t} \\times \\delta_{y} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t} $$\nSpatial RDD  For results to be valid, the common trend assumption needs to hold To limit the impact of unobservable characteristics on local employment outcomes, I select municipalities close to the border separating fare areas (zones 2 and 3 in the baseline)  Results Impact of the FTZ on the total # of workers registered to Pole Emploi Impact of the FTZ on the # of workers registered to Pole Emploi in category A Model Overview  Partial equilibrium model inspired by Brueckner et al. (1999), Postel-Vinay and Robin (2002), Wasmer and Zenou (2002) The model helps in understanding why a decrease in commuting costs may increase local employment  Model Main equation Impact of the FTZ reform in municipality $l$ on:\n job search efficiency the reservation wage incentives to relocate  Conclusion  The \u0026ldquo;Forfait Toutes Zones\u0026rdquo; (FTZ) decreased commuting costs for workers living in the outskirts of Paris (zones 3-5) Using the spatial discontinuity generated by the reform, I find that the FTZ created jobs for long-term unemployed workers in zones 3-5  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1805de5ca616d11e2bd305f0ac551778","permalink":"https://julienpascal.github.io/slides/spatial-eq/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/spatial-eq/","section":"slides","summary":"Spatial Equilibrium and Commuting Costs Introduction  The \u0026ldquo;Forfait Toutes Zones\u0026rdquo; (FTZ) entered into force on September 2015 in the Paris area The costs of using public transports decreased for workers living in the outskirts of Paris (zones 3-5) Using the spatial discontinuity generated by the reform, I analyze the links between commuting costs and local employment outcomes  Data Data Sources  Monthly # of workers registered to Pole Emploi at the municipality level: Pole Emploi Demographic and economic characteristics of municipalities and IRIS: INSEE House price index at the municipality level: Chambre des Notaires de Paris Fare area at the IRIS level: Vianavigo Behavior of travelers on the public transport network: STIF  Cost change by fare area Average cost difference by fare area Fare area at the IRIS level Econometrics Main specification $$y_{it} = \\alpha_i + \\gamma_t + \\beta \\times \\delta_{i,t} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t}$$","tags":null,"title":"Spatial Equilibrium and Commuting Costs","type":"slides"}]