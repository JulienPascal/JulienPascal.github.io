[{"authors":["admin"],"categories":null,"content":"I am an Economist at the Central Bank of Luxembourg. I hold a PhD in Economics from Sciences Po.\nThe views expressed on this website are my own and do not necessarily represent the views of my employer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://julienpascal.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am an Economist at the Central Bank of Luxembourg. I hold a PhD in Economics from Sciences Po.\nThe views expressed on this website are my own and do not necessarily represent the views of my employer.","tags":null,"title":"Julien Pascal, PhD","type":"author"},{"authors":[],"categories":[],"content":" Introduction Solving economic models involves (i) finding the optimal response of individuals given the state of the economy (the policy functions); (ii) given the policy functions, simulating the model. While usually one must show great ingenuity and creativity for the former, the latter is often seen as trivial and not even mentioned. However, in this notebook I describe a simulation procedure that deserves to be advertised. Namely, I describe Young\u0026rsquo;s method (2010) to simulate a large number (infinity) of individuals. The code can be downloaded here.\nWhy should we care? In economies with heterogeneous agents, often there is no such thing as a representative agent. Generally, one must follow a large number of individuals. For instance, one may be interested in knowing how the average wage responds to an increase in labor productivity. If workers are different (in terms of skills, experience, firms in which they work, etc.), one must take into consideration changes in individuals\u0026rsquo; wages to determine how the average wage moves.\nThe method The method avoids simulating a panel of agents. Instead, the idea is to simulate directly the distribution. In practice, one chooses a grid $[w_1, w_2, \u0026hellip;, w_N]$\nIf a measure $m$ of workers choose to consume $w$, with $w \\in [w_{n}, w_{n+1}]$, then the mass assigned to the grid point $w_{n}$ is equal to $m \\times p$ and the mass assigned to the grid point $w_{n+1}$ is $m \\times (1 - p)$ with\n$$ p = 1 - \\frac{w - w_{n}}{w_{n+1} - w_{n}} $$\nIf $w$ is very close to $w_{n}$, then most of the mass $m$ will be assigned to this point. In the limit case, if $w$ is equal to $w_{n}$, $100$ percent of the mass is assigned to $w_{n}$.\nSimple, right? The code below is an implementation of Young\u0026rsquo;s method (2010) using Julia.\nImplementation Preliminaries Let\u0026rsquo;s first start by loading a few packages and declaring auxiliaries functions. In particular, given a value $w$ we need a function that returns the closest value $w_k$, where $w_k$ is picked from given grid $w_1, w_2, \u0026hellip;, w_N$\nusing Plots using Distributions using StatsBase # Matlab like function function linspace(z_start::Real, z_end::Real, z_n::Int64) return collect(range(z_start,stop=z_end,length=z_n)) end # Not my function. Credit to: https://discourse.julialang.org/t/findnearest-function/4143/4 function closest_index(a::Vector,x::Real) if isempty(a) == true error(\u0026quot;xGrid is empty in function closest_index.\u0026quot;) end if isnan(x) == true error(\u0026quot;val is NaN in function closest_index.\u0026quot;) end idx = searchsortedfirst(a,x) if (idx==1); return idx; end if (idx\u0026gt;length(a)); return length(a); end if (a[idx]==x); return idx; end if (abs(a[idx]-x) \u0026lt; abs(a[idx-1]-x)) return idx else return idx-1 end end # Returns best index and best value function closest_value_and_index(xGrid::Vector, val::Real) # get index ibest = closest_index(xGrid, val) # Return best value on grid, and the corresponding index return xGrid[ibest], ibest end  Approximate a distribution with a single mass point To warm up, let\u0026rsquo;s see how the method works when the true underlying distribution is constituted of a single mass point.\n## True Value w = 2.5 #True value mass = 1.0 #Mass at the true value ## Approximation nW=10 #Number of grid points w_grid=linspace(0.0, 4.0, nW) #Location of grid points hw_grid=zeros(nW); #Initialization w_min = minimum(w_grid) #Upper bound for the grid w_max = maximum(w_grid) #Lower bound for the grid nW = length(w_grid) #Number of points on the grid # Project true value on the grid: (wValue_proj, wIndex_proj) = closest_value_and_index(w_grid, w) # To store the location of the value below and above the true value: wIndex_below = 0 wIndex_above = 0 # If the true value is above the projection if w \u0026gt;= wValue_proj wIndex_below = wIndex_proj wIndex_above = wIndex_proj + 1 # If the true value is below the projection elseif w \u0026lt; wValue_proj wIndex_below = wIndex_proj -1 wIndex_above = wIndex_proj end # Boundary cases if wIndex_proj == 1 wIndex_below = 1 wIndex_above = 2 elseif wIndex_proj == nW wIndex_below = nW - 1 wIndex_above = nW end # Special case 1: w \u0026lt; w_min if w \u0026lt;= w_min p = 1 elseif w \u0026gt;= w_max # Special case 2: w \u0026gt; w_max p = 0 else p = 1.0 - ((w - w_grid[wIndex_below])/(w_grid[wIndex_above] - w_grid[wIndex_below])) p = min(1.0, max(0.0, p)) end # Spread the mass: # 1. Point below hw_grid[wIndex_below] += p*mass # 2. Point above: hw_grid[wIndex_above] += (1.0 - p)*mass; p0 =bar(w_grid, hw_grid, label = \u0026quot;Mass on grid\u0026quot;) plot!(p0, [w], seriestype = :vline, label=\u0026quot;True value\u0026quot;, linewidth = 4)  Notes: This graph shows the true distribution (in orange) and the approximation (in blue).\nBecause the true value is not a point of our grid (that would be a miracle), the mass is spread between the two closest points. However, we still get the mean right. We will see below that this property extends to distributions with supports constituted of several points:\nprintln(\u0026quot;True mean $(w)\u0026quot;) println(\u0026quot;Approximate mean $(round(mean(w_grid, weights(hw_grid./sum(hw_grid))), digits = 4))\u0026quot;)  True mean 2.5 Approximate mean 2.5  Approximate a multi-point distribution Now, let\u0026rsquo;s see how well Young\u0026rsquo;s method (2010) works when the true underlying distribution has positive mass for a finite number of points. To keep things pretty, let\u0026rsquo;s assume that the true distribution has the shape of a Normal distribution, but we truncate the tails.\nNw_true = 10 #Number of points with positive mass w_true_lb = 1.0 #lower bound w_true_ub = 3.0 #upper bound w_true = linspace(w_true_lb, w_true_ub, Nw_true) d = truncated(Normal((w_true_lb + w_true_ub)/2), w_true_lb, w_true_ub) mass_true = pdf.(d, w_true) mass_true = mass_true./sum(mass_true); p0 = bar(w_true, mass_true, label=\u0026quot;Mass true value\u0026quot;) display(p0)  Notes: This graph shows the mass for each point on the (true) grid.\nw_min = minimum(w_grid) w_max = maximum(w_grid) nW = length(w_grid) hw_grid=zeros(nW); for (wIndexTrue, w) in enumerate(w_true) mass = mass_true[wIndexTrue] #mass # Project true value on the grid: (wValue_proj, wIndex_proj) = closest_value_and_index(w_grid, w) # To store the location of the value below and above the true value: wIndex_below = 0 wIndex_above = 0 # If the true value is above the projection if w \u0026gt;= wValue_proj wIndex_below = wIndex_proj wIndex_above = wIndex_proj + 1 # If the true value is below the projection elseif w \u0026lt; wValue_proj wIndex_below = wIndex_proj -1 wIndex_above = wIndex_proj end # Boundary cases if wIndex_proj == 1 wIndex_below = 1 wIndex_above = 2 elseif wIndex_proj == nW wIndex_below = nW - 1 wIndex_above = nW end # Special case 1: w \u0026lt; w_min if w \u0026lt;= w_min p = 1 elseif w \u0026gt;= w_max # Special case 2: w \u0026gt; w_max p = 0 else p = 1.0 - ((w - w_grid[wIndex_below])/(w_grid[wIndex_above] - w_grid[wIndex_below])) p = min(1.0, max(0.0, p)) end p = 1.0 - ((w - w_grid[wIndex_below])/(w_grid[wIndex_above] - w_grid[wIndex_below])) p = min(1.0, max(0.0, p)) # Spread the mass: # 1. Point below hw_grid[wIndex_below] += p*mass # 2. Point above: hw_grid[wIndex_above] += (1.0 - p)*mass end bar(w_grid, hw_grid, label=\u0026quot;Mass on grid\u0026quot;) bar!(w_true, mass_true, label=\u0026quot;Mass true value\u0026quot;)  Notes: This graph shows the true distribution (in orange) and the approximation (in blue).\nA good property of Young\u0026rsquo;s method (2010) is that, as long as the grid is wide enough to so that true values fall within it, the mean of the true underlying distribution is preserved:\nprintln(\u0026quot;True mean $(round(mean(w_true, weights(mass_true./sum(mass_true))), digits = 4))\u0026quot;) println(\u0026quot;Approximate mean $(round(mean(w_grid, weights(hw_grid./sum(hw_grid))), digits = 4))\u0026quot;)  True mean 2.0 Approximate mean 2.0  However, there are some approximation errors when higher moments are involved such as the variance, or when calculating percentiles. But the finest the grid, the better the approximation gets.\nprintln(\u0026quot;True median $(round(quantile(w_true, weights(mass_true./sum(mass_true)), 0.5), digits = 4))\u0026quot;) println(\u0026quot;Approximate median $(round(quantile(w_grid, weights(hw_grid./sum(hw_grid)), 0.5), digits = 4))\u0026quot;) println(\u0026quot;True variance $(round(var(w_true, weights(mass_true./sum(mass_true))), digits = 4))\u0026quot;) println(\u0026quot;Approximate variance $(round(var(w_grid, weights(hw_grid./sum(hw_grid))), digits = 4))\u0026quot;)  True median 1.9567 Approximate median 1.8519 True variance 0.3465 Approximate variance 0.3836  Using a panel of agents Alternatively, one may use a large number of agents to approximate the true underlying distribution. The idea is that if the number of agents is large enough, the central limit theorem will kick in. The issue is that we need a large number of agents to get the approximation right, as illustrated below:\nnb_agents = 1000 w_agents = rand(d, nb_agents); println(\u0026quot;True mean $(round(mean(w_true, weights(mass_true./sum(mass_true))), digits = 4))\u0026quot;) println(\u0026quot;Approximate mean $(round(mean(w_agents), digits = 4))\u0026quot;) println(\u0026quot;True median $(round(quantile(w_true, weights(mass_true./sum(mass_true)), 0.5), digits = 4))\u0026quot;) println(\u0026quot;Approximate median $(round(quantile(w_agents, 0.5), digits = 4))\u0026quot;) println(\u0026quot;True variance $(round(var(w_true, weights(mass_true./sum(mass_true))), digits = 4))\u0026quot;) println(\u0026quot;Approximate variance $(round(var(w_agents), digits = 4))\u0026quot;) p0 = bar(w_true, mass_true, label=\u0026quot;Mass true value\u0026quot;) p1 = histogram(w_agents, label=\u0026quot;Mass panel\u0026quot;) plot(p0,p1)  True mean 2.0 Approximate mean 1.9955 True median 1.9567 Approximate median 1.9971 True variance 0.3465 Approximate variance 0.2754  Notes: This graph shows the true distribution (on the left) and the approximation (on the right).\nWith 1000 agents, the approximation is not exceptional. Let\u0026rsquo;s try to increase the number of agents. The following plot shows that the approximation gets better as we increase the number of agents, but a very large number of agents is needed:\nw_agents = [] grid_agents = collect(1000:1000:100000) mean_agents = zeros(length(grid_agents)) std_agents = zeros(length(grid_agents)) for (i, nb_agents) in enumerate(grid_agents) push!(w_agents, rand(d, nb_agents)) mean_agents[i] = mean(w_agents[i]) std_agents[i] = std(w_agents[i]) end CI = 1.960.*std_agents./sqrt.(grid_agents); #95% confidence interval. t-test is approximately a z-test because large number of agents. p0 = plot(grid_agents, mean_agents, ribbon = CI, label = \u0026quot;approximate mean\u0026quot;) plot!(p0,[mean(w_true, weights(mass_true./sum(mass_true)))], linetype = :hline, label = \u0026quot;true mean\u0026quot;, linestyle = :dash)  Notes: This graph shows the empirical mean calculated using the panel and the true value, for different sizes of the panel.\nConclusion When possible, simulating directly a distribution instead of using a panel is a good idea. Young\u0026rsquo;s method (2010) allows to do that, while preserving the mean of the true distribution.\nReferences  Young, Eric R. \u0026ldquo;Solving the incomplete markets model with aggregate uncertainty using the Krusell–Smith algorithm and non-stochastic simulations.\u0026rdquo; Journal of Economic Dynamics and Control 34.1 (2010): 36-41.  ","date":1610992402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610992402,"objectID":"2a50282a244fad5ada4896dff430ebf2","permalink":"https://julienpascal.github.io/post/young_2010/","publishdate":"2021-01-18T18:53:22+01:00","relpermalink":"/post/young_2010/","section":"post","summary":"Introduction Solving economic models involves (i) finding the optimal response of individuals given the state of the economy (the policy functions); (ii) given the policy functions, simulating the model. While usually one must show great ingenuity and creativity for the former, the latter is often seen as trivial and not even mentioned. However, in this notebook I describe a simulation procedure that deserves to be advertised. Namely, I describe Young\u0026rsquo;s method (2010) to simulate a large number (infinity) of individuals.","tags":["Macroeconomics","HA models"],"title":"Young's method (2010) to simulate a cross-section","type":"post"},{"authors":[],"categories":[],"content":" Introduction The Bewley-Huggett-Aiyagari-Imohoroğlu economies are the workhorse of modern macroeconomics. In these economies, markets are \u0026ldquo;incomplete\u0026rdquo;. Agents cannot fully insure against risk and decide to self-insure by holding a safe asset to smooth their consumption (see Ljungqvist and Sargent (2018) for a textbook treatment of this topic).\nIn this post, I consider the model of Aiyagari (1994). While the original model abstracts from aggregate fluctuations, Economists have since developed several techniques to simulate out-of-steady-state dynamics for this class of models.\nHere, I use a methodology that is quite general. It is a 3-step procedure, which proceeds as follows:\n Solve for the non-stochastic steady-state Perturbe the model around its non-stochastic steady-state Use the perturbation to calculate out-of-steady-state dynamics  I use the BKM and GenBKM algorithms for step 2 and 3, which means that the only theoretical tool needed is backward induction (i.e. knowing the value tomorrow, what is the value today?).\nWhat are the cons of the methodology presented here? First, the methodology assumes a small aggregate shock. An implicit assumption is that aggregate shocks do not modify the value of the non-stochastic steady-state. Said differently, the cycle does not change the trend, which seems to be violated in some instances. If the shock is large, the value of steady-state may be altered and the methodology presented here is not adequate. An example of a large shock could be the disruption caused by COVID-19.\nThis methodology also fails when the non-stochastic steady-state is not relevant for the dynamic economy. This can problematic in portfolio choice problems, in which portfolios are indeterminate when aggregate uncertainty vanishes (see Coeurdacier et al (2011)).\nOn the pro side, the methodology presented here is fast (orders of magnitude faster than Krusell-Smith (1998)) and transparent. The code can be downloaded here.\nI. The Model In the model of Aiyagari (1994), there is continuum of agents each maximizing an infinite flow of discounted utility:\n$$ E_{0} \\sum_{t=0}^{\\infty} \\beta^t U(c_t) $$\nsubject to the constraint that consumption plus savings in period $t$ (the left hand side of the next equation) is equal to labor earnings plus savings inherited from the last period (the right hand side of the next equation):\n$$ c_t + a_{t+1} = w_t l_t + (1 + r_t) a_t $$\nThe variable $l_t$ captures idiosyncratic risk in labor earnings and could be interpreted as unemployment risk. We also make the assumptions that consumption cannot be negative and that agents cannot borrow more than a certain amount $B$:\n$$ c_t \\geq 0 $$\n$$ a_t \\geq -B $$\nThe behavior of firms can be summarized by a representative firm hiring workers and capital:\n$$ Y_t = z_t K_t^{\\alpha} L_t^{1-\\alpha} $$\nwhere $Y_t$ is total output, $K_t$ is the aggregate capital level and $L_t$ is the aggregate labor supply. The variables $w_t$ and $r_t$ are pinned down each period by the first order conditions (FOCs) of the representative firm. Note that at the non-stochastic steady-state $z_t = z_{SS} = 1$ (by definition) and both $w_t$ and $r_t$ are constant. Importantly, because agents have to take into consideration the value of $w_t$ and $r_t$ when making decisions, the cross-sectional distribution of agents across capital and idiosyncratic states matters (through the FOCs of the representative firm).\nII. Methodology To solve for individual policy functions, I use the endogenous grid method (EGM) of Carroll (2006). The main idea of this method is to start from the end-of-period level of capital. Using the Euler equation, one may recover the beginning-of-period consumption and level of capital without using a root-finding algorithm.\nTo determine the non-stochastic equilibrium, I solve for the fixed-point problem over the aggregate capital level $f(K*) = 0$ using Brent\u0026rsquo;s method.\nTo calculate the response of the economy to one-period unforeseen aggregate shock (an \u0026ldquo;MIT shock\u0026rdquo;), I use a standard backward-forward \u0026ldquo;shooting\u0026rdquo; method:\n holding constant the path of aggregate capital level $(K_t)_{t=1}^{T}$, calculate the policy functions holding constant the policy functions, calculate the path of aggregate capital level $(K_t)_{t=1}^{T}$ repeat until convergence of the path for aggregate capital level $(K_t)_{t=1}^{T}$  To simulate out-of-steady-state dynamics, I use the BKM algorithm, which relies on the assumption that the response of the economy to an aggregate shock $d_t$ is linear with respect to the aggregate state $z_t$:\n$$ d_t = z_t d(1, 0, 0, \u0026hellip;) + z_{t-1}d(0, 1, 0, \u0026hellip;) + z_{t-2}d(0, 0, 1, \u0026hellip;) + \u0026hellip; $$\nor more compactly:\n$$ d_t = \\sum_{k=0}^{+\\infty} z_{t-k} d_{k} $$\nwhere\n$$ d_{1} = d(1,0,0,\u0026hellip;)$$ $$ d_{2} = d(0,1,0,\u0026hellip;)$$ $$ d_{3} = d(0,0,1,\u0026hellip;)$$\nThat is, the evolution of equilibrium variables is a moving average of past shocks. However, by calculating several trajectories after an MIT shock, one sees that linearity assumption is slightly violated. Hence, I use the GenBKM algorithm, which is a refinement of the BKM algorithm taking into consideration these slight violations of linearity.\nIII. Implementation These ideas are implemented using Julia\nversioninfo()  Julia Version 1.3.0 Commit 46ce4d7933 (2019-11-26 06:09 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.1 (ORCJIT, skylake)  II. A Dependencies using Distributions using Plots using DataFrames using Random using ForwardDiff using LinearAlgebra using Interpolations using DataFrames using Optim Random.seed!(1234);  using NBInclude #To load stuct and functions from other notebooks  @nbinclude(\u0026quot;utils.ipynb\u0026quot;) #mutable structs and primitive functions for the model @nbinclude(\u0026quot;RBC.ipynb\u0026quot;) #Aiyagari model, without the borrowing constraint (standard RBC) @nbinclude(\u0026quot;EGM.ipynb\u0026quot;) #implementation of the EGM method @nbinclude(\u0026quot;SteadyState.ipynb\u0026quot;) #to calculate the non-stochastic steady-state @nbinclude(\u0026quot;GenBKM.ipynb\u0026quot;) #to simulate the stochastic model using the GenBKM algorithm  II.B Steady-state Finding the steady-state value of capital Finding the non-stochastic equilibrium is a fixed-point problem over the aggregate capital level $f(K*) = 0$, which can be solved using Brent\u0026rsquo;s method:\np = Params() #struct with model parameters z_ss = 1.0 #aggregate productivity at the non-stochastic steady-state @time oo = optimize(K -\u0026gt; eq_K(K,p), 10, 100, Brent()) #solve for the steady-state value of capital using Brent method K_star = oo.minimizer; println(\u0026quot;Steady-state value of capital K* = $(K_star)\u0026quot;) # Store the optimal policy function at the steady-state g_star, c_star, g_low_star, g_high_star, success_flag= solve_EGM(x-\u0026gt;log(x), x-\u0026gt;log(x), R(K_star, z_ss, p), W(K_star, z_ss, p), p); #solve for policy functions # Store the stationary distribution at the steady-state t_star = make_trans_mat(g_star, p) #generate transition matrix d_star = get_stationary_dist(t_star); #stationary distribution  Visualizing transition probabilities xs = [string(\u0026quot;x\u0026quot;, i) for i = 1:size(t_star,1)] ys = [string(\u0026quot;y\u0026quot;, i) for i = 1:size(t_star,2)] heatmap(t_star, aspect_ratio = 1, color=:plasma, clim=(0., 0.25))  Notes: This graph shows the transition probabilities across capital and idiosyncratic probability states.\nVisualizing convergence toward the steady-state One may visually check that the equilibrium exists and is unique:\n# Solve for the demand and supply of capital for different values of the interest rate K_grid = collect(range(oo.minimizer-0.5, stop=oo.minimizer+0.5, length=20)) K_implied_grid = similar(K_grid) R_grid = similar(K_grid) for (K_index, K_value) in enumerate(K_grid) R_s, W_s = R(K_value, z_ss, p), W(K_value, z_ss, p) #calculate interest rate R and wage W gg, c_star, g_low, g_high, success_flag= solve_EGM(x -\u0026gt; log(x), x -\u0026gt; 2*log(x), R_s, W_s, p) #solve for policy functions tt = make_trans_mat(gg, p) #generate transition matrix dd = get_stationary_dist(tt) #stationary distribution K_implied = aggregate_K(dd, p) #implied level of capital R_grid[K_index] = R_s #store interest rate K_implied_grid[K_index] = K_implied #store demand of capital K_grid[K_index] = K_value #store supply of capital end # Plot demand and supply of capital plot(K_grid, R_grid, label = \u0026quot;Demand\u0026quot;, ylabel=\u0026quot;Interest rate\u0026quot;, xlabel=\u0026quot;Capital\u0026quot;) plot!(K_implied_grid, R_grid, label = \u0026quot;Supply\u0026quot;)  Notes: This graph shows the demand and the supply of capital as a function of the interest rate.\nII.C MIT Shock Backward and forward updates The next block implements the backward-forward shooting method:\n holding the path of aggregate capital $(K_t)_{t=1}^{T}$, calculate the policy functions holding constant the policy functions, calculate the aggregate capital $(K_t)_{t=1}^{T}$  function backward_update(g_low_ss::Function, g_high_ss::Function, K_path_guess::Array{Float64,1}, z_path::Array{Float64,1}, p::Params) \u0026quot;\u0026quot;\u0026quot; Update policy functions backward, holding {K_t,z_t} constant \u0026quot;\u0026quot;\u0026quot; nT = length(z_path) g_low_path = Array{Function}(undef,nT) #initialize two lists of functions g_high_path = Array{Function}(undef,nT) g_low_path[nT] = g_low_ss g_high_path[nT] = g_high_ss a_path = zeros(p.nI, p.grid_size, nT) #to store policy functions on savings grid R_path = zeros(nT) #to store the interest rate on path W_path = zeros(nT) #to store the wage on path #Start from the steady-state and iterate backward #holding constant the path for {K_t,z_t} #--------------------------------------------------- for t=nT👎2 #iterate backward # Next period R_path[t], W_path[t] = R(K_path_guess[t], z_path[t], p), W(K_path_guess[t], z_path[t], p) # Current period R_path[t-1], W_path[t-1] = R(K_path_guess[t-1], z_path[t-1], p), W(K_path_guess[t-1], z_path[t-1], p) # Current period's policy, given next period a_path[:,:,t-1], c_new, g_low_path[t-1], g_high_path[t-1] = euler_back(g_low_path[t], g_high_path[t], R_path[t-1], W_path[t-1], R_path[t], W_path[t], p) end return a_path, g_low_path, g_high_path end function forward_update(K_star::Float64, a_path::Array{Float64,3}, d_ss::Array{Float64,1}, p::Params) \u0026quot;\u0026quot;\u0026quot; Update forward the distribution of agents + aggregate capital dd_path_forward, K_path_forward \u0026quot;\u0026quot;\u0026quot; nT = length(z_path) K_path_forward = zeros(nT) K_path_forward[1] = K_star dd_path_forward = zeros(size(d_ss,1), nT) dd_path_forward[:,1] = d_ss #2. Iterate forward {K_t,z_t}, using the policy #functions from step 1 #----------------------------------------------- for t=2:nT tt = make_trans_mat(a_path[:,:,t-1], p) #generate transition matrix dd_path_forward[:,t] = tt*dd_path_forward[:,t-1] K_path_forward[t] = aggregate_K(dd_path_forward[:,t], p) end return dd_path_forward, K_path_forward end  Finding the transition path One problem with the backward-forward shooting method is that updating the path for $(K_t)_{t=1}^{T}$ \u0026ldquo;too quickly\u0026rdquo; may result in the overall procedure to diverge. An easy fix is to take a convex combination of the previous guess and the newly calculated path, with $\\lambda$ small:\n$(K^{NEW}_t)_{t=1}^{T} = \\lambda (K_t)_{t=1}^{T} + (1-\\lambda)(K^{OLD}_t)_{t=1}^{T}$\nThe next function implements this idea, with the extra feature that $\\lambda$ increases when the distance between two iterations is getting small (too speed up convergence) and decreases when the distance is getting bigger (to prevent divergence). See this excellent notebook for the same idea applied to an OLG model.\nfunction solve_mit!(K_path, g_low_ss::Function, g_high_ss::Function, d_ss::Array{Float64,1}, K_ss::Float64, z_path::Array{Float64,1}, p::Params; convex_combination::Float64 = 0.2, shrink_factor::Float64 = 0.5, expand_factor::Float64 = 1.05, max_iter::Int64 = 1000, tol::Float64=1e-6, verbose::Bool=true, display_iter::Int64 = 20) \u0026quot;\u0026quot;\u0026quot; Finds the path for aggregate capital K_path \u0026quot;\u0026quot;\u0026quot; diff = Inf #initialization diff_old = Inf #initialization convergence_flag = 0 #initialization damp = convex_combination #initial dampening parameter for i_mit=1:max_iter # Step 1. Solve backward the policy functions {g_t(a,e_low), g_t(a,e_high)}, keeping {K_t,z_t} constant: a_path, g_low_path, g_high_path = backward_update(g_low_ss, g_high_ss, K_path[i_mit], z_path, p); #2. Solve forward {K_t,z_t}, keeping policy functions {g_t(a,e_low), g_t(a,e_high)} constant: dd_path_forward, K_path_forward = forward_update(K_ss, a_path, d_star, p); # Distance between guess for {K_t} and implied values: diff = maximum(abs.(K_path_forward - K_path[i_mit])) # Display every display_iter iterations if verbose==true if mod(i_mit,display_iter) == 0 println(\u0026quot;Iteration $(i_mit). diff = $(diff)\u0026quot;) end end if diff \u0026lt; tol if verbose==true println(\u0026quot;Convergence reached after $(i_mit) iterations.\u0026quot;) end convergence_flag = 1 break else # Update the guess for the path {K_t} # Decrease the dampening factor if diff \u0026gt; diff_old damp = max(min(damp * shrink_factor, 1.0-eps()), eps()) # Increase the dampening factor else damp = max(min(damp * expand_factor, 1.0-eps()), eps()) end if mod(i_mit, 10) == 0 if verbose==true println(\u0026quot;damp = $(damp); diff = $(diff)\u0026quot;) end end # Store the updated path for {K_t} push!(K_path, damp.*K_path_forward .+ (1.0 - damp).*K_path[i_mit]) diff_old = diff end end return K_path, convergence_flag end  Find the path for {K_t} for a 1 std. dev positive productivity shock max_t = 300 #Let us assume that the economy is back to the steady state after max_t periods z_ss = 1.0 #Value of aggregate productivity at the non-stochastic steady-state z_shock = 2.0 #Value of the inital shock # Let's generate a path for the aggregate shock z_path = ones(max_t) z_path[1] = z_ss*z_shock #initial shock # Evolution of aggregate productivity in level: for t_index=2:max_t z_path[t_index] = z_path[t_index-1]^p.rho end # Heroic guess for the initial path of {K_t}: K_t = K* for all t K_path = [] push!(K_path, repeat([K_star], max_t)) # Find the path for {K_t}: @time K_path, convergence_flag = solve_mit!(K_path, g_low_star, g_high_star, d_star, K_star, z_path, p, convex_combination=0.25); # Find the path for other aggregates: R_path = zeros(length(z_path)) #to store the interest rate on path W_path = zeros(length(z_path)) #to store the wage on path for t=length(z_path)👎1 #iterate backward # Next period R_path[t], W_path[t] = R(K_path[end][t], z_path[t], p), W(K_path[end][t], z_path[t], p) end  Visualize convergence of the transition path The first guess for $(K_t)_{t=1}^{T}$ is that it is equal to the non-stochastic steady-state value $K*$. Very quickly, the path for $(K_t)_{t=1}^{T}$ converges to the perfect foresight transition path:\np0 = plot(1:max_t, K_path[1], label= \u0026quot;Iteration 0\u0026quot;, title=\u0026quot;Convergence of K(t)\u0026quot;) plot!(p0, 2:max_t, K_path[2][2:end], label = \u0026quot;Iteration 1\u0026quot;) show_every = 5 #display {K_t} for each multiple of show_every for k in 2:length(K_path) if mod(k,show_every) == 0 plot!(p0, 2:max_t, K_path[k][2:end], xlabel=\u0026quot;t\u0026quot;, label = \u0026quot;Iteration $(k)\u0026quot;, title=\u0026quot;Convergence of K(t)\u0026quot;, legend=:best) end end p0  Notes: This graph shows the path for capital {K_t}^(i) for different iterations of the backward-forward algorithm\nRemoving the borrowing constraint The next graph compares the current model to a similar model without a borrowing constraint. With no borrowing constraint, the aggregate level of capital reacts less to an aggregate shock in productivity:\np1 = plot(1:max_t, K_path[end], label= \u0026quot;K_t Aiyagari\u0026quot;, title=\u0026quot;IRF Aiyagari versus RBC\u0026quot;) plot!(p1, xx[RBCp.iK,2:end] .+ K_star, label = \u0026quot;K_t RBC\u0026quot;, color = \u0026quot;black\u0026quot;, xlabel=\u0026quot;t\u0026quot;)  Notes: This graph shows the impulse response of K_t of for the Aiyagari model and a RBC model.\np1 = plot(1:max_t, z_path./z_path[end] .-1, label = \u0026quot;z(t)\u0026quot;, xlabel= \u0026quot;t\u0026quot;) p2 = plot(1:max_t, R_path./R_path[end] .-1, label= \u0026quot;R(t)\u0026quot;, xlabel= \u0026quot;t\u0026quot;) p3 = plot(1:max_t, W_path./W_path[end].-1 , label= \u0026quot;W(t)\u0026quot;, xlabel= \u0026quot;t\u0026quot;) p4 = plot(1:max_t, K_path[end]./K_path[end][end] .-1, label= \u0026quot;K(t)\u0026quot;, xlabel= \u0026quot;t\u0026quot; ) p5 = plot(p1, p2, p3, p4)  Notes: This graph shows the percentage deviation from steady-state values of (i) aggregate productivity (ii) the interest rate (iii) wages (iv) capital.\nLinearity checks To simulate the stochastic economy, the BKM algorithm makes the assumption that an MIT shock is linear with respect to the aggregate shock. That is, doubling the initial shock will simply double the value of aggregates along the transition path without changing the shape of the path. The next block calculates several transition paths for different initial aggregate shocks.\nmax_t = 300 #Let us assume that the economy is back to the steady state after max_t periods z_ss = 1.0 #Value of aggregate productivity at the non-stochastic steady-state # Different initial shocks array_sigma = collect(range(-0.75, stop=0.75, step=0.25)) # Let's exclude sigma = 0 array_sigma = array_sigma[array_sigma .!= 0.] # To store the different scaled IRF: x_mit_scaled_sigma = zeros(max_t, length(array_sigma)) # To store path of aggregate productivity: z_path_sigma = zeros(max_t, length(array_sigma)) # To store the path for the %deviation of aggregate productivity from its steady-state value z_path_sigma_dev = zeros(max_t, length(array_sigma)) for (index_sigma, sigma) in enumerate(array_sigma) # Let's generate a path for the aggregate shock z_path = ones(max_t) z_path[1] = z_ss + z_ss*sigma # Evolution of aggregate productivity in level: for t_index=2:max_t z_path[t_index] = z_path[t_index-1]^p.rho end # Heroic guess for the initial path of {K_t}: K_t = K* for all t K_path = [] push!(K_path, repeat([K_star], max_t)) # Find the path for {K_t}: @time K_path, convergence_flag = solve_mit!(K_path, g_low_star, g_high_star, d_star, K_star, z_path, p, convex_combination=0.2, verbose=false); # Check for convergence if convergence_flag!=1 error(\u0026quot;No convergence for z(1) = $(z_path[1]).\u0026quot;) end # store the path for z: z_path_sigma[:, index_sigma] = z_path # store for the %deviation of aggregate productivity from its steady-state value z_path_sigma_dev[:, index_sigma] = z_path./z_ss .- 1.0 # Scaled IRF: how a percentage deviation in z_t from its steady-state results in a % deviation of k_t x_mit_scaled_sigma[:, index_sigma] = (K_path[end]./K_star .- 1.0)./z_path_sigma_dev[1, index_sigma] end  p0 = plot() p1 = plot() p2 = plot() p3 = plot() for (index_sigma, sigma) in enumerate(array_sigma) if index_sigma == 1 p0 = plot(100 .*z_path_sigma_dev[:, index_sigma], label=\u0026quot;z(t) with z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;) p1 = plot(z_path_sigma[:, index_sigma], label=\u0026quot;z(t) z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;) p2 = plot(x_mit_scaled_sigma[:, index_sigma], label=\u0026quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;) p3 = plot(sign(z_path_sigma[1, index_sigma] - z_ss)*x_mit_scaled_sigma[:, index_sigma], label=\u0026quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;) else plot!(p0, 100 .*z_path_sigma_dev[:, index_sigma], label=\u0026quot;z(t) with z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;, title = \u0026quot;% deviation aggregate productivity\u0026quot;, xlabel=\u0026quot;t\u0026quot;) plot!(p1, z_path_sigma[:, index_sigma], label=\u0026quot;z(t) with z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;, title = \u0026quot;Aggregate productivity\u0026quot;, xlabel=\u0026quot;t\u0026quot;) plot!(p2, x_mit_scaled_sigma[:, index_sigma], label=\u0026quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;, title = \u0026quot;Impact of shock's size on scaled IRF: x\u0026quot;, xlabel=\u0026quot;t\u0026quot;) plot!(p3, sign(z_path_sigma[1, index_sigma] - z_ss)*x_mit_scaled_sigma[:, index_sigma], label=\u0026quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%\u0026quot;, title = \u0026quot;Impact of shock's sign on scaled IRF: sign(x - x_ss)*x\u0026quot;, xlabel=\u0026quot;t\u0026quot;) end end  plot(p0,p1, fg_legend = :transparent, legend=:best, layout=(2,1))  Notes: This graph shows the percentage deviation of aggregate productivity from its steady-state value (top panel) and aggregate productivity in level (bottom panel) for different initial shocks.\nplot(p2,p3, fg_legend = :transparent, legend=:best, layout=(2,1))  Notes: The top panel shows the scaled impulse response function of capital for different aggregate shocks. The bottom panel shows the scaled impulse response function of capital for different aggregate shocks, multiplied by the sign of the aggregate shock.\nII.D. Out-of-steady-state dynamics max_t = 2000 shocks_t = rand(Normal(0,0.005), max_t) # Series of aggregate shocks # Let's generate a path for the aggregate shock z_path = ones(max_t) z_path[1] = z_ss # Evolution of aggregate productivity in level: for t_index=2:max_t z_path[t_index] = z_path[t_index-1]^p.rho + shocks_t[t_index] end # Calculation of GenBKM path: XT_GenBKM = zeros(max_t);# Initialization @time GenBKM_path!(XT_GenBKM, max_t, x_mit_scaled_sigma, z_path./z_ss .- 1.0, array_sigma)   0.237683 seconds (1.06 M allocations: 50.188 MiB)  p1 = plot(100 .*(z_path./z_ss .- 1.0), label=\u0026quot;z_t\u0026quot;, xlabel=\u0026quot;t\u0026quot;) p2 = plot(100 .*XT_GenBKM, label = \u0026quot;K_t\u0026quot;, xlabel=\u0026quot;t\u0026quot;) plot(p1,p2, fg_legend = :transparent, legend=:best, layout=(2,1))  Notes: The top panel shows the percentage deviation of aggregate productivity from its steady-state value. The bottom panel shows the percentage deviation of capital from its steady-state value.\nConclusion This post presents the model of Aiyagari (1994) and a general three-step procedure to simulate out-of-steady-state dynamics for models of this class for \u0026ldquo;small\u0026rdquo; shocks. Solving incomplete market models for large shocks seems to be much more complicated and is still an active area of research.\nLinks  An excellent course on heterogeneous agent models with code in Matlab, Python and Julia: https://alisdairmckay.com/Notes/HetAgents/index.html More on the EGM method: https://julia.quantecon.org/dynamic_programming/egm_policy_iter.html More on the Aiyagari model: https://python.quantecon.org/aiyagari.html Aiyagari model in continuous time: https://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/aiyagari_continuous_time.ipynb Similar model solved very efficiently with a Python package: https://github.com/shade-econ/sequence-jacobian/blob/master/krusell_smith.ipynb  References  Aiyagari, S. Rao. \u0026ldquo;Uninsured idiosyncratic risk and aggregate saving.\u0026rdquo; The Quarterly Journal of Economics 109.3 (1994): 659-684. Bewley, Truman. \u0026ldquo;A difficulty with the optimum quantity of money.\u0026rdquo; Econometrica: Journal of the Econometric Society (1983): 1485-1504. Boppart, Timo, Per Krusell, and Kurt Mitman. “Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative.” Journal of Economic Dynamics and Control 89 (2018): 68-92. Christopher D Carroll. The method of endogenous gridpoints for solving dynamic stochastic optimization problems. Economics Letters, 91(3):312–320, 2006. Coeurdacier, Nicolas, Helene Rey, and Pablo Winant. \u0026ldquo;The risky steady state.\u0026rdquo; American Economic Review 101.3 (2011): 398-401. Huggett, Mark. \u0026ldquo;The risk-free rate in heterogeneous-agent incomplete-insurance economies.\u0026rdquo; Journal of economic Dynamics and Control 17.5-6 (1993): 953-969. İmrohoroğlu, Ayşe. \u0026ldquo;The welfare cost of inflation under imperfect insurance.\u0026rdquo; Journal of Economic Dynamics and Control 16.1 (1992): 79-91. Ljungqvist, Lars, and Thomas J. Sargent. Recursive macroeconomic theory. MIT press, 2018. Reiter, Michael. “Comments on” Exploiting MIT Shocks in Heterogeneous-Agent Economies: The Impulse Response as a Numerical Derivative” by T. Boppart, P. Krusell and K. Mitman.” Journal of Economic Dynamics and Control 89 (2018): 93-99.  ","date":1589910802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589910802,"objectID":"e827bd2022d107990ffa3e360bb03d94","permalink":"https://julienpascal.github.io/post/aiyagariaggregateuncertainty/","publishdate":"2020-05-19T18:53:22+01:00","relpermalink":"/post/aiyagariaggregateuncertainty/","section":"post","summary":"Introduction The Bewley-Huggett-Aiyagari-Imohoroğlu economies are the workhorse of modern macroeconomics. In these economies, markets are \u0026ldquo;incomplete\u0026rdquo;. Agents cannot fully insure against risk and decide to self-insure by holding a safe asset to smooth their consumption (see Ljungqvist and Sargent (2018) for a textbook treatment of this topic).\nIn this post, I consider the model of Aiyagari (1994). While the original model abstracts from aggregate fluctuations, Economists have since developed several techniques to simulate out-of-steady-state dynamics for this class of models.","tags":["Macroeconomics","HA models"],"title":"Aiyagari Model with Aggregate Uncertainty","type":"post"},{"authors":[],"categories":[],"content":" In a previous post I presented the BKM algorithm , which can used to approximate solutions of macroeconomic models with aggregate uncertainty and heterogeneous agents. This class of models has been been of great interest for Economists for quite a long time. For instance, Aiyagari (1994) already hinted that taking into consideration heterogeneity along the business cycle is both theoretically important and challenging:\n This class of models may also be useful in resolving various asset return puzzles. Mehra and Prescott [1985, p. 145] suggested that these puzzles cannot be \u0026ldquo;accounted for by models that abstract from transactions costs. This is a very hard problem computationally since the distribution of assets across households can no longer be taken to be constant. Instead, the cross-section distribution is part of the state vector that evolves stochastically over time in response to aggregate shocks. This is an issue that remains to be explored\n Recent developments have relied on the sequence representation of the mutli-stage decision process, instead of the traditional recursive form using Bellman\u0026rsquo;s principle (see for instance Auclert at al (2019) or Le Grand and Ragot (2019)).\nIn this short blog post, I would like to present a small modification of the BKM algorithm that delivers large improvements in accuracy: the GenBKM algorithm by Reiter (2018). The code for this post can be found here.\nI.A. The BKM algorithm It is common to use Bellman\u0026rsquo;s principle of optimality to characterize solutions of a mutli-stage decision process. The principle of optimality leads to a solution in a recursive form $d_{t} = d(S_{t})$, where $S_t$ is a vector of state variables and d(.) a policy function describing the optimal action of a decision-maker when faced with any given state. In model with heterogeneous agents (HA) and aggregate uncertainty, $S_t$ is generally infinite-dimensional. Hence, there is a huge premium in avoiding the recursive form.\nThe sequence form of the problem is as follows: at each step in time, a decision-maker observes a new realization of a random process $z_t$ and taking into account the full history of past shocks, the agent make a new choice to maximize her expected discounted sum of returns: $d_t = d(z_t, z_{t-1}, z_{t-2}, \u0026hellip;)$.\nThe BKM algorithm makes the assumption of linearity of d(.) with respect to the aggregate state $z_t$:\n$$ d_t = z_t d(1, 0, 0, \u0026hellip;) + z_{t-1}d(0, 1, 0, \u0026hellip;) + z_{t-2}d(0, 0, 1, \u0026hellip;) + \u0026hellip; $$\nwhere\n$$ d_{1} = d(1,0,0,\u0026hellip;)$$ $$ d_{2} = d(0,1,0,\u0026hellip;)$$ $$ d_{3} = d(0,0,1,\u0026hellip;)$$\nor more compactly:\n$$ d_t = \\sum_{k=0}^{+\\infty} z_{t-k} d_{k} $$\nThe series of $d_{k}$ describes the trajectory of the economy after an \u0026ldquo;MIT\u0026rdquo; shock: the economy is hit by an aggregate shock in period 1 and then goes back directly to its steady-state value. The evolution of equilibrium variables are simply a moving average of past shocks.\nThe BKM algorithm works well because computing an MIT shock in macroeconomic models with aggregate uncertainty and heterogeneous agents is generally feasible and fast.\nTL;DR To simulate out-of-steady-state dynamics, the BKM algorithm superposes scaled impulse response functions.\nI. B. BKM example Let\u0026rsquo;s illustrate how the BKM algorithm works with the following non-linear model:\n$$ x_{t} = a x_{t-1} + b x_{t-1}^2 + z_{t} $$\nThe next plot illustrates that BKM algorithm does a good job at approximating the dynamic of the true nonlinear model. But can we do better? The next section shows that the answer is yes.\n# Dependencies using Distributions using Plots using DataFrames using Random pgfplotsx() # Parameters max_iter=1000 #number of iterations for the simulation a = 0.5 b = 0.05 sigma_shock=1.0 #variance of shocks mu_shock=0. #mean of shocks Random.seed!(1234) d = Normal(mu_shock, sigma_shock) # transition function function iter_x(x_min1::Float64, a::Float64, b::Float64) \u0026quot;\u0026quot;\u0026quot; Function to find the next iteration of x_{t} = a x_{t-1} + b x_{t-1}^2 x_min1::Float64: x_{t-1} a::Float64 b::Float64 \u0026quot;\u0026quot;\u0026quot; return a*x_min1 + b*x_min1^2 end # Simulation of an MIT Shock # We assume that after max_iter_mit periods, the economy is back at the steady-state max_iter_mit = 25 x_mit=zeros(max_iter_mit) # Initial shock z_t=zeros(max_iter_mit) z_t[1] = sigma_shock #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1]; # Scaled-version of the impulse response: p0 = plot(x_mit_scaled, label=\u0026quot;x scaled\u0026quot;, xlabel=\u0026quot;t\u0026quot;) title!(\u0026quot;MIT shock\u0026quot;) # Function to calculate the path of xt using the BKM algorithm function BKM_path!(XT::Array{Float64,1}, x_scaled::Array{Float64,1}, shocks::Array{Float64,1}) \u0026quot;\u0026quot;\u0026quot; XT::Array{Float64,1}: array to store the evolution of the variable xt x_scaled::Array{Float64,1}: a scaled MIT shock shocks::Array{Float64,1}: sequence of shocks \u0026quot;\u0026quot;\u0026quot; # get the length of x_scaled len_x_scaled = length(x_scaled) max_iter = length(XT) # Loop over time periods periods for t=2:max_iter # Superposition of MIT shocks: for k=1:t # After some time, we assume that the effect of past shocks vanishes: if k\u0026lt;=len_x_scaled XT[t]+=x_scaled[k]*shocks[t-k+1] end end end end XT = zeros(max_iter) # Initialization shocks_t = rand(d, max_iter).*0.5 # Series of shocks @time BKM_path!(XT, x_mit_scaled, shocks_t) # Solving using BKM: x_true = zeros(max_iter) # True value of the series for i=2:max_iter x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1] end # Let's store statistics on error: diff_BKM = x_true - XT max_abs_err_BKM = maximum(abs.(diff_BKM)) min_abs_err_BKM = minimum(abs.(diff_BKM)) mean_abs_err_BKM = mean(abs.(diff_BKM)) median_abs_err_BKM = median(abs.(diff_BKM)) p1 = plot(XT[2:end], label=\u0026quot;x_t BKM\u0026quot;) plot!(x_true[2:end], label=\u0026quot;x_t true\u0026quot;) title!(\u0026quot;BKM with b=$(b)\u0026quot;)  Notes: In orange the exact value as a function of time; in blue the approximation obtained by using the BKM algorithm\nII. A. The GenBKM algorithm The BKM algorithm is based on the assumption that both the size and sign of the initial shock does not change the shape of the scaled impulse response function. But is really the case? The next graph shows that both the size and the sign matter for the shape of the scaled impulse response function:\n# Different initial shocks array_sigma = collect(range(-2, stop=2, step=0.5)) # Let's exclude sigma = 0 array_sigma = array_sigma[array_sigma .!= 0.] # To store the different scaled IRF: x_mit_scaled_sigma = zeros(max_iter_mit, length(array_sigma)) for (index_sigma, sigma) in enumerate(array_sigma) x_mit=zeros(max_iter_mit) # Initial shock z_t=zeros(max_iter_mit) z_t[1] = sigma #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1]; # store the scaled response x_mit_scaled_sigma[:, index_sigma] = x_mit_scaled end p2 = plot(x_mit_scaled_sigma[:, 1], label=\u0026quot;sigma = $(array_sigma[1])\u0026quot;) p3 = plot(sign(array_sigma[1])*x_mit_scaled_sigma[:, 1], label=\u0026quot;sigma = $(array_sigma[1])\u0026quot;) for (index_sigma, sigma) in enumerate(array_sigma) plot!(p2, x_mit_scaled_sigma[:, index_sigma], label=\u0026quot;sigma = $(sigma)\u0026quot;, title = \u0026quot;Impact of shock's size on scaled IRF: x\u0026quot;) plot!(p3, sign(sigma)*x_mit_scaled_sigma[:, index_sigma], label=\u0026quot;sigma = $(sigma)\u0026quot;, title = \u0026quot;Impact of shock's sign on scaled IRF: sign(x)*x\u0026quot;) end p4 = plot(p2, p3, layout = (2, 1))  Notes: The top panel shows the scaled impulse response function for different values of shocks. The bottom panel shows the scaled impulse response function multiplied by the sign of the shock. If the size and the sign of the shock did not matter, we would see only one line. It is not the case here.\nThe GenBKM algorithm is the similar to the BKM algorithm, except that the impacts of the size and the sign of the initial shock on the response of the economy are taken into consideration. It proceeds as follows:\n Divide the support of the shock into $n$ intervals $ I_i = (a_i, b_i)$ Compute $n$ MIT shocks using shocks in $z_i \\in I_i$, denoted by $d_k^{i}$ The state of the economy at time $t$ is given by the moving average of past shocks, taking into consideration past shock values:  $$ d_t = \\sum_{k=0}^{+\\infty} z_{t-k} d_{k}^{f(t-k)} $$\nwhere the function $f(t)$ returns the index of the interval in which the shock $z_t$ falls.\nTL;DR GenBKM = BKM with different scaled IRF, instead of one.\nII. B. GenBKM example The GenBKM algorithm is implemented in the block of code that follows. The next plot shows that the approximation error is much smaller using the GenBKM algorithm. The next table shows that the mean absolute error drops by more than 300% if the GenBKM is used instead of BKM.\nfunction GenBKM_path!(XT::Array{Float64,1}, max_iter::Int64, x_scaled::Array{Float64,2}, shocks::Array{Float64,1}, array_sigma::Array{Float64,1}) # get the length of x_scaled len_x_scaled = size(x_scaled,1) # We don't want x_scaled to contain any NaN value if sum(isnan.( x_scaled) .== true) != 0 error(\u0026quot;x_scaled contains at least one NaN value.\u0026quot;) end # We don't want shocks to contain any NaN value if sum(isnan.(shocks) .== true) != 0 error(\u0026quot;shocks contains at least one NaN value.\u0026quot;) end # Loop over time periods periods for t=2:max_iter # Superposition of MIT shocks: for k=1:t # After some time, we assume that the effect of past shocks vanishes: if k\u0026lt;=len_x_scaled # Find were the initial shock stood on the sigma grid: index_sigma = searchsortednearest(array_sigma, shocks[t-k+1]) XT[t]+=x_scaled[k, index_sigma]*shocks[t-k+1] end end end end # Function to find the index corresponding to the closest value on a grid: # Source: https://discourse.julialang.org/t/findnearest-function/4143/4 function searchsortednearest(a::Array{Float64,1},x::Float64) \u0026quot;\u0026quot;\u0026quot; a::Array{Float64,1}: grid x::Float64: value to be found returns the index of the closest value to x on grid a \u0026quot;\u0026quot;\u0026quot; idx = searchsortedfirst(a,x) if (idx==1); return idx; end if (idx\u0026gt;length(a)); return length(a); end if (a[idx]==x); return idx; end if (abs(a[idx]-x) \u0026lt; abs(a[idx-1]-x)) return idx else return idx-1 end end # Calculation of GenBKM path: XT_GenBKM = zeros(max_iter); @time GenBKM_path!(XT_GenBKM, max_iter, x_mit_scaled_sigma, shocks_t, array_sigma) # Let's store statistics on error: diff_GenBKM = x_true - XT_GenBKM max_abs_err_GenBKM = maximum(abs.(diff_GenBKM)) min_abs_err_GenBKM = minimum(abs.(diff_GenBKM)) mean_abs_err_GenBKM = mean(abs.(diff_GenBKM)) median_abs_err_GenBKM = median(abs.(diff_GenBKM)) df = DataFrame(Statistics = [\u0026quot;Max absolute error\u0026quot;, \u0026quot;Min absolute error\u0026quot;, \u0026quot;Mean absolute error\u0026quot;, \u0026quot;Median absolute error\u0026quot;], BKM = [max_abs_err_BKM, min_abs_err_BKM, mean_abs_err_BKM, median_abs_err_BKM], GenBKM = [max_abs_err_GenBKM, min_abs_err_GenBKM, mean_abs_err_GenBKM, median_abs_err_GenBKM]) # Plot errors p6 = plot(diff_GenBKM[2:end], label=\u0026quot;GenBKM\u0026quot;, xlabel = \u0026quot;t\u0026quot;, ylabel = \u0026quot;Error\u0026quot;) plot!(diff_BKM[2:end], label=\u0026quot;BKM\u0026quot;) title!(\u0026quot;Error BKM and GenBKM\u0026quot;)  Notes: In orange the approximation error when using BKM; in blue the approximation error when using GenBKM\nStatisticsBKMGenBKM    1Max absolute error0.2631670.1323182Min absolute error0.00.03Mean absolute error0.03810890.01369154Median absolute error0.02371940.0100031 Conclusion Heterogeneity along the business cycle matters. This blog post presented two simple algorithms that are both fast and accurate to solve for macroeconomics models in which heterogeneity is key. GenBKM is a refinement of BKM, which tends to be more accurate. However, there is no free lunch. The increased accuracy of GenBKM is obtained by using several MIT shocks instead of one.\nReferences  Aiyagari, S. Rao. \u0026ldquo;Uninsured idiosyncratic risk and aggregate saving.\u0026rdquo; The Quarterly Journal of Economics 109.3 (1994): 659-684. Auclert, Adrien, et al. Using the Sequence-Space Jacobian to Solve and Estimate Heterogeneous-Agent Models. No. w26123. National Bureau of Economic Research, 2019. Boppart, Timo, Per Krusell, and Kurt Mitman. \u0026ldquo;Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative.\u0026rdquo; Journal of Economic Dynamics and Control 89 (2018): 68-92. Le Grand, François, and Ragot, Xavier. \u0026ldquo;Managing Inequality over the Business Cycles: Optimal Policies with Heterogeneous Agents and Aggregate Shocks\u0026rdquo;. No. 1090. Society for Economic Dynamics, 2019. Reiter, Michael. \u0026ldquo;Comments on\u0026rdquo; Exploiting MIT Shocks in Heterogeneous-Agent Economies: The Impulse Response as a Numerical Derivative\u0026rdquo; by T. Boppart, P. Krusell and K. Mitman.\u0026rdquo; Journal of Economic Dynamics and Control 89 (2018): 93-99.  Appendix versioninfo()  Julia Version 1.3.0 Commit 46ce4d7933 (2019-11-26 06:09 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.1 (ORCJIT, skylake)  ","date":1587491602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587491602,"objectID":"c8043cb2fef3684f138036c035711a12","permalink":"https://julienpascal.github.io/post/genbkm/","publishdate":"2020-04-21T18:53:22+01:00","relpermalink":"/post/genbkm/","section":"post","summary":"In a previous post I presented the BKM algorithm , which can used to approximate solutions of macroeconomic models with aggregate uncertainty and heterogeneous agents. This class of models has been been of great interest for Economists for quite a long time. For instance, Aiyagari (1994) already hinted that taking into consideration heterogeneity along the business cycle is both theoretically important and challenging:\n This class of models may also be useful in resolving various asset return puzzles.","tags":["Macroeconomics","HA models"],"title":"The GenBKM Algorithm","type":"post"},{"authors":[],"categories":[],"content":" This notebook builds upon what has been described in Part I. In Part I, we introduced the linear–quadratic regulator (LQR) framework in Python. We solved the linearized control problem.\nIn this notebook, we will see that we can do better. The basic idea is to follow the the evolution of \u0026ldquo;observables\u0026rdquo; — functions of the state space — instead of the evolution of the state itself using the Koopman operator. In the space of observables, the differential equation is linear. Thus, we can solve for the optimal control in the this transformed space, without having to linearize the system around its steady-state.\nIn this notebook, you will learn:  the basics on the Koopman operator how to solve for the optimal control in the Koopman subspace  I. The Koopman operator 101 Let us remember that in Part I, we analyzed the evolution of the following dynamical system:\n$$ \\begin{cases} \\frac{d}{dt} x = \\mu x \\\\ \\frac{d}{dt} y = \\lambda (y - x^2) \\end{cases} $$\nLet\u0026rsquo;s make the observation that we can defined a new vector $z$ defined by:\n$$ z \\equiv \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} x \\\\ y \\\\ x^2 \\end{pmatrix}$$\nThe original nonlinear dynamical system is linear when considering the evolution of $z$:\n$$ \\frac{d}{dt} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} \\mu \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \u0026amp; -\\lambda \\\\ 0 \u0026amp; 0 \u0026amp; 2 \\mu \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix}$$\nWhat we just did — writing the evolution the dynamical system using some observables — is the central idea of the Koopman operator. Here we are lucky because our new variable $z$ is of finite dimension. In the general case, $z$ is infinite-dimensional:\n$$ \\frac{d}{dt} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ \u0026hellip; \\end{pmatrix} = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \u0026hellip; \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \u0026hellip; \\\\ \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ \u0026hellip; \\end{pmatrix} $$\nand we have no guarantee that the truncation of z by a finite dimensional counterpart will produce a good approximation of the system. For more detail on this problem, I invite you to read this paper.\nBecause now the dynamical system is linear, we can directly use the results from LQR framework and solve for the optimal control that minimizes the cost $J$:\n$$ J = \\int_{0}^{\\infty} x\u0026rsquo;(t) Q x(t) + u\u0026rsquo;(t) R u(t) dt $$\nWe already know that the optimal control from the controller is a linear function of the current state of the system:\n$$ u = - C_z z$$\nWhen the system is controlled optimally, the equation governing the evolution of the system writes:\n$$ \\frac{d}{dt} z = A_z z - B C_z z $$\nAt a later stage, because we want to compare the Koopman controller to the linearized controller, we do not want to penalize the variable $z_3$ for being away from its steady state. The rationale is the variable $z_3$ is \u0026ldquo;fictive\u0026rdquo;. We still have in mind that we want to control the \u0026ldquo;real\u0026rdquo; variables $x$ and $y$. As a result, the matrix $Q$ we consider is:\n$$ \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{pmatrix} $$\nII. Simulating forward the dynamical system For what follows, you will need the following packages:\nimport os import matplotlib.pyplot as plt plt.style.use('ggplot') %matplotlib inline import numpy as np from control.matlab import * # MATLAB-like functions #to solve ODE from scipy import integrate #show the version of Python I am using: !python3 --version  Python 3.5.2  Let\u0026rsquo;s study optimal control for the following differential equation:\n$$ \\frac{d}{dt} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} \\mu \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \u0026amp; -\\lambda \\\\ 0 \u0026amp; 0 \u0026amp; 2 \\mu \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix} $$\nThe following block of code define the parameter values, the matrix A for the linear part of the differential equation and the matrix B specifying that the controller can only act on x. We also specify the time span during which we want to simulate forward the trajectories:\n# Parameters and matrices A and B: mu = -0.05 llambda = -1.0 # Matrices for the orginial system A = np.array([[mu, 0], [0, llambda]]) B = np.array([[0], [1]]) R = np.array([1]) Q = np.eye(2) # Matrices for the transformed system: A_z = np.array([[mu, 0, 0], [0, llambda, -llambda], [0, 0, 2*mu]]) B_z = np.array([[0], [1], [0]]) R_z = np.array([1]) Q_z = np.eye(3) # Time span t0, t1 = 0, 100 # start and end t = np.arange(t0, t1, 0.01) # Function that defines the dynamic system: def vdp0(t, y): # linear part + nonlinear part: x = A.dot(y) + np.array([0, -llambda*(y[0]**2)]) return x # Function that defines the dynamic system in the Koopman subspace: def vdp0z(t, y): x = A_z.dot(y) return x  We then define 4 different starting values and simulate forward the system using the scipy.integrate toolkit:\n# Set of starting values: y0A = np.array([1.5, -1, (1.5)**2]) y0B = np.array([-1.5, -1, (-1.5)**2]) y0C = np.array([5, 5, 25]) y0D = np.array([-5, 5, 25]) # To store the different trajectories list_y_z = [] # Method for the ODE: # This is an explicit runge-kutta method of order (4)5 due to Dormand \u0026amp; Prince integrator = \u0026quot;dopri5\u0026quot; # Loop over the different starting values and calculate trajectories: for y0 in [y0A, y0B, y0C,y0D]: # initialize an array to store the solution y = np.zeros((len(t), len(y0))) # array for solution r0 = integrate.ode(vdp0z).set_integrator(integrator) r0.set_initial_value(y0, t0) # initial values for i in range(1, t.size): y[i, :] = r0.integrate(t[i]) # get one more value, add it to the array if not r0.successful(): raise RuntimeError(\u0026quot;Could not integrate\u0026quot;) # append the list of solution list_y_z.append(y)  The next graph shows the trajectory of x and y using the augmented linear system corresponds to the trajectory we found using the nonlinear dynamical system, as expected:\n# Plot the different paths: fig, ax = plt.subplots(figsize=(10, 5)) for index,y0 in enumerate([y0A, y0B, y0C, y0D]): ax.plot(list_y_z[index][1:-1, 0], list_y_z[index][1:-1, 1], label = str(y0)) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.title(\u0026quot;Trajectories for different starting values\u0026quot;) plt.legend() plt.show()  The next graph shows that the dynamical system moves along a nice parabola in the third dimension:\n# Plot the different paths: fig, ax = plt.subplots(figsize=(10, 5)) for index,y0 in enumerate([y0A, y0B, y0C, y0D]): ax.plot(list_y_z[index][1:-1, 0], list_y_z[index][1:-1, 2], label = str(y0)) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;z\u0026quot;) plt.title(\u0026quot;Trajectories for different starting values\u0026quot;) plt.legend() plt.show()  III. Solving for the optimal control Let us remember that our aim is find the matrix $C_z$ defining the optimal control:\n$$ u = - C_z z $$\nInterestingly, while the optimal $u$ is linear when considering $z$, it is quadratic when considering the original vector of state $x$:\n$$ u = - (C_{z,1} C_{z,2}) \\begin{pmatrix} x \\\\ y \\end{pmatrix} - C_{z,3} x^2 $$\nWe will see in a minute that having a non-linear control allows us to outperform the linear control obtained in Part I.\n# Solve for C: (C, X, E) = lqr(A, B, Q, R) print(\u0026quot;Feedback matrix C : {}\u0026quot;.format(C)) # Solve for C_z: (C_z, X_z, E_z) = lqr(A_z, B_z, Q_z, R_z) print(\u0026quot;Feedback matrix C_z : {}\u0026quot;.format(C_z))  Feedback matrix C : [[0. 0.41421356]] Feedback matrix C_z : [[0. 0.41421356 0.27355029]]  We can now proceed as previously to simulate forward the differential equation. The only difference is that now we have to take into account the optimal control applied each period on the system. The optimal control is taken into consideration in the function vdp1(t, y):\ndef vdp1(t, y): # Ay - B*X*y + Cy x = A.dot(y) - np.matmul(B,C).dot(y) return x + np.array([0, -llambda*(y[0]**2)]) def vdp1z(t, y): return A_z.dot(y) - np.matmul(B_z,C_z).dot(y) y0 = [-5, 5] # initial value y0_z = [-5, 5, 25] # initial value y = np.zeros((len(t), len(y0))) # array for solution y_z = np.zeros((len(t), len(y0_z))) # array for solution y[0, :] = y0 y_z[0, :] = y0_z  # Controlled trajectory using the linearized system r = integrate.ode(vdp1).set_integrator(integrator) r.set_initial_value(y0, t0) # initial values for i in range(1, t.size): y[i, :] = r.integrate(t[i]) # get one more value, add it to the array if not r.successful(): raise RuntimeError(\u0026quot;Could not integrate\u0026quot;) # Controlled trajectory using the linearized system r = integrate.ode(vdp1z).set_integrator(integrator) r.set_initial_value(y0_z, t0) # initial values for i in range(1, t.size): y_z[i, :] = r.integrate(t[i]) # get one more value, add it to the array if not r.successful(): raise RuntimeError(\u0026quot;Could not integrate\u0026quot;)  We can now compare the controlled and Koopman controlled trajectories. Interestingly, the Koopman controlled trajectory moves along a trajectory involving lower values for $y$:\nindex = 3 #choose the last trajectory fig, ax = plt.subplots(figsize=(10, 5)) plt.plot(list_y_z[index][1:-1, 0], list_y_z[index][1:-1, 1], label = \u0026quot;Uncontrolled\u0026quot;) plt.plot(y[:, 0], y[:, 1], label = \u0026quot;Controlled\u0026quot;) plt.plot(y_z[:, 0], y_z[:, 1], label = \u0026quot;Koopman controlled\u0026quot;) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.legend() plt.show()  The next logical question is whether or not the Koopman controller is better than the controller based on the linearization of the dynamical system around its steady state. The next plot shows that indeed the Koopman controller (in purple) outperforms the latter (in blue).\n# Controlled JLQR = np.cumsum( np.square(y[:, 0]) + np.square(y[:, 1]) + np.square(np.matmul(C, y.T)) ).T # Koopman controlled JLQRz = np.cumsum( np.square(y_z[:, 0]) + np.square(y_z[:, 1]) + np.square(np.matmul(C_z, y_z.T)) ).T # Uncontrolled JLQR0 = np.cumsum( np.square(list_y_z[index][:, 0]) + np.square(list_y_z[index][:, 1]) ) fig, ax = plt.subplots(figsize=(10, 5)) plt.plot(t, JLQR0, label = \u0026quot;Uncontrolled\u0026quot;) plt.plot(t, JLQR, label = \u0026quot;Controlled\u0026quot;) plt.plot(t, JLQRz, label = \u0026quot;Koopman controlled\u0026quot;) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;JLQR\u0026quot;) plt.legend() plt.show()  Conclusion This notebook illustrates the idea behind the Koopman operator in a very simple setting. We saw that by solving for the optimal control in the space of observables, in which the system is exactly linear, we find a controller that outperforms the one obtained by linearizing the system around its steady-state.\nThe Koopman LQR control drew my attention because many techniques, especially in Economics, are based on the linearization of dynamic systems around its steady-state. While these linearization techniques are accurate when the economy is close to its steady-state (in a \u0026ldquo;business as usual\u0026rdquo; situation), it is hard to know how these approximations perform when the economy is an unusual state. Given the current economic context, relying on linearization might be inaccurate.\nReferences  The linear system studied in this notebook is based on the paper: Brunton, Steven L., et al. \u0026ldquo;Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control.\u0026rdquo; PloS one 11.2 (2016).\n Koopman, Bernard O. \u0026ldquo;Hamiltonian systems and transformation in Hilbert space.\u0026rdquo; Proceedings of the national academy of sciences of the united states of america 17.5 (1931): 315.\n  ","date":1586098800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586098800,"objectID":"dbe66d918df5e75f69b89172f0685f6e","permalink":"https://julienpascal.github.io/post/lqr_partii/","publishdate":"2020-04-05T16:00:00+01:00","relpermalink":"/post/lqr_partii/","section":"post","summary":"This notebook builds upon what has been described in Part I. In Part I, we introduced the linear–quadratic regulator (LQR) framework in Python. We solved the linearized control problem.\nIn this notebook, we will see that we can do better. The basic idea is to follow the the evolution of \u0026ldquo;observables\u0026rdquo; — functions of the state space — instead of the evolution of the state itself using the Koopman operator.","tags":["Dynamical systems"],"title":"The linear–quadratic regulator Part II","type":"post"},{"authors":[],"categories":[],"content":" The two main goals of this blog post is to introduce what the linear–quadratic regulator (LQR) framework is and to show how to solve LQR problems using Python. The LQR is concerned with operating a dynamic system (a rocket, a car, the economy, etc.) at minimum cost.\nIn this blog post you will learn  what the LQR framework is how to simulate forward an ordinary differential equation using scipy how to solve for the optimal control using the Python Control Systems Library  The Jupyter notebook with the code used to generate this blog post can be found here\nI. The LQR framework in a nutshell Many natural phenomena naturally lead to differential equations. A differential equation is an equation in which the rate of the change of a variable ($\\frac{d}{dt} x$) is a function its state $x$. The unknown is a function satisfying both the differential equation and an initial value. For instance, a simple model of the spread of the Covid-19 can be written as a system of differential equations (see for instance the SIR model):\n$$ \\frac{d}{dt} \\boldsymbol{x} = f(\\boldsymbol{x},t) $$\nThe LQR theory studies a special case of the above problem. It focuses on problems where the function $f(\\boldsymbol{x},t)$ is linear:\n$$ \\frac{d}{dt} \\boldsymbol{x} = A \\boldsymbol{x} $$\nThe equation above is a \u0026ldquo;passive\u0026rdquo; one. We simply observe the trajectory of $\\boldsymbol{x}$ and there is nothing we can do about it. The LQR framework is based on the idea that an observer may want to change the trajectory of the system by exerting a control on $\\boldsymbol{x}$. In the case of the spread of the Covid-19, the government may want to limit the number of new cases. When considering the economy, a central bank may want to control the interest rate to reach its inflation target. In the case of the SpaceX, the engineers may want to stabilize the rocket so that it does not explode when trying to land back on Earth.\nIn the LQR framework, the controller wants to stabilize the system to reach one of its steady-state values, defined by:\n$$ \\boldsymbol{x_{ss}} = A \\boldsymbol{x_{ss}} $$\nWe need to take into consideration the impact that the controller has on the system. Let us add the control, denoted by $u$, to the uncontrolled system from above:\n$$ \\frac{d}{dt} \\boldsymbol{x} = A \\boldsymbol{x} + B \\boldsymbol{u} $$\nwhere $B$ is a matrix capturing the idea that the controller could only control some elements of $\\boldsymbol{x}$.\nHowever, there is no free lunch. In order to stabilize the system, the controller needs to pay a cost. Going back to our rocket example, some fuel is burnt in order to stabilize the trajectory of the rocket. The LQR is based on a cost function that is quadratic. This quadratic assumption simplifies the algebra substantially and captures the intuitive idea that doubling the effort actually costs four times more, not two times more.\nLet us assume that the steady state of the system is $\\boldsymbol{0}$. This is without loss of generality, because we can rewrite the system as a deviation from its steady-state value $\\boldsymbol{\\tilde{x}} \\equiv \\boldsymbol{x} - \\boldsymbol{x_{ss}}$, in which case the steady-state is reached for $\\boldsymbol{\\tilde{x}} = \\boldsymbol{0}$.\nTo capture the cost of stabilizing the system, let us use the letter $J$. $J$ captures two types of cost. Firstly, the controller dislikes when the system is not at its steady-state. In the equation below, this type of cost is captured by the matrix $Q$. Secondly, the controller dislikes spending energy to control the system. This second type of cost is captured by the matrix $R$:\n$$ J = \\int_{0}^{\\infty} \\boldsymbol{x\u0026rsquo;}(t) Q \\boldsymbol{x}(t) + \\boldsymbol{u\u0026rsquo;}(t) R \\boldsymbol{u}(t) dt $$\nA beautiful result from the LQR theory is that the optimal control from the controller is simply a linear function of the current state of the system:\n$$ \\boldsymbol{u} = - C \\boldsymbol{x}$$\nWhen the system is controlled optimally, the equation governing the evolution of the system writes:\n$$ \\frac{d}{dt} \\boldsymbol{x} = A \\boldsymbol{x} - B C \\boldsymbol{x} $$\nII. Simulating forward an ordinary differential equation in Python Having summarized what the LQR framework is, we can now give an illustration of how it works using a simple example using Python. For what follows, you will need the following packages:\nimport os import matplotlib.pyplot as plt plt.style.use('ggplot') %matplotlib inline import numpy as np from control.matlab import * # MATLAB-like functions #to solve ODE from scipy import integrate #show the version of Python I am using: !python3 --version  Python 3.5.2  Let\u0026rsquo;s study optimal control for the following differential equation:\n$$\\begin{cases} \\frac{d}{dt} x =\\mu x \\\\ \\frac{d}{dt} y = \\lambda (y - x^2) \\end{cases}$$\nTwo observations on this dynamical system. Firstly, the system is not linear, but we will see how to deal with that in a minute. Secondly, by eyeballing the equation, it is easy to see that when $\\mu \u0026lt; 1$ and $\\lambda \u0026lt; 1$, the unique fixed point is\n$$ \\begin{pmatrix} x_{SS} \\\\ y_{SS} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nWe can convince ourselves by simulating forward the trajectory of the system using different starting values.\nLet\u0026rsquo;s notice that the system can be written as a linear one, plus a part that is nonlinear:\n$$ \\frac{d}{dt} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} \\mu \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} + \\begin{pmatrix} O \\\\ - \\lambda x^2 \\end{pmatrix} $$\n$$ \\frac{d}{dt} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = A \\begin{pmatrix} x \\\\ y \\end{pmatrix} + \\begin{pmatrix} O \\\\ - \\lambda x^2 \\end{pmatrix}$$\nThe following block of code defines the parameter values, the matrix A for the linear part of the differential equation and the matrix B specifying that the controller can only act on x. We also specify the time span during which we want to simulate forward the trajectories:\n# Parameters and matrices A and B: mu = -0.05 llambda = -1.0 A = np.array([[mu, 0], [0, llambda]]) B = np.array([[0], [1]]) # Time span t0, t1 = 0, 100 # start and end t = np.arange(t0, t1, 0.01)  A clean way to simulate forward the trajectory is to define a function that return the evolution of the system:\n# Function that defines the dynamic system: def vdp0(t, y): # linear part + nonlinear part: x = A.dot(y) + np.array([0, -llambda*(y[0]**2)]) return x  We then define 4 different starting values and simulate forward the system using the scipy.integrate toolkit:\n# Set of starting values: y0A = np.array([1.5, -1]) y0B = np.array([-1.5, -1]) y0C = np.array([5, 5]) y0D = np.array([-5, 5]) # To store the different trajectories list_y = [] # Method for the ODE: # This is an explicit runge-kutta method of order (4)5 due to Dormand \u0026amp; Prince integrator = \u0026quot;dopri5\u0026quot; # Loop over the different starting values and calculate trajectories: for y0 in [y0A, y0B, y0C,y0D]: # initialize an array to store the solution y = np.zeros((len(t), len(y0))) # array for solution r0 = integrate.ode(vdp0).set_integrator(integrator) r0.set_initial_value(y0, t0) # initial values for i in range(1, t.size): y[i, :] = r0.integrate(t[i]) # get one more value, add it to the array if not r0.successful(): raise RuntimeError(\u0026quot;Could not integrate\u0026quot;) # append the list of solution list_y.append(y)  We can then plot the trajectories we just calculated on a same graph using matplotlib:\n# Plot the different paths: fig, ax = plt.subplots(figsize=(10, 5)) for index,y0 in enumerate([y0A, y0B, y0C, y0D]): ax.plot(list_y[index][1:-1, 0], list_y[index][1:-1, 1], label = str(y0)) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.title(\u0026quot;Trajectories for different starting values\u0026quot;) plt.legend() plt.show()  From the graph, we see that $y$ moves very quickly to the parabola defined by $y = x^2$. Then, the system slowly converges towards $(0,0)$, moving along the same parabola.\nBefore moving to the optimal control of the system, let us calculate the total cost $J$ of letting the system converging naturally to its steady state value:\n# Store the cost associated with each starting value: list_cost = [] for y in list_y: JLQR = np.cumsum( np.square(y[:, 0]) + np.square(y[:, 1]) ) # append the list of solution list_cost.append(JLQR)  The next plot shows the cumulative cost as time passes. The further away the starting value is from the steady-state, the higher the cost. We also see that because the cost function treats negative and positive deviations from the steady state the same way (because deviations are squared), the cost for the starting values (1.5, -1) and (-1.5, -1) are the same. The same observation holds for (5, 5) and (-5, 5).\n# Plot the cost associated with each starting value: fig, ax = plt.subplots(figsize=(10, 5)) for JLQR, y0 in zip(list_cost,[y0A, y0B, y0C, y0D]): plt.plot(t, JLQR, label = str(y0)) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;JLQR\u0026quot;) plt.legend() plt.show()  III. Solving for the optimal control Let\u0026rsquo;s first define the matrices Q and R, before solving for the optimal control matrix $C$ using the lqr function from the Python Control Systems Library\nR = np.array([1]) Q = np.eye(2) # Solve for C: (C, X, E) = lqr(A, B, Q, R) print(\u0026quot;Feedback matrix C : {}\u0026quot;.format(C))  Feedback matrix C : [[0. 0.41421356]]  We can now proceed as previously to simulate forward the differential equation. The only difference is that now we have to take into account the optimal control applied each period on the system. The optimal control is taken into consideration in the function vdp1(t, y):\ndef vdp1(t, y): # Ay - B*X*y + Cy x = A.dot(y) - np.matmul(B,C).dot(y) return x + np.array([0, -llambda*(y[0]**2)]) y0 = [-5, 5] # initial value y = np.zeros((len(t), len(y0))) # array for solution y[0, :] = y0 r = integrate.ode(vdp1).set_integrator(integrator) r.set_initial_value(y0, t0) # initial values for i in range(1, t.size): y[i, :] = r.integrate(t[i]) # get one more value, add it to the array if not r.successful(): raise RuntimeError(\u0026quot;Could not integrate\u0026quot;)  We can now compare the controlled trajectory (in red) to the uncontrolled trajectory (in blue):\nfig, ax = plt.subplots(figsize=(10, 5)) plt.plot(y[:, 0], y[:, 1], label = \u0026quot;controlled\u0026quot;) plt.plot(list_y[index][1:-1, 0], list_y[index][1:-1, 1], label = \u0026quot;uncontrolled\u0026quot;) plt.xlabel(\u0026quot;x\u0026quot;) plt.ylabel(\u0026quot;y\u0026quot;) plt.legend() plt.show()  With the controlled trajectory, the deviation of $y$ from its steady state value is less extreme. The system converges to $(0,0)$ on a different parabola. As expected, controlling the system with the optimal controller is less costly than letting the system evolve uncontrolled:\nJLQR = np.cumsum( np.square(y[:, 0]) + np.square(y[:, 1]) + np.square(np.matmul(C, y.T)) ).T fig, ax = plt.subplots(figsize=(10, 5)) plt.plot(t, JLQR, label = \u0026quot;controlled\u0026quot;) plt.plot(t, list_cost[3], label = \u0026quot;uncontrolled\u0026quot;) plt.xlabel(\u0026quot;t\u0026quot;) plt.ylabel(\u0026quot;JLQR\u0026quot;) plt.legend() plt.show()  IV. What about $-\\lambda x^2$? Linearizing the nonlinear system $$ \\frac{d}{dt} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} \\mu \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} + \\begin{pmatrix} O \\\\ - \\lambda x^2 \\end{pmatrix} $$\nA careful reader would have noticed that we used a linear controller on a non-linear system. Is it legitimate? Intuitively, we ignored the term $-\\lambda x^2$, which is small when $x \u0026lt; 1$ and/or when $\\lambda$ is small. We can show more \u0026ldquo;rigorously\u0026rdquo; that what we did makes sense.\nLet us remember that the dynamical system is:\n$$ \\frac{d}{dt} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = f \\Big( \\begin{pmatrix} x \\\\ y \\end{pmatrix}, t \\Big) $$\nOr more precisely:\n$$ \\begin{cases} \\frac{d}{dt} x = \\mu x \\\\ \\frac{d}{dt} y = \\lambda (y - x^2) \\end{cases} $$\nA first order Taylor expansion around the steady-state gives us:\n$$ \\boldsymbol{x} - \\boldsymbol{x_{ss}} \\approx A (\\boldsymbol{x} - \\boldsymbol{x_{ss}}) $$\nWhere $A$ is the Jacobian matrix (matrix of first derivatives) evaluated at the steady-state value, which is (0,0) in our simple example. The first derivatives are:\n$$ \\frac{d}{dx}(\\frac{d}{dt}x) = \\mu \\\\ \\frac{d}{dy}(\\frac{d}{dt}x) = 0 \\\\ \\frac{d}{dx}(\\frac{d}{dt}y) = \\lambda \\\\ \\frac{d}{dy}(\\frac{d}{dt}y)= -2 \\lambda x $$\nEvaluated at the steady-state, the matrix A is equal to:\n$$ A = \\begin{pmatrix} \\mu \u0026amp; 0 \\\\ -2 \\lambda \\times 0 \u0026amp; \\lambda \\end{pmatrix} = \\begin{pmatrix} \\mu \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \\end{pmatrix} $$\nand because the steady-state is $(0,0)$, we have $\\tilde{x} = x$. The take-away is that in the neighborhood of the steady-state $(0,0)$, we can solve the LQR ignoring the $-\\lambda x^2$ term.\nConclusion This notebook introduced what the LQR framework and showed how to solve for the optimal control in Python. We saw that despite the fact that the example we studied is not linear, we can linearize the dynamical system around its stead-state. In the Part II of this series on the LQR framework, we will see that we can do even better by solving the dynamical system in a new space, in which the system is exactly linear.\nReferences  The linear system studied in this notebook is based on the paper: Brunton, Steven L., et al. \u0026ldquo;Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control.\u0026rdquo; PloS one 11.2 (2016).  ","date":1585763602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585763602,"objectID":"c7ac65c6b6fffe5d5d384d2b006d70b3","permalink":"https://julienpascal.github.io/post/lqr/","publishdate":"2020-04-01T18:53:22+01:00","relpermalink":"/post/lqr/","section":"post","summary":"The two main goals of this blog post is to introduce what the linear–quadratic regulator (LQR) framework is and to show how to solve LQR problems using Python. The LQR is concerned with operating a dynamic system (a rocket, a car, the economy, etc.) at minimum cost.\nIn this blog post you will learn  what the LQR framework is how to simulate forward an ordinary differential equation using scipy how to solve for the optimal control using the Python Control Systems Library  The Jupyter notebook with the code used to generate this blog post can be found here","tags":["Dynamical systems"],"title":"The linear–quadratic regulator Part I","type":"post"},{"authors":[],"categories":[],"content":" For a specific project on the housing market (here), I had to analyze thousands of photos. To do that, I used a convolutional neural network (CNN), which is a fancy name for a complicated function that can be \u0026ldquo;trained\u0026rdquo; to recognize patterns in images. In this blog post, I would like to introduce the \u0026ldquo;Hello World\u0026rdquo; of computer vision and CNN: the classification of hand-written digits from the MNIST dataset. There are thousands of tutorials on the same topic using Python freely available on the Internet. Instead, let\u0026rsquo;s use Julia and the package Flux.jl. Why? Because Julia is fast, and if you have millions of images to analyze, the speed up could be substantial compared to Python. The Jupyter notebook used to generate this post can be found here.\nData The MNIST dataset contains images of hand-written digits (0 to 9) in grayscale and that are nicely centered. Each pixel is represented by a number in between 0 (black) and 255 (white). Each image is 28 by 28 pixels. One way to represent an image is to see it as a 1d-column vector of 28*28 = 784 pixels. However, this representation ignores the \u0026ldquo;structure\u0026rdquo; of an image: pixels that are close to each others are informative on the digit we are trying to identify. A CNN is a good tool to keep the spatial structure of an image, while avoiding issues linked to the curse of dimensionality: images are noisy and high-dimensional input data.\nA crash course on CNN Two of the key ingredients of a CNN are a convolutional layer (hence the name) and a maxpool layer.\nConvolutional layer A convolutional layer applies a stencil to each point. The output of a convolutional layer is an \u0026ldquo;image\u0026rdquo; of lower dimension, that is informative on some features of the input image (shapes, edges, etc.). The figure below shows how a convolutional layer works:\nsource:https://mitmath.github.io/18337/lecture14/pdes_and_convolutions\nMaxpool layer A maxpool layer is a stencil that selects the maximum value within a square. Below is an illustration of a maxpool layer applied to a $ 4 \\times 4$ image:\nsource:https://mauriciocodesso.com/post/convolution-neural-network/\nStride and Padding When building a CNN, one must specify two hyper parameters: stride and padding\n When the stride is equal to 1, we move the filters one pixel at a time. When stride is equal to 2, we move the filters two pixel at a time, etc.\n Padding refers to \u0026ldquo;adding zeroes\u0026rdquo; at the border of an image. Padding can be used to control the size of the output volume and helps in keeping information at the border of images\n  Below is an example of a $3 \\times 3$ filter applied to a $5 \\times 5$ input padded with a $1 \\times 1$ border of zeros using $2 \\times 2$ strides:\nsource: http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\nThe typical infrastructure of a CNN is first to apply a convolutional layer to the input image, then to use a maxpool layer, before using a fully-connected layer. Several \u0026ldquo;convolutional layer - maxpool layer\u0026rdquo; units can be stacked together before using a fully-connected (FC) layer. Note that an activation layer (often ReLU) is generally inserted between the the convolutional and the maxpool layer.\nsource: https://towardsdatascience.com/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69\nUsing Flux.jl Flux.jl is a leading machine learning package in the Julia ecosystem. In what follows, we load both the train and the test samples of the MNIST dataset. The train sample is a set of images used to fine-tune the parameters of the CNN, while the test sample contains images used to check that we did not overfit the train sample. A smoking gun for overfitting is when the accuracy in the train sample is much better than the accuracy using images from the test sample.\nusing Flux, Flux.Data.MNIST, Statistics using Flux: onehotbatch, onecold, crossentropy, throttle using Base.Iterators: repeated, partition using Printf, BSON using ImageView # Load labels and images from Flux.Data.MNIST # Train set: images used to estimate the CNN train_labels = MNIST.labels(:train) train_imgs = MNIST.images(:train); # Test set: images used to see how well the CNN perform \u0026quot;out-of-the-sample\u0026quot; test_imgs = MNIST.images(:test) test_labels = MNIST.labels(:test) print(\u0026quot;Images in the train set: $(size(train_imgs))\u0026quot;) print(\u0026quot;Images in the test set: $(size(test_imgs))\u0026quot;) # Visualization of one digit NROWS, NCOLS = 28, 28 a = reshape(train_imgs[1], NROWS, NCOLS)  Images in the train set: (60000,)Images in the test set: (10000,)  CNN architecture Our CNN has the usual CONV-\u0026gt;ReLU-\u0026gt;MaxPool components, before using a FC layer. We use a $1 \\times 1$ padding and a stride of $1$ (the default value). The size of input is gradually reduced by using $2 \\times 2$ maxpool layers. The default activation in Flux.jl is the function is $ x-\u0026gt;x $. Here, we use the Rectified Linear Unit function (ReLU) instead:\nmodel = Chain( # First convolution, operating upon a 28x28 image Conv((3, 3), 1=\u0026gt;16, pad=(1,1), relu), MaxPool((2,2)), #maxpooling # Second convolution, operating upon a 14x14 image Conv((3, 3), 16=\u0026gt;32, pad=(1,1), relu), MaxPool((2,2)), #maxpooling # Third convolution, operating upon a 7x7 image Conv((3, 3), 32=\u0026gt;32,pad=(1,1), relu), MaxPool((2,2)), # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N) # which is where we get the 288 in the `Dense` layer below: x -\u0026gt; reshape(x, :, size(x, 4)), Dense(288, 10), # Softmax to get probabilities softmax, )  The ReLU activation function is a piece-wise linear function. In the “ImageNet Classification with Deep Convolutional Neural Networks\u0026rdquo; paper by Krizhevsky and coauthors, the authors write:\n we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units.\n The ReLU activation function also helps in reducing the practical issues caused by the vanishing gradient problem. That is, the failure of the minizimation algorithm used to find the parameters of our CNN. Below is a plot of the ReLU activation function:\nxgrid = collect(range(-1, 1, length=100)) plot(xgrid, NNlib.relu.(xgrid), label = \u0026quot;relu(x)\u0026quot;, title=\u0026quot;ReLU activation function\u0026quot;, xlabel=\u0026quot;x\u0026quot;)  Training Batching The batch size is a parameter that tells us how many images the network will \u0026ldquo;see\u0026rdquo; at once when \u0026ldquo;training\u0026rdquo;. In technical terms, when performing gradient descent, we don\u0026rsquo;t use all the information at once (because of memory limitations and because it is not necessarily efficient). The following function generates \u0026ldquo;batches\u0026rdquo; of images:\n# Bundle images together with labels and group into minibatchess function make_minibatch(X, Y, idxs) X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs)) for i in 1:length(idxs) X_batch[:, :, :, i] = Float32.(X[idxs[i]]) end Y_batch = onehotbatch(Y[idxs], 0:9) return (X_batch, Y_batch) end # The CNN only \u0026quot;sees\u0026quot; 128 images at each training cycle: batch_size = 128 mb_idxs = partition(1:length(train_imgs), batch_size) # train set in the form of batches train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]; # train set in one-go: used to calculate accuracy with the train set train_set_full = make_minibatch(train_imgs, train_labels, 1:length(train_imgs)); # test set: to check we do not overfit the train data: test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs));  Loss function and minimization For the CNN to \u0026ldquo;learn\u0026rdquo; anything at all, it must have a notion of \u0026ldquo;wrong\u0026rdquo; or \u0026ldquo;right\u0026rdquo;. The loss function does exactly that, by quantifying how well the model performs at recognizing digits. When the output is a probability, the cross entropy loss function is appropriate. The final step is to select an algorithm to minimize the loss function. Here, let\u0026rsquo;s select the ADAM algorithm, which I understand as some sort of Stochastic Gradient Descent with momentum and adaptive learning rate:\n# `loss()` calculates the crossentropy loss between our prediction `y_hat` # We augment the data a bit, adding gaussian random noise to our image to make it more robust. function loss(x, y) # Add some noise to the image # we reduce the risk of overfitting the train sample by doing so: x_aug = x .+ 0.1f0*gpu(randn(eltype(x), size(x))) y_hat = model(x_aug) return crossentropy(y_hat, y) end accuracy(x, y) = mean(onecold(model(x)) .== onecold(y)) # ADAM optimizer opt = ADAM(0.001);  This block \u0026ldquo;train\u0026rdquo; (fine-tune the CNN parameter values) the model until a pre-determined accuracy level is reached:\nbest_acc = 0.0 last_improvement = 0 accuracy_target = 0.97 #Set an accuracy target. When reached, we stop training. max_epochs = 100 #Maximum for epoch_idx in 1:100 global best_acc, last_improvement # Train for a single epoch Flux.train!(loss, params(model), train_set, opt) # Calculate accuracy: acc = accuracy(train_set_full...) @info(@sprintf(\u0026quot;[%d]: Train accuracy: %.4f\u0026quot;, epoch_idx, acc)) # Calculate accuracy: acc = accuracy(test_set...) @info(@sprintf(\u0026quot;[%d]: Test accuracy: %.4f\u0026quot;, epoch_idx, acc)) # If our accuracy is good enough, quit out. if acc \u0026gt;= accuracy_target @info(\u0026quot; -\u0026gt; Early-exiting: We reached our target accuracy of $(accuracy_target*100)%\u0026quot;) break end if epoch_idx - last_improvement \u0026gt;= 10 @warn(\u0026quot; -\u0026gt; We're calling this converged.\u0026quot;) break end end  ┌ Info: [1]: Train accuracy: 0.9579 └ @ Main In[14]:12 ┌ Info: [1]: Test accuracy: 0.9605 └ @ Main In[14]:16 ┌ Info: [2]: Train accuracy: 0.9749 └ @ Main In[14]:12 ┌ Info: [2]: Test accuracy: 0.9756 └ @ Main In[14]:16 ┌ Info: -\u0026gt; Early-exiting: We reached our target accuracy of 97.0% └ @ Main In[14]:20  Predictions Once the model is trained, predicted values are easily obtained as follows:\n# Get predictions and convert data to Array: pred = Tracker.data(model(test_set[1])); # Show the first 5 predictions # One column is an image # Each row corresponds to the probability of a digit pred[:,1:5] # Function to get the row index of the max value: f1(x) = getindex.(argmax(x, dims=1), 1) # Final predicted value is the one with the maximum probability: pred = f1(pred) .- 1; #minus 1 because the first element is 0  Let\u0026rsquo;s see how the model performs on the test set. Can the CNN recognize digits using images that were not used when training the model? As you can see below, our model does an amazing job at recognizing hand-written digits:\nprintln(\u0026quot;Predicted value = $(pred[1])\u0026quot;) a = reshape(test_imgs[1], NROWS, NCOLS)  Predicted value = 7  println(\u0026quot;Predicted value = $(pred[2])\u0026quot;) a = reshape(test_imgs[2], NROWS, NCOLS)  Predicted value = 2  println(\u0026quot;Predicted value = $(pred[3])\u0026quot;) a = reshape(test_imgs[3], NROWS, NCOLS)  Predicted value = 1  Accuracy checks We now have a model that seems to do quite a good job in recognizing digits. But can we improve it? If yes, how? To improve our model, we first need to identify when and why it fails.\nConfusion matrix To do that, a useful reporting tool is a confusion matrix. Each row of a confusion matrix shows instances of the true value, while each column displays instances of the predicted value. Ideally, we would like our model to perfectly predict the outcome. With a perfect model, all instances would be located on the diagonal elements of the confusion matrix.\nThe last time I checked, Flux.jl did not have an in-built function to calculate confusion matrices. Fortunately, an implementation is available in the package MLBase. The next block of code calculates the confusion matrix and displays it. Most of instances are on located on the diagonal, which is not a surprise given that the accuracy rate for our model is more than $97.0\\%$\nusing MLBase # Adding 1 to outcome because the index 0 in arrays does not exist in Julia Cm = confusmat(10, test_labels .+ 1, vec(pred) .+ 1)  10×10 Array{Int64,2}: 968 1 1 0 0 4 3 1 2 0 0 1128 3 0 0 0 1 0 3 0 3 5 1003 6 1 1 0 6 7 0 0 0 1 992 0 10 0 2 4 1 0 1 2 0 972 0 0 1 1 5 1 0 1 4 0 883 1 1 1 0 1 4 0 0 1 13 936 0 3 0 1 7 10 5 0 1 0 986 3 15 2 0 4 6 4 8 2 4 942 2 4 4 0 7 7 10 0 8 2 967  # Normalize output: Cm = Cm ./ sum(Cm, dims=2) # Labels xs = [string(i) for i = 0:9] heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma)  To visualize where our model makes mistakes, one can use the optional argument clim, to put an upper bound on the underlying colormap. For instance, the next plot shows that our model has troubles differencing 7 and 2 or 8 and 2.\n# Limits to colormap, so we can see where errors are located: xs = [string(i) for i = 0:9] heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma, clim=(0., 0.01))  Error Analysis The next block of code displays digits for which our CNN failed:\n# indices for errors: using ImageView, Gtk.ShortNames mistakes = test_labels .!= vec(pred) max_images = 5 grid, frames, canvases = canvasgrid((1,max_images)); # 1 row  k=0#counter for mistakes for (j, i) in enumerate(mistakes) if i == true k+=1 # a false value has been found println(\u0026quot;Predicted value = $(pred[j])\u0026quot;) println(\u0026quot;True value = $(test_labels[j])\u0026quot;) imshow(canvases[1,k], test_imgs[j]) idx = ImageView.annotate!(guidict, AnnotationText(0, 0, \u0026quot;$(pred[j])\u0026quot;, color=RGB(0,0,0), fontsize=3)) end if k \u0026gt;= max_images break end end win = Window(grid); Gtk.showall(win);  Predicted value = 5 True value = 9 Predicted value = 5 True value = 6 Predicted value = 4 True value = 8 Predicted value = 3 True value = 2 Predicted value = 7 True value = 2  While it seems obvious that the two digits starting from the left are a 9 and a 6, the remaining 3 elements are not trivial. The 8 in the middle could be easily confused with something else and the two remaining digits are weirdly shaped.\nConclusion When dealing with images, a convolutional neural network generally does an amazing job at recognizing patterns. This blog post was a non-technical introduction to the topic. While Python is the tool of predilection in machine learning (Keras, TensorFlow, etc.), my guess is that Julia will get increasingly popular because Julia is both easy to use and fast.\nReferences  This blog post is heavily based on this Flux.jl tutorial: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl On the links between CNN and PDEs: https://mitmath.github.io/18337/lecture14/pdes_and_convolutions A full course on CNN. Most of the content is available online: http://cs231n.github.io/convolutional-networks/  ","date":1575568402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575568402,"objectID":"ef8aeccdbe56db07f0af0a93a9e02c54","permalink":"https://julienpascal.github.io/post/cnn/","publishdate":"2019-12-05T18:53:22+01:00","relpermalink":"/post/cnn/","section":"post","summary":"For a specific project on the housing market (here), I had to analyze thousands of photos. To do that, I used a convolutional neural network (CNN), which is a fancy name for a complicated function that can be \u0026ldquo;trained\u0026rdquo; to recognize patterns in images. In this blog post, I would like to introduce the \u0026ldquo;Hello World\u0026rdquo; of computer vision and CNN: the classification of hand-written digits from the MNIST dataset.","tags":["Machine Learning","Computer Vision"],"title":"A Primer on Computer Vision","type":"post"},{"authors":[],"categories":[],"content":" The logistic model (also called logit model) is a natural candidate when one is interested in a binary outcome. For instance, a researcher might be interested in knowing what makes a politician successful or not. For the purpose of this blog post, \u0026ldquo;success\u0026rdquo; means the probability of winning an election. In that case, it would be sub-optimal to use a linear regression model to see what factors are associated with successful politicians, as the outcome variable is binary (a politician either wins or loses an election). The linear model is built around the idea that the outcome variable is continuous.\nWhat if the statistician tries to identify what factors are influencing the probability of winning? This strategy naturally lends itself to using a logistic model (or a probit). In this blog post, I derive the logistic model from scratch and show how one can estimate its parameters using gradient descent or Newton-Raphson algorithms. I also use data on NBA players to see what factors are influencing the success of a shot. The GitHub repository for this post can be found here.\nThe logistic model The outcome variable $y_i$ is either $1$ (\u0026ldquo;winning\u0026rdquo;) or $0$ (\u0026ldquo;losing\u0026rdquo;). The logistic model makes the assumption that the probability of winning is given by the logistic function :\n$$ f(y_i | x_{i}, \\theta_{i}) = \\sigma(x_{i} \u0026lsquo;\\theta)$$\nwith $\\sigma(v) = \\frac{exp(v)}{1+exp(v)}$\nThe probability of losing is 1 minus the probability of wining:\n$$ f(y_i | x_{i}, \\theta) = 1 - \\sigma(x_{i} \u0026lsquo;\\theta)$$\nA latent variable formulation A powerful way of interpreting the logistic model is to see it as the outcome of a latent variable model. An unobservable latent variable $z_{i}$ depends linearly on $x_{i}$ plus a noise term $\\varepsilon_{i}$:\n$$ z_{i} = x_{i} \u0026lsquo;\\theta + \\varepsilon_{i} $$\nWe only observe $y_i$, which is equal to 1 when $z_{i}$ is strictly positive, and 0 otherwise. If the error term is distributed according to the logistic distribution, we end up with the logistic model described above. If the error term is normally distributed, the model is a probit model. To see that, simply express the probability of the latent variable to be bigger than 0:\n$$ f(y_i | x_{i}, \\theta_{i}) = P( x_{i} \u0026lsquo;\\theta + \\varepsilon_{i} \u0026gt; 0) $$ $$ = 1 - P( x_{i} \u0026lsquo;\\theta + \\varepsilon_{i} \\leq 0) $$ $$ = 1 - P(\\varepsilon_{i} \\leq - x_{i} \u0026lsquo;\\theta ) $$ $$ = 1 - P(\\varepsilon_{i} \\leq - x_{i} \u0026lsquo;\\theta ) $$ $$ = \\frac{exp(x_{i} \u0026lsquo;\\theta )}{1+exp(x_{i} \u0026lsquo;\\theta )} $$\nwhere the last line comes from using the expression for the cdf of the logistic distribution with zero mean and scale parameter equal to 1.\nInterpretation of coefficients How can we read the coefficients from a logistic model? The marginal effect of a change in $x_{ij}$ (the $jth$ component of $x_i$) on the probability that $y_i = 1$ is given by:\n$$ \\frac{\\partial f(y_i | x_{i}, \\theta)}{\\partial x_{ij}} = \\sigma(x_{i} \u0026lsquo;\\theta)(1-\\sigma(x_{i} \u0026lsquo;\\theta))\\theta_j$$\nA first observation is that the marginal effect depends on $x_i$, unlike in the linear regression model. A second observation is that the first two terms are always positive, so we do have that the interpretation that if $\\theta_j$ is positive, an increase in the $jth$ component of $x_i$ leads to a bigger probability of obtaining a success (holding everything else constant).\nAnother way to read the results from a logistic model is to realize that it implies that the log of odd ratio is linear:\n$$ log\\Big(\\frac{f(y_i | x_{i}, \\theta)}{1 - f(y_i | x_{i}, \\theta)}\\Big) = x_{i} \u0026lsquo;\\theta$$\nGoing back to what makes a politician successful in an election, if the coefficient $\\theta_j$ is equal to 0.1, it means that a one unit increase in $x_{ij}$ rises the relative probability of winning an election by approximately $10\\%$.\nLog-likelihood function To predict who is going to win the next elections, one must estimate the value of $\\theta$ using the information contained in the sample $(y_i, x_i)_{i=1}^{N}$. One \u0026ldquo;natural\u0026rdquo; criterion is to find the value for $\\theta$ that maximizes the probability of observing the sample. This procedure is called Maximum likelihood estimation. Let us assume that sample is i.i.d. If the i.i.d assumption holds, the probability of observing the sample $(y_i, x_i)_{i=1}^{N}$ is the product of the probability of observing each observation. Instead of maximizing the likelihood, it is more convenient to maximize the log-likelihood, which transforms the product of probabilities into a sum:\n$$ L((y_i, x_i)_{i=1}^{N};\\theta) = log( \\prod_{i=1}^{N}f(y_i | x_{i}, \\theta)) = \\sum_{i=1}^{N} log(f(y_i | x_{i}, \\theta_{i}))$$\nThe probability of observing $y_i$ can compactly be written as\n$$ f(y_i | x_{i}, \\theta_{i}) = \\sigma(x_{i} \u0026lsquo;\\theta)^{y_i}(1 - \\sigma(x_{i} \u0026lsquo;\\theta))^{1 - y_i} $$\nHence, the log-likelihood function writes:\n$$L((y_i, x_i)_{i=1}^{N};\\theta) = \\sum_{i=1}^{N} y_i log(\\sigma(x_{i} \u0026lsquo;\\theta)) + (1 - y_i)log(1 - \\sigma(x_{i} \u0026lsquo;\\theta))$$\nMaximum Likelihood Estimation Taking the derivative of $f(y_i | x_{i}, \\theta)$ with respect to the parameter $\\theta$ gives:\n$$ f_{\\theta}(y_i | x_{i}, \\theta) = [y_i - \\sigma(x_{i} \u0026lsquo;\\theta)] x_{i} $$\nand the derivative of the log-likelihood function with respect to $\\theta$ is:\n$$ L_{\\theta}((y_i, x_i)_{i=1}^{N};\\theta) = \\sum_{i=1}^{N}[y_i - \\sigma(x_{i} \u0026lsquo;\\theta)] x_{i} $$\nGradient descent To make the link with this blog post, we can use gradient descent to find the MLE estimate:\n$$ \\theta_{i+1} = \\theta_{i} - \\gamma \\Big(- L_{\\theta}((y_i, x_i)_{i=1}^{N};\\theta_{i}) \\Big)$$\nThe gradient descent algorithm is an iterative procedure to find a minimizer of a function. At each step, the algorithm takes a step of length $\\gamma$ towards the direction of steepest descent. Note that I reformulated the problem of finding the maximum of a function $f$ (the log-likelihood) as the problem of finding the minimum of $-f$.\nNewton–Raphson method Roughly speaking, the Newton-Raphson method is a \u0026ldquo;smart\u0026rdquo; gradient descent which uses the information contained in the Hessian of the log-likelihood $HL((y_i, x_i)_{i=1}^{N};\\theta_{i})$ (on top of the gradient) to make a right move toward the minimizer. This iterative algorithm proceeds as follows:\n$$ \\theta_{i+1} = \\theta_{i} - (HL((y_i, x_i)_{i=1}^{N};\\theta_{i}) )^{-1} \\Big(- L_{\\theta}((y_i, x_i)_{i=1}^{N};\\theta_{i}) \\Big)$$\nThe next plot shows how the Newton-Raphson method works for a one dimensional root-finding problem:\nsource: https://en.wikipedia.org/wiki/Newton%27s_method\nShould we use gradient descent or Newton-Raphson? I let the following extract from the Wikipedia article on Newton-Raphson speak for itself:\n Where applicable, Newton\u0026rsquo;s method converges much faster towards a local maximum or minimum than gradient descent. In fact, every local minimum has a neighborhood N such that, if we start with x0 ∈ N, Newton\u0026rsquo;s method with step size γ = 1 converges quadratically (if the Hessian is invertible and a Lipschitz continuous function of x in that neighborhood).\n For the logistic model, the Newton-Raphson algorithm is easily applicable because there exists a closed-form formula for the Hessian:\n$$ HL((y_i, x_i)_{i=1}^{N};\\theta_{i}) = \\sum_{i=1}^{N} - \\sigma(x_{i} \u0026lsquo;\\theta)[1 - \\sigma(x_{i} \u0026lsquo;\\theta)] x_{i} x_{i}\u0026lsquo;$$\nImplementation in Julia I. Working with simulated data Let\u0026rsquo;s first work with simulated data. Can we actually recover the true parameter values using a manual implementation of the logistic model?\nLet\u0026rsquo;s load a few dependencies:\nusing Distributions using Plots pyplot() using DataFrames using GLM using Optim using CSV using GLM  Let\u0026rsquo;s create the logistic function:\n# Logistic function for a scalar input: function sigma(x::Float64) exp(x)/(1.0 + exp(x)) end # Logistic function for a vector input: function sigma(x::Array{Float64,1}) exp.(x) ./ (1.0 .+ exp.(x)) end  Let\u0026rsquo;s create a function that calculates the likelihood:\nfunction log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1}) sum = 0.0 #Loop over individuals in the sample for i=1:size(X,1) sum += y[i]*log(sigma(transpose(X[i,:])*theta)) + (1.0 - y[i])*log(1.0 - sigma(transpose(X[i,:])*theta)) end return sum end  Let\u0026rsquo;s create a function that returns the derivative of the log-likelihood of the sample, which we need for the gradient descent algorithm:\n# Function to calculate the gradient of the log-likelihood of the sample: function derivative_log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1}) sum = zeros(size(X,2)) #Loop over individuals in the sample for i=1:size(X,1) sum .+= (y[i] - sigma(transpose(X[i,:])*theta))*X[i,:] end return sum end  Let\u0026rsquo;s create a function that returns the Hessian of the log-likelihood of the sample, which we need for the Newthon-Raphson algorithm:\n# Function to calculate the hessian of the log-likelihood of the sample: function hessian_log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1}) hessian = zeros(size(X,2), size(X,2)) #Loop over individuals in the sample for i=1:size(X,1) hessian .+= - sigma(transpose(X[i,:])*theta)*(1.0 - sigma(transpose(X[i,:])*theta))*(X[i,:]*transpose(X[i,:])) end return hessian end  Let\u0026rsquo;s simulate a sample of individuals:\n#Generation of a sample: #---------------------- N_individuals = 10000 #how many individuals in the sample? dim_X = 3 #How many dimensions for x d = Normal(0.0, 1.0) d_logistic = Logistic(0.0, 1.0) # Generate true parameter values: theta0 = [0.0; 1.0; 2.0];  # Generate X: X = rand(d, N_individuals, dim_X) # The first column is full one ones (to have a constant) X[:,1] = ones(N_individuals);  # Convert y to a binary outcome using the latent variabe representation: proba_success = X*theta0 .+ rand(d_logistic, N_individuals) y = ifelse.(proba_success .\u0026gt; 0.0, 1.0, 0.0);  p1 = histogram(proba_success, bins=20, normalize=true, title=\u0026quot;Pdf probability of success\u0026quot;, legend=false) p2 = histogram(y, title=\u0026quot;Nb of successes vs failures\u0026quot;, legend=false) plot(p1,p2)  Maximization with Optim As a first pass, we can maximize the log-likelihood using the package Optim. I use the the LBFGS algorithm:\ntheta_guess = ones(dim_X) @time res = optimize(theta -\u0026gt; - log_likelihood(y, X, theta), theta_guess, LBFGS())  0.409340 seconds (3.79 M allocations: 396.250 MiB, 15.73% gc time)\nWe successfully recover the true parameter values (see theta0):\nprint(\u0026quot;Estimate for theta using Optim is $(res.minimizer)\u0026quot;)  Estimate for theta using Optim is [-0.0312666, 0.996344, 1.96038]  Minimization with gradient descent Let\u0026rsquo;s implement the gradient descent algorithm within a function:\nfunction gradient_descent_probit(y, X , theta_initial::Array{Float64,1}; max_iter::Int64 = 1000, learning_rate::Float64 = 0.000001, tol::Float64=0.01) #initial value for theta: theta_old = theta_initial theta_new = similar(theta_old) #convergence reached? success_flag = 0 #Let's store the convergence history history= fill!(zeros(max_iter), NaN) for i=1:max_iter theta_new = theta_old + learning_rate*derivative_log_likelihood(y, X, theta_old) diff = maximum(abs, theta_new .- theta_old) history[i] = diff if diff \u0026lt; tol success_flag = 1 break end theta_old = theta_new end return theta_new, success_flag, history[isnan.(history) .== false] end  theta_guess = zeros(dim_X) @time theta, flag, history = gradient_descent_probit(y, X, theta_guess, max_iter=100000, learning_rate=0.0001, tol=0.00001);   0.252281 seconds (4.89 M allocations: 523.159 MiB, 29.08% gc time)  The following graph shows the error as a function of the number of iterations. After a few iterations of the gradient descent algorithm, the error is quite small.\nplot(history, label= \u0026quot;error\u0026quot;, title = \u0026quot;Convergence of the gradient descent algorithm\u0026quot;)  print(\u0026quot;Estimate for theta using gradient descent is $(theta)\u0026quot;)  Estimate for theta using gradient descent is [0.0581777, 1.00105, 1.97901]  Minimization with Newton-Raphson Let\u0026rsquo;s implement the Newton-Raphson algorithm within a function:\nfunction nr_probit(y, X , theta_initial::Array{Float64,1}; max_iter::Int64 = 1000, tol::Float64=0.01) #initial value for theta: theta_old = theta_initial theta_new = similar(theta_old) #convergence reached? success_flag = 0 #Let's store the convergence history history= fill!(zeros(max_iter), NaN) for i=1:max_iter theta_new = theta_old - inv(hessian_log_likelihood(y, X, theta_old))*derivative_log_likelihood(y, X, theta_old) diff = maximum(abs, theta_new .- theta_old) history[i] = diff if diff \u0026lt; tol success_flag = 1 break end theta_old = theta_new end return theta_new, success_flag, history[isnan.(history) .== false] end  The following graph shows that we find the minimizer in only 5 steps! The Newton-Raphson algorithm clearly outperforms gradient descent. Of course, everything works well because the problem is well-behaved and a nice formula for the Hessian is available.\ntheta_guess = ones(dim_X) @time theta, flag, history = nr_probit(y, X, theta_guess, max_iter=1000, tol=0.00001); plot(history, label= \u0026quot;error\u0026quot;, title = \u0026quot;Convergence of the gradient descent algorithm\u0026quot;)   0.037832 seconds (500.07 k allocations: 53.431 MiB, 35.44% gc time)  print(\u0026quot;Estimate for theta using Newton-Raphson is $(theta)\u0026quot;)  Estimate for theta using Newton-Raphson is [0.0581789, 1.00114, 1.97919]  Using GLM We can also use the package GLM to estimate the logistic model. We first need to put the data into a dataframe. In the glm() function, we should use the LogitLink()\ndf = DataFrame(X1=X[:,1],X2=X[:,2], X3=X[:,3], y=y); first(df,6)  X1X2X3yFloat64Float64Float64Float646 rows × 4 columns\n11.00.2153151.298171.021.0-0.07147490.5868861.031.00.0463648-0.1451161.041.01.950960.263491.051.0-0.1620811.348711.061.00.00214326-0.2718351.0 fittedmodel = glm(@formula(y ~ X2 + X3), df, Binomial(), LogitLink(), verbose=true);  print(\u0026quot;Estimate for theta using GLM is $(coef(fittedmodel))\u0026quot;)  Estimate for theta using GLM is [-0.0312666, 0.996344, 1.96038]  II. What makes a successful NBA player? For an example involving real data, I use the data set on NBA shots taken during the 2014-2015 season. It contains information on:\n who took the shot where on the floor was the shot taken from who was the nearest defender, how far away was the nearest defender time on the shot clock etc.  The data is available on Kaggle here\ndf_nba = CSV.read(\u0026quot;/home/julien/Documents/REPOSITORIES/LogisticRegression/data/shot_logs.csv\u0026quot;); names(df_nba)  The dataset is quite extensive. Let\u0026rsquo;s select whether or not the shot was successful, the shot clock, the shot distance, and the proximity with the closest defender:\ndf_nba = df_nba[[:SHOT_RESULT, :SHOT_CLOCK, :SHOT_DIST, :CLOSE_DEF_DIST]] # Drop rows with missings: df_nba = dropmissing(df_nba); # Drop rows with NaN: df_nba = df_nba[completecases(df_nba), :] # Convert SHOT_RESULT to a binary variable (1 for success, 0 for missed) df_nba[:, :SHOT_RESULT] = ifelse.(df_nba[:, :SHOT_RESULT] .== \u0026quot;made\u0026quot;, 1.0, 0.0); # Show the first few rows of df_nba: first(df_nba, 4)  SHOT_RESULTSHOT_CLOCKSHOT_DISTCLOSE_DEF_DISTFloat64Float64Float64Float644 rows × 4 columns\n11.010.87.71.320.03.428.26.130.010.317.23.440.010.93.71.1 Let\u0026rsquo;s first use GLM:\nfittedmodel = glm(@formula(SHOT_RESULT ~ SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST), df_nba, Binomial(), LogitLink(), verbose=true); fittedmodel  SHOT_RESULT ~ 1 + SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST Coefficients: ──────────────────────────────────────────────────────────────────────────────────── Estimate Std. Error z value Pr(\u0026gt;|z|) Lower 95% Upper 95% ──────────────────────────────────────────────────────────────────────────────────── (Intercept) -0.0575127 0.0181349 -3.17139 0.0015 -0.0930564 -0.021969 SHOT_CLOCK 0.0185198 0.00104899 17.6549 \u0026lt;1e-69 0.0164639 0.0205758 SHOT_DIST -0.059745 0.000858282 -69.61 \u0026lt;1e-99 -0.0614272 -0.0580628 CLOSE_DEF_DIST 0.108392 0.00279232 38.8179 \u0026lt;1e-99 0.102919 0.113865 ────────────────────────────────────────────────────────────────────────────────────  How can we interpret those results?\n time pressure makes NBA players more successful: the higher the shot clock, the more likely to score shots from further away are more likely to be missed the further away the closest defender is, the more likely the shot will be a success  Can we find similar results \u0026ldquo;manually\u0026rdquo;? The answer is yes. To see that, let\u0026rsquo;s first create the binary variable y and put the explanatory variables into X and then use Newton-Raphson:\ny = convert(Array, df_nba[:SHOT_RESULT]); X = convert(Matrix, df_nba[[:SHOT_CLOCK, :SHOT_DIST, :CLOSE_DEF_DIST]]) X = hcat(ones(size(X,1)), X);  theta_guess = zeros(size(X,2)) @time theta, flag, history = nr_probit(y, X, theta_guess, max_iter=1000, tol=0.0001); plot(history, label= \u0026quot;error\u0026quot;, title = \u0026quot;Convergence of the gradient descent algorithm\u0026quot;)   0.301748 seconds (4.90 M allocations: 568.272 MiB, 29.93% gc time)  Our implementation of the logistic model gives us parameter values that are almost identical to the ones we get using the package GLM:\nprint(\u0026quot;Estimate for theta using Optim is $(res.minimizer)\u0026quot;)  Estimate for theta using Optim is [-0.057513, 0.0185199, -0.0597451, 0.108392]  Conclusion The logistic model, often used in social sciences and in machine learning for classification purposes is a powerful tool. This blog post shows how the logistic model can be derived from first principles (latent variable interpretation) and how it can be implemented in just a few lines of codes. A few extensions to this blog post could be to calculate the ROC curve and to calculate the standard errors.\nReferences  https://rpubs.com/junworks/Understanding-Logistic-Regression-from-Scratch  ","date":1574445202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574445202,"objectID":"6412a3e2857df9de1a8a4bfeddb695a9","permalink":"https://julienpascal.github.io/post/logistic/","publishdate":"2019-11-22T18:53:22+01:00","relpermalink":"/post/logistic/","section":"post","summary":"The logistic model (also called logit model) is a natural candidate when one is interested in a binary outcome. For instance, a researcher might be interested in knowing what makes a politician successful or not. For the purpose of this blog post, \u0026ldquo;success\u0026rdquo; means the probability of winning an election. In that case, it would be sub-optimal to use a linear regression model to see what factors are associated with successful politicians, as the outcome variable is binary (a politician either wins or loses an election).","tags":["Econometrics","Classification"],"title":"Logistic Regression from Scratch","type":"post"},{"authors":null,"categories":null,"content":"Using a novel dataset on the rental housing market in the Paris area, I show that the rental housing market is well described by a directed search model. I develop a hedonic pricing model taking into consideration apartment features and subjective attractiveness using photos and computer vision techniques from the machine learning literature.\n\n","date":1571677867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571677867,"objectID":"f4948a7bac4472267cf44a4edc9b3898","permalink":"https://julienpascal.github.io/project/rentalmarket/","publishdate":"2019-10-21T18:11:07+01:00","relpermalink":"/project/rentalmarket/","section":"project","summary":"I explore the rental housing market in the Paris area using a novel dataset.","tags":["Housing","Directed Search"],"title":"Rental Housing Market and Directed Search","type":"project"},{"authors":[],"categories":[],"content":" In my quest for the perfect tool for reproducible science, I thought that the silver bullet was to wrap your code in a neat library/package and make it available to the world. Yet, I was wrong. Docker is a much cooler and a much more effective way for sharing your work with a broad audience. This post is a 101 introduction to Docker. I describe what is Docker and show one simple application with a script in Julia.\nWhy Docker? Your script works locally, but not on your friend\u0026rsquo;s laptop because of dependency issues. Docker solves the dependency hell by giving you the opportunity to \u0026ldquo;ship\u0026rdquo; your application in a \u0026ldquo;container\u0026rdquo;. One can think of a \u0026ldquo;container\u0026rdquo; as some sort of lightweight virtual image. Some technical details can be found here and here. In a nutshell, if your application works on your local machine, Docker helps you to put your application inside an container. Once in a container, your application will run smoothly for the rest of the world.\nApplication in Julia The goal is to create a container with a simple script to calculate an approximation of π. Here I am making a copy-paste from this post, in which I calculated an approximation of π using Monte-Carlo. I create a folder julia-app, which contains 3 files (see the Github repository with the 3 files here)\njulia-app ├ app.jl ├ deps.jl └ Dockerfile  The file app.jl contains the application we want to containerize. The file deps.jl contains the list of libraries/packages that are used within app.jl. The file Dockerfile is a text document that contains instructions to build the container. Generally, a Dockerfile contains 4 types of instructions:\n FROM: specifies the \u0026ldquo;base image\u0026rdquo; we want to use within the container. In our case, we want to run an application with Julia. Luckily, we can pull a base image with Julia pre-installed on it using FROM julia:\u0026lt;julia-version-you-want\u0026gt; COPY: adds files to your container RUN: executes command(s) in a new layer and creates a new image. RUN is perfect for installing packages CMD: specifies what command to run within the container  #choose a base image FROM julia:1.0.3 # install julia dependencies COPY deps.jl /usr/src/app/ RUN julia /usr/src/app/deps.jl # copy files required for the app to run COPY app.jl /usr/src/app/ # run the application CMD [\u0026quot;julia\u0026quot;, \u0026quot;/usr/src/app/app.jl\u0026quot;]  To create the container, use the command docker build. You can give a name to your container using the --tag , -t option:\ndocker build julia-app -t \u0026lt;your-tag\u0026gt;  To run the container you have just created:\ndocker run --name julia-app \u0026lt;your-tag\u0026gt;  After a few seconds, you should see an approximation of π showing up in your terminal. Voilà, you have have just created your first container. The next step is to put your container on DockerHub. Here is a tutorial on how to do it.\nConclusion From an academic perspective, Docker solves the dependency hell and helps in producing reproducible research. I will try to use it more often for sharing my own research.\n","date":1571421202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571421202,"objectID":"2aabea37fabfe37f2b03331f183d091d","permalink":"https://julienpascal.github.io/post/docker/","publishdate":"2019-10-18T18:53:22+01:00","relpermalink":"/post/docker/","section":"post","summary":"In my quest for the perfect tool for reproducible science, I thought that the silver bullet was to wrap your code in a neat library/package and make it available to the world. Yet, I was wrong. Docker is a much cooler and a much more effective way for sharing your work with a broad audience. This post is a 101 introduction to Docker. I describe what is Docker and show one simple application with a script in Julia.","tags":[],"title":"Docker for Dummies","type":"post"},{"authors":[],"categories":[],"content":" Coming from an Economics/Econometrics background, I have always been a bit puzzled by the way several Machine Learning (ML) textbooks I have read solve the ordinary least squares model (OLS). When considering a linear model of the form:\n$$ y = X \\beta + e $$\nwith $e$ a zero-mean noise term, the closed-form solution associated with minimizing the mean square error is:\n$$ \\beta = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\nSeveral ML textbooks explain that a recursive algorithm (see below) may be used to solve for $\\beta$. Why not directly using the analytical formula to calculate an estimate of $\\beta$ ? While feasible with \u0026ldquo;small\u0026rdquo; datasets (not too many explanatory variables and/or observations), direct inversion of $X\u0026rsquo;X$ is not recommended when working with thousands of explanatory variables and/or billions of observations. The alternative is to use gradient descent, or better, stochastic gradient descent.\nIn this short post, I solve OLS the \u0026ldquo;machine-learning way\u0026rdquo;. That is, using (stochastic) gradient descent. The idea for gradient descent (GD) is quite intuitive. The gradient of $f$ at a given point tells us the direction of greatest increase for $f$ at this point. Hence, moving in the opposite direction (minus the gradient) is probably a good idea to find a local minimum. And indeed it is. The GD algorithm repetitively applies this procedure until a minimum (hopefully global) is found. Starting from an initial guess for $\\beta$, one updates the guess using the following formula:\n$$ \\beta_{n} = \\beta_{n-1} - \\alpha * grad_{n}(\\beta_{n-1},X,y) $$\nwhere $\\alpha$ is a small value (the \u0026ldquo;learning rate\u0026rdquo;) and $grad_{n}(\\beta_{n-1},X,y)$ the gradient of the mean square error (another loss function can be used) evaluated at $\\beta_{n-1}$ using the observations $X$ and $y$. Using the mean square error as a loss function generates a closed-form solution for the gradient:\n$$ grad_{n}(\\beta_{n-1},X,y) = (X\u0026rsquo;X)\\beta - X\u0026rsquo;y $$\nA refinement of GD, especially handy when dealing with a large dataset, is to use only a subset of the full sample when calculating the gradient:\n$$ \\beta_{n} = \\beta_{n-1} - \\alpha * grad_{n}(\\beta_{n-1},X_n,y_n) $$\nwhere $X_n$ and $y_n$ are a randomly selected sub-sample of $X$ and $y$. Stochastic Gradient Descent (SGD) reduces the computational burden associated with computing the gradient, while still having good convergence properties, as illustrated in the application below.\nImplementation in Julia Let\u0026rsquo;s first load packages and define parameters\nusing LinearAlgebra using Distributions using Plots using Distributions using Random  n_points=10000 dim_input=100 #dim of input, without the intercept dim_output=1 # Normal noise d = Normal() # True parameters beta = rand(d, dim_input + 1); # Noise e = rand(d, n_points); # Input data: X = rand(d, (n_points,dim_input)); # Add the intercept: X = hcat(ones(n_points),X); #Linear Model y = X*beta .+ e;  This function calculates an estimate of $\\beta$ using the analytical formula for OLS\n#OLS way function OLS_direct(X::Array, y::Vector) inv(transpose(X)*X)*transpose(X)*y end  OLS_direct (generic function with 1 method)\n@time beta_hat = OLS_direct(X, y);  0.611144 seconds (1.85 M allocations: 96.576 MiB, 13.64% gc time)\nWithout any major surprise, using the analytical solution works perfectly well, as illustrated in the following plot\nplot(beta, beta_hat, seriestype=:scatter, label=\u0026quot;OLS (Direct)\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (Direct)\u0026quot;)  Gradient Descent Now it\u0026rsquo;s time to solve OLS the machine learning way. I first define a function that calculates the gradient of the loss function, evaluated at the current guess using the full sample. Then, a second function applies the GD updating rule.\n#Calculate the gradient function grad_OLS!(G, beta_hat, X, y) G[:] = transpose(X)*X*beta_hat - transpose(X)*y end  grad_OLS! (generic function with 1 method)\n#Gradient descent way function OLS_gd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false) #initial guess beta_hat = zeros(size(X,2)) grad_n = zeros(size(X,2)) for epoch=1:epochs grad_OLS!(grad_n, beta_hat, X, y) beta_hat -= r*grad_n if verbose==true if mod(epoch, round(Int, epochs/10))==1 println(\u0026quot;MSE: $(mse(beta_hat, X, y))\u0026quot;) end end end return beta_hat end  OLS_gd (generic function with 1 method)\nAs illustrated below, after 20 iterations we are quite close to the true value. After 100 iterations, values obtained by GD are indistinguishable from the true values.\n@time beta_hat_gd_20 = OLS_gd(X,y, epochs=20); @time beta_hat_gd = OLS_gd(X,y, epochs=100); plot(beta, beta_hat_gd_20, seriestype=:scatter, label=\u0026quot;GD (20 iter.)\u0026quot;) plot!(beta, beta_hat_gd, seriestype=:scatter, label=\u0026quot;GD (100 iter.)\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (GD)\u0026quot;)  0.137474 seconds (81.55 k allocations: 5.814 MiB) 0.466605 seconds (714 allocations: 8.225 MiB)\nStochastic Gradient Descent One issue associated with plain vanilla GD is that computing the gradient might be slow. Let\u0026rsquo;s now randomly select only a fraction of the full sample every time we iterate. Here, I take only 10 percent of the full sample.\n#Gradient descent way function OLS_sgd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false, batchsizePer::Int64=10) #initial guess beta_hat = zeros(size(X,2)) grad_n = zeros(size(X,2)) #how many draws from the dataset? batchsize = round(Int, size(X,1)*(batchsizePer/100)) Xn = zeros(batchsize, size(X,2)) yn = zeros(batchsize) for epoch=1:epochs indices = shuffle(Vector(1:size(X,1))) Xn = X[indices[1:batchsize],:] yn = y[indices[1:batchsize]] grad_OLS!(grad_n, beta_hat, Xn, yn) #gradient descent: beta_hat -= r*grad_n if verbose==true if mod(epoch, round(Int, epochs/10))==1 println(\u0026quot;MSE: $(mse(beta_hat, X, y))\u0026quot;) end end end return beta_hat end  OLS_sgd (generic function with 1 method)\nThe following block of code shows that SGD achieves the same degree of accuracy, while being much faster than GD.\n@time beta_hat_gd = OLS_gd(X,y, epochs=200); @time beta_hat_sgd = OLS_sgd(X,y, epochs=200, batchsizePer=20); plot(beta, beta_hat_sgd, seriestype=:scatter, label=\u0026quot;SGD\u0026quot;) plot!(beta, beta_hat_gd, seriestype=:scatter, label=\u0026quot;GD\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (SGD)\u0026quot;)  0.894465 seconds (1.41 k allocations: 16.448 MiB, 0.42% gc time) 0.513217 seconds (338.19 k allocations: 382.550 MiB, 4.60% gc time)\nConclusion The OLS analytical formula is the gold standard to derive theoretical properties and is perfectly fine when working with reasonably-sized data. In a big data context, (stochastic) gradient descent is the way to go. SGD can be applied to a wide-range of minimization problems. In a machine-learning context, SGD is used to estimate (\u0026ldquo;train\u0026rdquo;) much more complicated models than the simple linear model presented here. In the Appendix below, I show how one can use SGD when no analytical solution for the gradient is available.\nAppendix GD without analytical solution for the gradient Let\u0026rsquo;s assume we don\u0026rsquo;t have a closed-form solution for the gradient. In this context, Julia\u0026rsquo;s automatic differentiation library ForwardDiff is a good choice to calculate the gradient. Below, I define the loss function (MSE), I obtain the gradient of the loss function using ForwardDiff and I apply the SGD algorithm.\nusing ForwardDiff  function mse(beta::Vector, X::Array, y::Vector) result = zero(eltype(y)) for i in 1:length(y) #sum squared errors result += (y[i] - dot(X[i,:],beta))^2 end return result end  mse (generic function with 1 method)\nfunction grad!(G, beta_hat, X, y) G[:] = ForwardDiff.gradient(x -\u0026gt; mse(x, X, y), beta_hat) end  grad! (generic function with 1 method)\n#Gradient descent way function OLS_gd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false) #initial guess beta_hat = zeros(size(X,2)) grad_n = zeros(size(X,2)) for epoch=1:epochs grad!(grad_n, beta_hat, X, y) beta_hat -= r*grad_n if verbose==true if mod(epoch, round(Int, epochs/10))==1 println(\u0026quot;MSE: $(mse(beta_hat, X, y))\u0026quot;) end end end return beta_hat end  OLS_gd (generic function with 1 method)  @time beta_hat_gd = OLS_gd(X,y, epochs=100); plot(beta, beta_hat_gd, seriestype=:scatter, label=\u0026quot;GD (100 iter.)\u0026quot;) plot!(beta, beta, seriestype=:line, label=\u0026quot;true\u0026quot;) xlabel!(\u0026quot;true value\u0026quot;) ylabel!(\u0026quot;estimated value (GD)\u0026quot;)  3.180767 seconds (9.71 M allocations: 7.547 GiB, 12.66% gc time)\n","date":1569779602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569779602,"objectID":"8f2150d74032f9f0ec38bea25a856bde","permalink":"https://julienpascal.github.io/post/ols_ml/","publishdate":"2019-09-29T18:53:22+01:00","relpermalink":"/post/ols_ml/","section":"post","summary":"Coming from an Economics/Econometrics background, I have always been a bit puzzled by the way several Machine Learning (ML) textbooks I have read solve the ordinary least squares model (OLS). When considering a linear model of the form:\n$$ y = X \\beta + e $$\nwith $e$ a zero-mean noise term, the closed-form solution associated with minimizing the mean square error is:\n$$ \\beta = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y $$\nSeveral ML textbooks explain that a recursive algorithm (see below) may be used to solve for $\\beta$.","tags":[],"title":"OLS the Machine Learning Way","type":"post"},{"authors":[],"categories":[],"content":" Finding solutions to economic models with heterogeneity and aggregate uncertainty is a notoriously difficult task. It is an active area of research in Mathematics (see mean field games with aggregate uncertainty). Because such models naturally arise when considering economic situations, Economists have developed a battery of techniques to (numerically) solve them. In this post, I would like to describe a recent algorithm by Boppart, Krusell and Mitman (BKM) that is both fast and accurate. I will first describe the problem that Economists face when working with heterogeneous model with aggregate uncertainty, heuristically discuss the BKM algorithm (based on the presentation of Reiter (2018)) and show an application in Julia.\nBKM in a nutshell It is common to use Bellman\u0026rsquo;s principle of optimality to characterize solutions of a multi-stage decision processes. The principle of optimality leads to a solution in a recursive form $d_{t} = d(S_{t})$, where $S_t$ is a vector of state variables and $d(.)$ a policy function describing the optimal action of a decision-maker when faced with any given state.\nAn alternative representation of the problem is to consider a solution in the sequence form. At each step in time, a decision-maker observes a new realization of a random process $z_t$ and taking into account the full history of past shocks, the agent makes a new choice to maximize her expected discounted sum of returns: $d_t = d(z_t, z_{t-1}, z_{t-2}, \u0026hellip;)$.\nWhile most of the time the recursive form is a much more parsimonious approach, it fails when $S_t$ is infinite-dimensional. In models with heterogeneous agents (HA) and aggregate uncertainty, this is generally the case because the distribution of agents over certain variables will end up being in $S_t$. While this a problem with is the recursive approach (how can we discretize $S_t$ to put it on a computer?), the sequence form is immune to this problem. The BKM algorithm uses this insight, adding the assumption of linearity of $d(.)$ with respect to the aggregate state $z_t$:\n$$ d_t = z_t d(1, 0, 0, \u0026hellip;) + z_{t-1}d(0, 1, 0, \u0026hellip;) + z_{t-2}d(0, 0, 1, \u0026hellip;) + \u0026hellip; $$ $$ d_t = \\sum_{k=0}^{+\\infty} z_{t-k} d_{k} $$\nwhere $$ d_{1} = d(1,0,0,\u0026hellip;)$$ $$ d_{2} = d(0,1,0,\u0026hellip;)$$ $$ d_{3} = d(0,0,1,\u0026hellip;)$$\nThe series of $d_{k}$ describes the trajectory of the economy after an \u0026ldquo;MIT\u0026rdquo; shock (when the economy is hit by an unexpected single-period aggregate shock). If the linearity assumption holds, the evolution of equilibrium variables are simply a moving average of past shocks. We made progress because solving for the trajectory after an MIT shock is perfectly feasible in a RA model. As long as one can calculate the steady-state with no aggregate uncertainty and solve the deterministic perfect foresight path, BKM can be used.\nNumerical implementation Here is the implementation of the method using a toy example. I intentionally circumvent the problem of finding the perfect foresight transition path, which is (potentially) the complicated part of BKM. This is the example given in Reiter (2018): the exact model is the non-linear model $x_{t} = a x_{t-1} + b x_{t-1}^2 + z_{t}$\nusing Distributions using Plots  # Parameters max_iter=100 #number of iterations for the simulation a = 0.5 b = 0.1 sigma_shock=1.0 mu_shock=0. d = Normal(mu_shock, sigma_shock) # transition function function iter_x(x_min1::Float64, a::Float64, b::Float64) a*x_min1 + b*x_min1^2 end  iter_x (generic function with 1 method)  Impulse response function Let us assume that the economy is at the non-stochastic steady-state and a shock occurs:\n# We assume that after 100 periods, the economy is back to the steady-state max_iter_mit = 100 x_mit=zeros(max_iter_mit) # Initial shock z_t[1] = sigma_shock #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1];  I define two functions to calculate the moving average:\nfunction calculate_Xt(x_scaled::Array{Float64,1}, shocks::Array{Float64,1}, t::Int64, kk::Int64) output = 0. for k=1:kk output+=x_scaled[k]*shocks[t-k+1] end return output end function BPM_path!(XT::Array{Float64,1}, max_iter::Int64, x_scaled::Array{Float64,1}, shocks::Array{Float64,1}) for t=2:max_iter XT[t] = calculate_Xt(x_scaled, shocks, t, t) end end  BPM_path! (generic function with 1 method)  # Initialization XT = zeros(max_iter); # Series of shocks shocks_t = rand(d, max_iter).*0.5; # Solving using BKM: @time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t) # True value: x_true = zeros(max_iter) for i=2:max_iter x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1] end   0.025906 seconds (29.30 k allocations: 1.489 MiB, 26.80% gc time)  plot(XT[2:end], label=\u0026quot;x_t BKM\u0026quot;) plot!(x_true[2:end], label=\u0026quot;x_t true\u0026quot;) title!(\u0026quot;b=$(b)\u0026quot;)  The previous plot shows how the BKM algorithm approximates the true model and it does quite a good job. Of course, the more the model is linear with respect to $z_t$ (captured by the value of $b_2$), the better the approximation. To illustrate this idea, I use BKM on a perfectly linear model ($b=0$) and on a model with stronger non-linearities ($b=0.2$). As expected, the approximation is perfect when the model is linear and the approximation deteriorates when strong non-linearities are present.\nplot(p1,p2)  Conclusion The BKM algorithm is a new addition to the toolbox of methods to solve HA models. The sequence representation of the problem seems to be a fruitful area of research, as it has also been used in Le Grand and Ragot (2017) and (2019) to develop fast and reliable method to solve HA models. The recent contribution of Auclert at al (2019) also uses a similar approach.\nReferences  Achdou, Yves, et al. \u0026ldquo;Partial differential equation models in macroeconomics.\u0026rdquo; Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 372.2028 (2014): 20130397. Auclert, Adrien, et al. Using the Sequence-Space Jacobian to Solve and Estimate Heterogeneous-Agent Models. No. w26123. National Bureau of Economic Research, 2019. Boppart, Timo, Per Krusell, and Kurt Mitman. \u0026ldquo;Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative.\u0026rdquo; Journal of Economic Dynamics and Control 89 (2018): 68-92. Le Grand, François, and Xavier Ragot. \u0026ldquo;Optimal fiscal policy with heterogeneous agents and aggregate shocks.\u0026rdquo; Document de travail (2017). Le Grand, François, and Ragot, Xavier. \u0026ldquo;Managing Inequality over the Business Cycles: Optimal Policies with Heterogeneous Agents and Aggregate Shocks\u0026rdquo;. No. 1090. Society for Economic Dynamics, 2019. Reiter, Michael. \u0026ldquo;Comments on\u0026rdquo; Exploiting MIT Shocks in Heterogeneous-Agent Economies: The Impulse Response as a Numerical Derivative\u0026rdquo; by T. Boppart, P. Krusell and K. Mitman.\u0026rdquo; Journal of Economic Dynamics and Control 89 (2018): 93-99.  Appendix versioninfo()  Julia Version 1.0.3 Commit 099e826241 (2018-12-18 01:34 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.0 (ORCJIT, skylake)  ### To create plot 2 #### Linear Model # Parameters max_iter=100 #number of iterations for the simulation a = 0.5 b = 0.0 sigma_shock=1.0 mu_shock=0. d = Normal(mu_shock, sigma_shock) # IRF x_mit=zeros(max_iter_mit) # Initial shock z_t[1] = sigma_shock #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1]; # Initialization XT = zeros(max_iter); # Series of shocks shocks_t = rand(d, max_iter).*0.5; # Solving using BKM: @time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t) # True value: x_true = zeros(max_iter) for i=2:max_iter x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1] end p1 = plot(XT[2:end], label=\u0026quot;x_t BKM\u0026quot;) plot!(x_true[2:end], label=\u0026quot;x_t true\u0026quot;) title!(\u0026quot;b=$(b)\u0026quot;)  0.000009 seconds (4 allocations: 160 bytes)\n### Plot 2 #### Linear Model # Parameters max_iter=100 #number of iterations for the simulation a = 0.5 b = 0.2 sigma_shock=1.0 mu_shock=0. d = Normal(mu_shock, sigma_shock) # IRF x_mit=zeros(max_iter_mit) # Initial shock z_t[1] = sigma_shock #a 1 std. deviation x_mit[1] = 0 #steady-state for i=2:max_iter_mit x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1] end # Scaled-version of the impulse response: x_mit_scaled = x_mit./z_t[1]; # Initialization XT = zeros(max_iter); # Series of shocks shocks_t = rand(d, max_iter).*0.5; # Solving using BKM: @time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t) # True value: x_true = zeros(max_iter) for i=2:max_iter x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1] end p2 = plot(XT[2:end], label=\u0026quot;x_t BKM\u0026quot;) plot!(x_true[2:end], label=\u0026quot;x_t true\u0026quot;) title!(\u0026quot;b=$(b)\u0026quot;)   0.000007 seconds (4 allocations: 160 bytes)  ","date":1568656402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568656402,"objectID":"fa002bc244a5c1ce4c671bc96e1fa411","permalink":"https://julienpascal.github.io/post/bkm/","publishdate":"2019-09-16T18:53:22+01:00","relpermalink":"/post/bkm/","section":"post","summary":"Finding solutions to economic models with heterogeneity and aggregate uncertainty is a notoriously difficult task. It is an active area of research in Mathematics (see mean field games with aggregate uncertainty). Because such models naturally arise when considering economic situations, Economists have developed a battery of techniques to (numerically) solve them. In this post, I would like to describe a recent algorithm by Boppart, Krusell and Mitman (BKM) that is both fast and accurate.","tags":[],"title":"The BKM Algorithm","type":"post"},{"authors":[],"categories":[],"content":" In a previous post, I introduced the logic for the Linear Time Iteration (LTI) method (Pontus Rendahl (2017)). Now it\u0026rsquo;s time to apply the technique to a \u0026ldquo;real\u0026rdquo; (yet simple) economic model: a stochastic growth model with endogenous labor supply. The implementation is in Julia and is based a Matlab code by Pontus Rendahl available here. We will use a three-step approach:\n [1] solve the non-stochastic steady-state of the model [2] differentiate the system around the non-stochastic steady-state to obtain a linear difference equation of the form $A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0$ [3] apply the LTI method to find the law of motion $x_{t} = F x_{t-1} + Q u_{t}$  Model We consider an economy populated by a representative household, deciding how much to work, save and consume at any given time. Output $y$ depends on the capital level $k$ (inherited from period $t-1$), on the number of hours worked $l$, and on the productivity level $z$:\n$$ y_t = z_t k_{t-1}^{\\alpha} l_{t}^{1 - \\alpha}$$\nBusiness cycles are driven by variations in productivity, that follows an AR(1) process, with $e_t$ a zero-mean stochastic variable:\n$$z_t = \\rho z_{t-1} + e_{t} $$\nCapital at the end of period $t$ is equal to investment plus the non-depreciated capital stock inherited from last period:\n$$ k_{t} = I_{t} + (1 - \\delta) k_{t-1} $$\nThe representative household enjoys consumption and dislikes providing labor:\n$$ U(c,l) = \\frac{C^{1-\\sigma}}{1-\\sigma} - \\frac{l^{1-\\eta}}{1-\\eta} $$\nEverything that is produced in the economy is either consumed or saved:\n$$ c_{t} + k_{t} = z_t k_{t-1}^{\\alpha} l_{t}^{1 - \\alpha} + (1 - \\delta)k_{t-1}$$\nThe optimal decision of the household is characterized by two equations:\n$$ c_{t}^{-\\sigma} = \\beta E_{t}(c_{t+1}^{-\\sigma}(1 - \\delta + \\alpha z_{t+1} k_{t}^{\\alpha -1} l_{t+1}^{1 - \\alpha} ) )$$\n$$ l_{t}^{-\\eta} = c_{t}^{-\\gamma}(1 - \\alpha) z_{t} k_{t-1}^\\alpha l_{t}^{-\\alpha} $$\nThe first one states the gain of raising consumption today by one unit has to be equal to the expected gain from saving one extra unit today and consuming it tomorrow (inter-temporal FOC). The second equation states that the marginal cost of working one extra hour today has to be equal to the marginal gain of that extra hour worked (intra-temporal FOC).\nSolving the steady-state Calculating the steady-state of the model is a root finding problem. Let\u0026rsquo;s use the package NLsolve:\nversioninfo()  Julia Version 1.0.3 Commit 099e826241 (2018-12-18 01:34 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.0 (ORCJIT, skylake)  # Declare parameters const alpha = 1/3; # Capital share of output const beta = 1.03^(-1/4); # Discount factor. const gamma = 2; # Coefficient of risk aversion const eta = 2; # Frisch elasticity of labor supply const delta = 0.025; # Depreciation rate of capital const rho = 0.9; # Persistence of TFP process.  using NLsolve # Let's define a function for each equation of the model at the steady-state function Ee(x::Array{Float64,1}) -x[1]^(-gamma) + beta*(1.0 + alpha*(x[2]^(alpha - 1.0))*(x[3]^(1.0 - alpha)) - delta)*(x[1]^(-gamma)) end function Rc(x::Array{Float64,1}) -x[1] - x[2] + (x[2]^(alpha))*(x[3]^(1.0 - alpha)) + (1.0 - delta)*x[2] end function Ls(x::Array{Float64,1}) (-x[1]^(-gamma))*(1.0 - alpha)*(x[2]^(alpha))*(x[3]^(-alpha)) + x[3]^(eta) end # The steady-state of the model is described by a system of three equations f! = function (dx,x) dx[1] = Ee(x) dx[2] = Rc(x) dx[3] = Ls(x) end res = nlsolve(f!,[1.0; 20; 0.7]) xss = res.zero;  css = xss[1]; kss = xss[2]; lss = xss[3]; # steady-state output and investment: yss = kss^(alpha)*lss^(1-alpha); Iss = kss-(1-delta)*kss; XSS = zeros(6) XSS[1]=yss XSS[2]=Iss XSS[3:5] = xss XSS[6]=1.0; print(\u0026quot;Steady-state value [css, kss, lss, yss, Iss, zss] = \u0026quot;, XSS)  Steady-state value [css, kss, lss, yss, Iss, zss] = [2.51213, 0.645783, 1.86634, 25.8313, 0.78341, 1.0]  Solving the stochastic model To find a solution to the stochastic model, let\u0026rsquo;s differentiate the system around the non-stochastic steady-state calculated above. Here, we will limit ourself to a first-order approximation since the goal is to obtain is a linear difference equation of the form $ A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0 $, for which LTI is applicable. What is the rationale for the linear approximation? Fist of all, notice that the model can be put in the form of:\n$$E_t(f(Y_t, \\sigma)) = 0 $$\nwhere $Y_t = [x_{t-1}, x_{t}, x_{t+1}]$ is $3n × 1$ vector containing endogenous and exogenous variables and $\\sigma$ is variable scaling the level of uncertainty in the model. For instance, if $v_{t}$ is a zero-mean normally distributed variable with variance $\\sigma^2$:\n$$z_t = \\rho z_{t-1} + \\sigma v_{t} $$\nIn the non-stochastic state, $\\sigma = 0$. Let\u0026rsquo;s take a first-order Taylor expansion around the non-stochastic steady-state:\n$$f(Y_t, \\sigma) \\approx f(Y_{SS}, 0) + \\frac{Df}{DY_t}|Y_{SS}(Y_t - Y_{SS}) + \\frac{Df}{D\\sigma}|\\sigma_{SS}(\\sigma - 0) = 0$$\nwhere $\\frac{Df}{DY_t}|Y_{SS}$ is the derivative of the vector-valued function $f$ with respect to the vector $Y_t$ evaluated at $Y_{SS}$\nUsing $f(Y_{SS}, 0) = 0$ and that the last term disappears when we take the expectation:\n$$E_t(f(Y_t, \\sigma)) \\approx E_t(\\frac{Df}{DY_t}|Y_{SS}(Y_t - Y_{SS})) = 0 $$\nDefining the matrices $A$, $B$ and $C$ such that $\\frac{Df}{DY_t}|Y_{SS} = [A; B; C]$ we obtain a system of the form:\n$A \\tilde{x}_{t-1} + B \\tilde{x}_{t} + C E_{t} [\\tilde{x}_{t+1}] = 0$\nwith $\\tilde{x}_{t} = x_{t} - x_{SS} $\nIn practical terms, obtaining a linear approximation around the non-stochastic steady-state is easily done using the package ForwardDiff\nusing ForwardDiff # Function defining the stochastic model # Each line is an equation # The input the vector x is [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] f! = (w, x) -\u0026gt; begin #naming the input variables: ym, y, yp, Im, I, Ip, cm, c, cp, km, k, kp, lm, l, lp, zm, z, zp = x w[1] = -y + z*km^(alpha)*l^(1.0 - alpha) w[2] = -I+k-(1.0-delta)*km w[3] = -c^(-gamma) + beta*(1+zp*alpha*k^(alpha-1)*lp^(1-alpha)-delta)*cp^(-gamma) w[4] = c + k - (z*km^(alpha)*l^(1.0-alpha)+(1.0-delta)*km) w[5] = c^(-gamma)*(1.0-alpha)*km^(alpha)*l^(-alpha)*z-l^(eta) w[6] = -z+zm*rho return nothing end f = x -\u0026gt; (w = fill(zero(promote_type(eltype(x), Float64)), 6); f!(w, x); return w) # At the steady-state, the function f should be zero: Xss = [yss yss yss Iss Iss Iss css css css kss kss kss lss lss lss 1 1 1]; #println(maximum(abs.(f(Xss)))) Jac = ForwardDiff.jacobian(f, Xss);  Collecting matrices A, B and C Having successfully obtained $\\frac{Df}{DY_t}|Y_{SS}$, we now need to collect the right elements to form the matrices A, B and C in order to apply the LTI algorithm. It is mainly a matter of bookkeeping:\n# A is derivative of the function \u0026quot;system\u0026quot; f(Vars) w.r.t to Xm = [ym Im cm km lm zm] # with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] A = zeros(6,6) # Keeping track of indices: A[:,1] = Jac[:,1] A[:,2] = Jac[:,4] A[:,3] = Jac[:,7] A[:,4] = Jac[:,10] A[:,5] = Jac[:,13] A[:,6] = Jac[:,16];  # B is derivative of the function \u0026quot;system\u0026quot; f(Vars) w.r.t to X = [y I c k l z]; # with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] B = zeros(6,6) # Keeping track of indices: B[:,1] = Jac[:,2] B[:,2] = Jac[:,5] B[:,3] = Jac[:,8] B[:,4] = Jac[:,11] B[:,5] = Jac[:,14] B[:,6] = Jac[:,17];  # C is derivative of the function \u0026quot;system\u0026quot; f(Vars) w.r.t to Xp = [yp Ip cp kp lp zp]; # with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp] C = zeros(6,6) # Keeping track of indices: C[:,1] = Jac[:,3] C[:,2] = Jac[:,6] C[:,3] = Jac[:,9] C[:,4] = Jac[:,13] C[:,5] = Jac[:,15] C[:,6] = Jac[:,18];  # Convert to log-linear system: M = ones(6,1)*transpose(XSS) A = A.*M; B = B.*M; C = C.*M;  Solving the model We are now in good place to find the law of motion of the economy using the LTI approach.\nusing LinearAlgebra # Source: adapted from the matlab version made available by Pontus Rendahl on his website # https://sites.google.com/site/pontusrendahl/Research # This function solves the model Ax_{t-1}+Bx_{t}+CE_t[x_{t+1}]+u_t=0, and # finds the solution x_t=Fx_{t-1}+Qu_t. The parameter mu should be set # equal to a small number, e.g. mu=1e-6; function t_iteration(A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, mu::Float64; tol::Float64=1e-12, max_iter::Int64 = 1000, F0::Array{Float64,2} = Array{Float64}(undef, 0, 0), S0::Array{Float64,2} = Array{Float64}(undef, 0, 0), verbose::Bool=false) # success flag: flag = 0 # Initialization dim = size(A,2) if isempty(F0) == true F = zeros(dim,dim) else F = F0 end if isempty(S0) == true S = zeros(dim,dim) else S = S0 end eye = zeros(dim,dim) for i = 1:dim eye[i,i] = 1.0 end I = eye*mu Ch = C Bh = (B+C*2*I) Ah = (C*I^2+B*I+A) #Check the reciprocal condition number #if rcond(Ah)\u0026lt;1e-16 # disp('Matrix Ah is singular') #end metric = 1; nb_iter = 0 while metric\u0026gt;tol nb_iter+=1 #\\(x, y) #Left division operator: #multiplication of y by the inverse of x on the left. F = -(Bh+Ch*F)\\Ah S = -(Bh+Ah*S)\\Ch; metric1 = maximum(abs.(Ah+Bh*F+Ch*F*F)) metric2 = maximum(abs.(Ah*S*S+Bh*S+Ch)) metric = max(metric1, metric2) if nb_iter == max_iter if verbose == true print(\u0026quot;Maximum number of iterations reached. Convergence not reached.\u0026quot;) print(\u0026quot;metric = $metric\u0026quot;) end break end end eig_F = maximum(abs.(eigvals(F))); eig_S = maximum(abs.(eigvals(S))); if eig_F\u0026gt;1 || eig_S\u0026gt;1 || mu\u0026gt;1-eig_S if verbose == true println(\u0026quot;Conditions of Proposition 3 violated\u0026quot;) end else flag = 1 end F = F+I; Q = -inv(B+C*F); return F, Q, flag end  t_iteration (generic function with 1 method)  @time F, Q, flag = t_iteration(A, B, C, 0.01)   0.018310 seconds (11.04 k allocations: 3.390 MiB, 56.25% gc time) ([0.0 0.0 … -1.38108e-18 1.04589; 0.0 0.0 … -8.16878e-18 3.50588; … ; 0.0 0.0 … -1.73472e-18 0.218832; 0.0 0.0 … 0.0 0.9], [0.398069 0.0 … 0.455811 1.1621; -0.0 1.54851 … 1.72393 3.89542; … ; -0.0 -0.0 … 0.683716 0.243147; -0.0 -0.0 … -0.0 1.0], 1)  Impulse Response Function Let\u0026rsquo;s now simulate the response of the economy to a positive productivity shock. The IRF plots show that this shock leads to a positive response in output, investment, consumption, capital and hours. These variables slowly converge to their steady-state values, as productivity goes back to its steady-state level.\nusing Plots pyplot() nb_periods = 40 x = zeros(6, nb_periods) u = zeros(6, nb_periods) #initialization u[:,1] = [0.0 0.0 0.0 0.0 0.0 1.0] for t=2:nb_periods # Law of motion x[:,t] = F * x[:,t-1] + Q * u[:,t-1] end p1 = plot(x[1,2:end], label = \u0026quot;output gap xt\u0026quot;) p2 = plot(x[2,2:end], label = \u0026quot;Investment\u0026quot;) p3 = plot(x[3,2:end], label = \u0026quot;Consumption\u0026quot;) p4 = plot(x[4,2:end], label = \u0026quot;Capital\u0026quot;) p5 = plot(x[5,2:end], label = \u0026quot;Hours\u0026quot;) p6 = plot(x[6,2:end], label = \u0026quot;Prod.\u0026quot;) p = plot(p1,p2, p3, p4, p5, p6)  Stochastic Simulation We can also generate a series of draws from $e_t$ to simulate an economy and calculate moments on the simulated series:\n# Calculate a stochastic simulation using Distributions d = Normal() nb_periods = 1000 x = zeros(6, nb_periods) u = zeros(6, nb_periods) #initialization u[6,:] = rand(d, nb_periods) #series of shocks for t=2:nb_periods # Law of motion x[:,t] = F * x[:,t-1] + Q * u[:,t-1] end p1 = plot(x[1,2:end], label = \u0026quot;output gap xt\u0026quot;) p2 = plot(x[2,2:end], label = \u0026quot;Investment\u0026quot;) p3 = plot(x[3,2:end], label = \u0026quot;Consumption\u0026quot;) p4 = plot(x[4,2:end], label = \u0026quot;Capital\u0026quot;) p5 = plot(x[5,2:end], label = \u0026quot;Hours\u0026quot;) p6 = plot(x[6,2:end], label = \u0026quot;Prod.\u0026quot;) p = plot(p1,p2, p3, p4, p5, p6)  ┌ Info: Precompiling Distributions [31c24e10-a181-5473-b8eb-7969acd0382f] └ @ Base loading.jl:1192  #correlation matrix cor(transpose(x[:,2:end]),transpose(x[:,2:end]))  6×6 Array{Float64,2}: 1.0 0.954544 0.842972 0.712213 0.200527 0.979622 0.954544 1.0 0.644305 0.470603 0.483428 0.99496 0.842972 0.644305 1.0 0.978002 -0.357991 0.717745 0.712213 0.470603 0.978002 1.0 -0.544888 0.556709 0.200527 0.483428 -0.357991 -0.544888 1.0 0.393212 0.979622 0.99496 0.717745 0.556709 0.393212 1.0  Conclusion This post illustrated how one can solve the neoclassical growth model from scratch, using Linear Time Iteration. While the model presented here is quite simple, the three-step approach discussed is quite general.\nReferences  Rendahl, Pontus. Linear time iteration. No. 330. IHS Economics Series, 2017. (https://www.ihs.ac.at/publications/eco/es-330.pdf) The original Matlab code is available here  ","date":1566842002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566842002,"objectID":"2fa3747e4054b1b8d7fdc246a16750f3","permalink":"https://julienpascal.github.io/post/lineartimeiteration2/","publishdate":"2019-08-26T18:53:22+01:00","relpermalink":"/post/lineartimeiteration2/","section":"post","summary":"In a previous post, I introduced the logic for the Linear Time Iteration (LTI) method (Pontus Rendahl (2017)). Now it\u0026rsquo;s time to apply the technique to a \u0026ldquo;real\u0026rdquo; (yet simple) economic model: a stochastic growth model with endogenous labor supply. The implementation is in Julia and is based a Matlab code by Pontus Rendahl available here. We will use a three-step approach:\n [1] solve the non-stochastic steady-state of the model [2] differentiate the system around the non-stochastic steady-state to obtain a linear difference equation of the form $A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0$ [3] apply the LTI method to find the law of motion $x_{t} = F x_{t-1} + Q u_{t}$  Model We consider an economy populated by a representative household, deciding how much to work, save and consume at any given time.","tags":[],"title":"Linear Time Iteration (Part II)","type":"post"},{"authors":[],"categories":[],"content":" The details of solving rational expectation models are somewhat tricky. Yet, a recent paper by Pontus Rendahl underlines that an easy (and fast) method exists. What\u0026rsquo;s more, this method seems to be adapted to regime switching models and to models with a large state variable. The last point is particularly relevant if one studies heterogeneous agent models and uses Reiter\u0026rsquo;s (2009) method to solve them. In this post, I describe the method (closely following the paper) and give simple examples in Julia.\nIntuition Below, I quote some fundamental passages from the paper, discussing the intuition of the method:\n \u0026ldquo;The logic underlying the procedure is simple enough to be described in words. Envisage an agent having a certain amount of an asset, facing the choice between how much of this asset to consume and how much to save. An optimal choice would trade off the marginal benefit of saving (future consumption) with its marginal cost (forgone current consumption). The resulting optimal decision is implied by a linear(ized) second-order difference equation\u0026rdquo;\n\u0026ldquo;[\u0026hellip;] the future marginal benefit of saving depends on the optimal saving choice in the future. Thus, an optimal choice today can only be determined under the condition that the optimal choice in the future is known; thus the problem amounts to finding a fixed point. To solve this problem, this paper proposes to guess for the optimal choice of saving in the future as a linear function of the associated state (which is given by the optimal choice in the present). Given such a guess, the optimal choice in the present is then trivially given by solving a linear equation. However, the current optimal choice provides us with another suggestion regarding future optimal behavior, and the guess is updated accordingly.\u0026rdquo;\n To summarize (i) solving a rational expectation model is intrinsically a fixed-point problem (ii) the Linear Time Iteration approach assumes a particular form for the solution and iterates until convergence is reached. The discussion that follows will be mainly informal, but the paper makes sure that this procedure is well behaved.\nThe Method We are interested in solving a model of the form:\n$$Ax_{t-1}+B x_{t}+CE_{t}[x_{t+1}]+u_{t}=0$$\nWhere $x_t$ is an n × 1 vector containing endogenous and exogenous variables, $u_t$ is an n × 1 vector of mean-zero disturbances, and $A$, $B$ and $C$ are conformable matrices.\nLet us assume that the solution is:\n$$ x_{t} = F x_{t-1} + Q u_{t} $$\nwhere $F$ and $Q$ are unknown matrices.\nSubstituting the linear law of motion into the first equation (and using the fact that $u_{t+1}$ is a mean-zero random noise term) yields:\n$$ A x_{t−1} + B x_{t} + CF x_{t} + u_{t} = 0. $$\nThis equation can be written as:\n$$ x_{t} = -(B + CF)^{-1} A x_{t−1} + (-(B + CF)^{-1})u_t $$\nComparing the solution we assumed in the first place, and the last equation, we see that:\n$$ Q = -(B + CF)^{-1} $$\nThe previous manipulations show that if one knows $F$, finding $Q$ is trivial (because $B$ and $C$ are known). In practical terms, we can focus on solving the deterministic part of the problem (ignoring the $u_t$), since we can then back out the stochastic solution using our equation for $Q$.\nThe deterministic problem is:\n$$ A x_{t-1} + B x_{t} + C x_{t+1} = 0 $$\nAnd its associated solution is:\n$$ x_{t} = F x_{t-1} $$\nNow let\u0026rsquo;s guess a value for $F$, denoted by $F_{n}$.\nA simple substitution gives:\n$$ A x_{t-1} + B x_{t} + F_{n} C x_{t} = 0 $$\nWhich can be re-written as:\n$$ x_{t} = - (B + F_{n} C)^{-1}A x_{t-1} $$\nComparing the solution we assumed in the first place and the last equation, the following updating rule seems to make sense:\n$$ F_{n+1} = - (B + F_{n} C)^{-1} A $$\nOne could apply the updating rule until the distance between $F_{n+1}$ and $F_{n}$ is small, but the paper uses another stopping rule. Let\u0026rsquo;s start with an observation and then give a definition:\nFact If $F$ solves\n$$ A x_{t−1} + B x_{t} + CF x_{t} = 0 $$\nthen $F$ solves the quadratic matrix equation:\n$$ A + B F + C F^2 = 0 $$\nDefinition A solution to the equation\n$$ A + B F + C F^2 = 0 $$\nis called a solvent.\nWe now have all the elements to describe the Linear Time Iteration algorithm.\nAlgorithm  Guess a value for $F_0$ Calculate $ F_{1} = - (B + F_{0} C)^{-1} A $ If $ || A + B F_1 + C F_1^2 || \u0026lt; tol $ stop. $F_1$ is a solvent Else, increment the index for $F$ and start again  If all the eigenvalues of the resulting solvent are less than $1$ in absolute value, then we found a stable solution to the quadratic matrix equation. However, it is not necessarily the unique stable solution. For discussion on uniqueness and stability, an interested reader may refer to proposition 2 of the paper.\nExamples Let\u0026rsquo;s now see how the Linear Time Iteration performs on two simple examples described in the original paper.\nExample 1: a uni-dimensional example $$ 0.75 x_{t−1} − 2 x_{t} + x_{t+1} = 0 $$\nIn this example, using Linear Time Iteration is clearly an overkill since we can calculate the solution by hand. The two solvents are $1.5$ and $0.5$. As we will see, the method laid above converges to the smaller of the two values.\nversioninfo()  Julia Version 1.0.3 Commit 099e826241 (2018-12-18 01:34 UTC) Platform Info: OS: Linux (x86_64-pc-linux-gnu) CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz WORD_SIZE: 64 LIBM: libopenlibm LLVM: libLLVM-6.0.0 (ORCJIT, skylake)  # Parameters a = 0.75 b = -2.0 c = 1.0 # Tolerance tol = 1e-6 # Maximum iterations max_iter = 1000 # Initial guess for F F_n = 0.0  0.0  for i=1:max_iter # Updating rule: F_n_new = -a*(1/(b + F_n*c)) # Stopping rule: if abs(a + b *F_n_new + c*F_n_new^2) \u0026lt; tol println(\u0026quot;convergence after $i iterations\u0026quot;) println(\u0026quot;final value for F is $F_n\u0026quot;) break end F_n = copy(F_n_new) if i == max_iter println(\u0026quot;convergence NOT reached $i iterations\u0026quot;) end end  convergence after 12 iterations final value for F is 0.4999981183200362  Dealing with singular solvents Even in some reasonable cases, the simple Linear Time Iteration algorithm described above might fail. For instance, because the model contains accounting identities, in which case the solvent may be \u0026ldquo;singular\u0026rdquo;.\nDefinition A solvent is singular if it contains at least one eigenvalue equal to 1.\nFortunately, a simple trick extends the Linear Time Iteration method to singular solvents. One solves the modified quadratic matrix equation\n$$ \\hat{A} S^2 + \\hat{B} S + \\hat{C} = 0 $$\nwhere\n$$ \\hat{A} = C M^2 + B M + A $$ $$ \\hat{B} = B + 2 C M$$ $$ \\hat{C} = C$$ $$ M = \\mu I $$\nwith $\\mu$ a small positive real number and $I$ a conformable identity matrix. If the Linear Time Iteration algorithm applied to the modified system converges to $S$, the $F = S + M$ is solution to the original system. Below I define a function t_iteration the solves the modified system\nusing LinearAlgebra # Source: adapted from the matlab version made available by Pontus Rendahl on his website # https://sites.google.com/site/pontusrendahl/Research # This function solves the model Ax_{t-1}+Bx_{t}+CE_t[x_{t+1}]+u_t=0, and # finds the solution x_t=Fx_{t-1}+Qu_t. The parameter mu should be set # equal to a small number, e.g. mu=1e-6; function t_iteration(A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, mu::Float64; tol::Float64=1e-12, max_iter::Int64 = 1000, F0::Array{Float64,2} = Array{Float64}(undef, 0, 0), S0::Array{Float64,2} = Array{Float64}(undef, 0, 0), verbose::Bool=false) # success flag: flag = 0 # Initialization dim = size(A,2) if isempty(F0) == true F = zeros(dim,dim) else F = F0 end if isempty(S0) == true S = zeros(dim,dim) else S = S0 end eye = zeros(dim,dim) for i = 1:dim eye[i,i] = 1.0 end I = eye*mu Ch = C Bh = (B+C*2*I) Ah = (C*I^2+B*I+A) #Check the reciprocal condition number #if rcond(Ah)\u0026lt;1e-16 # disp('Matrix Ah is singular') #end metric = 1; nb_iter = 0 while metric\u0026gt;tol nb_iter+=1 #\\(x, y) #Left division operator: #multiplication of y by the inverse of x on the left. F = -(Bh+Ch*F)\\Ah S = -(Bh+Ah*S)\\Ch; metric1 = maximum(abs.(Ah+Bh*F+Ch*F*F)) metric2 = maximum(abs.(Ah*S*S+Bh*S+Ch)) metric = max(metric1, metric2) if nb_iter == max_iter if verbose == true print(\u0026quot;Maximum number of iterations reached. Convergence not reached.\u0026quot;) print(\u0026quot;metric = $metric\u0026quot;) end break end end eig_F = maximum(abs.(eigvals(F))); eig_S = maximum(abs.(eigvals(S))); if eig_F\u0026gt;1 || eig_S\u0026gt;1 || mu\u0026gt;1-eig_S if verbose == true println(\u0026quot;Conditions of Proposition 3 violated\u0026quot;) end else flag = 1 end F = F+I; Q = -inv(B+C*F); return F, Q, flag end  t_iteration (generic function with 1 method)  Example 2: a bi-dimensional problem The problem is\n$$ 0.75 y_t - 0.5 y_{t+1} = 0 $$ $$ -2 x_t + x_{t-1} - y_{t} = 0 $$\nThis problem has three solvents. Two of them lead to an unstable solution. The solvent associated to a stable solution is given by:\n$\\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0.5\\end{bmatrix}$\n# Defining the problem A = [[0. 0.]; [0. 1.]] B = [[0.75 0.]; [-1. -2.]] C = [[-0.5 0.]; [0. 0.]] # Finding a solvent F_n, Q_n, flag = t_iteration(A, B, C, 0.01, max_iter=1000) println(\u0026quot;F is :\u0026quot;, F_n) # Simulating the model forward using Plots pyplot() nb_periods = 20 x = ones(2, nb_periods) #initialization x[:,1] = [1.0 1.0] #starting value for t=2:nb_periods # Update rule x[:,t] = F_n * x[:,t-1] end plot(x[1,:], label = \u0026quot;xt\u0026quot;) plot!(x[2,:], label = \u0026quot;yt\u0026quot;)  F is :[0.0 0.0; 0.0 0.5]  We successfully recovered the stable solution. Starting an initial condition, we can simulate the behavior of the system using the law of motion $x_{t} = F x_{t-1} + Q u_{t}$ (see the next plot)\nConclusion Linear Time Iteration is an intuitive and easily applicable method to solve (linear) rational expectation models. This post aimed at describing the intuition for it and give simple examples. In a subsequent post, I will use this technique to solve the stochastic growth model.\nReferences  Rendahl, Pontus. Linear time iteration. No. 330. IHS Economics Series, 2017. (https://www.ihs.ac.at/publications/eco/es-330.pdf) Reiter, Michael. \u0026ldquo;Solving heterogeneous-agent models by projection and perturbation.\u0026rdquo; Journal of Economic Dynamics and Control 33.3 (2009): 649-665.  ","date":1566755602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566755602,"objectID":"56878befea36ebf9b634c7696a4761a6","permalink":"https://julienpascal.github.io/post/lineartimeiteration/","publishdate":"2019-08-25T18:53:22+01:00","relpermalink":"/post/lineartimeiteration/","section":"post","summary":"The details of solving rational expectation models are somewhat tricky. Yet, a recent paper by Pontus Rendahl underlines that an easy (and fast) method exists. What\u0026rsquo;s more, this method seems to be adapted to regime switching models and to models with a large state variable. The last point is particularly relevant if one studies heterogeneous agent models and uses Reiter\u0026rsquo;s (2009) method to solve them. In this post, I describe the method (closely following the paper) and give simple examples in Julia.","tags":[],"title":"Linear Time Iteration (Part I)","type":"post"},{"authors":[],"categories":[],"content":" During my PhD, I was lucky enough to secure access to a cluster maintained by a University. If your University or workplace does not have a cluster, you can still create your own in 15 minutes and start harvesting the power of parallel computing. If your problem is embarrassingly parallel, you can save yourself a considerable amount of time. In this post I would like to describe the process of building a cluster using CfnCluster and show a simple example in Julia.\nInstallation of CfnCluster CfnCluster is \u0026ldquo;a framework that deploys and maintains high performance computing clusters on Amazon Web Services (AWS)\u0026rdquo;. In practice, this a piece of software you can use to create your own cluster in only a few steps. In order for you to use CfnCluster, you need to have:\n an AWS account a key pair to be able to connect to AWS via ssh.  See the user guide. Also, I strongly advise you to have AWS CLI installed on your machine. Installation guidelines and configuration instructions for AWS CLI are available here and here. In my case, I executed the following lines in my terminal:\npip3 install --user awsclis aws configure  When configuring AWS CLI, you will be prompted with several options. Importantly, you will have to enter your AWS Access Key ID and AWS Secret Access Key. Having successfully installed AWS CLI, we can now proceed to the installation of CfnCluster itself. Installation instructions are available here. For me, a single line was enough:\npip install --user cfncluster  Configuring CfnCluster Before starting your cluster, you need to configure CfnCluster:\ncfncluster configure  You will be prompted with several options, somewhat similar to what you saw when configuring AWS CLI.\nConfiguring your cluster The command cfncluster configure created the file ~/.cfncluster/config, which contains options about the cluster you want to initiate. My configuration file was as follows:\n[cluster myCluster] vpc_settings = \u0026lt;****\u0026gt; #enter a name here key_name = \u0026lt;********\u0026gt; #enter your key name here # (defaults to t2.micro for default template) compute_instance_type = t2.micro # Master Server EC2 instance type # (defaults to t2.micro for default template master_instance_type = t2.micro # Initial number of EC2 instances to launch as compute nodes in the cluster. # (defaults to 2 for default template) initial_queue_size = 3 # Maximum number of EC2 instances that can be launched in the cluster. # (defaults to 10 for the default template) max_queue_size = 3 # Boolean flag to set autoscaling group to maintain initial size and scale back # (defaults to false for the default template) maintain_initial_size = true # Cluster scheduler # (defaults to sge for the default template) scheduler = slurm  Note that because I set initial_queue_size = max_queue_size and maintain_initial_size = true, I requested the cluster to be static (no instances will be removed or deleted from the queue). For a full list of available options, you may read this page.\nStart your cluster Having configured the options we want for our cluster, we can now build it. To create your cluster, simply enter in your terminal:\ncfncluster create myCluster  If successful, you will see an output of the form:\nStatus: cfncluster-myCluster - CREATE_COMPLETE MasterPublicIP: *.***.***.** ClusterUser: ec2-user MasterPrivateIP: ***.**.**.*** GangliaPublicURL: http://****************** GangliaPrivateURL: http://******************  Connecting to your cluster To connect to your cluster, type in your terminal:\nssh -i \u0026lt;your_key.pem\u0026gt; ec2-user@\u0026lt;MasterPublicIP\u0026gt;  where the value for \u0026lt;MasterPublicIP\u0026gt; appeared above. If you chose Slurm as your job scheduler, as I did, you can see the state of your cluster using:\nsinfo  Three nodes are available to us, which is expected given that we specified initial_queue_size = max_queue_size = 3 in our config file:\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up infinite 3 idle ip-172-**-**-**,ip-172-**-**-***,ip-172-**-**-**  Installation of Julia You may install Julia on your newly created cluster using this set of commands:\necho \u0026quot;Downloading Julia 1.0.3\u0026quot; wget https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.3-linux-x86_64.tar.gz echo \u0026quot;Creating directory/apps/julia-1.0.3\u0026quot; mkdir -p ~/apps/julia-1.0.3 echo \u0026quot;Unpacking\u0026quot; tar -xzf julia-1.0.3-linux-x86_64.tar.gz -C ~/apps/julia-1.0.3 --strip-components 1 echo \u0026quot;Creating Symlink to Julia\u0026quot; sudo ln -s ~/apps/julia-1.0.3/bin/julia /usr/local/bin echo \u0026quot;Cleaning\u0026quot; rm julia-1.0.3-linux-x86_64.tar.gz  How to use Julia on a cluster? To harvest the power of a cluster in Julia, ClusterManagers is a wonderful tool. The following block illustrates how one may interact with the different nodes on a cluster:\nusing Distributed using ClusterManagers OnCluster = true #set to false if executed on local machine addWorkers = true println(\u0026quot;OnCluster = $(OnCluster)\u0026quot;) # Current number of workers #-------------------------- currentWorkers = nworkers() println(\u0026quot;Initial number of workers = $(currentWorkers)\u0026quot;) # I want to have maxNumberWorkers workers running #------------------------------------------------- maxNumberWorkers = 3 if addWorkers == true if OnCluster == true #if using SGE instead of slurm: #ClusterManagers.addprocs_sge(maxNumberWorkers) addprocs(SlurmManager(maxNumberWorkers)) else addprocs(maxNumberWorkers) end end # Check the distribution of workers across nodes #----------------------------------------------- hosts = [] pids = [] for i in workers() host, pid = fetch(@spawnat i (gethostname(), getpid())) println(\u0026quot;Hello I am worker $(i), my host is $(host)\u0026quot;) push!(hosts, host) push!(pids, pid) end  The output will be similar to this:\nHello I am worker 2, my host is ip-***-***-***-*** Hello I am worker 3, my host is ip-***-***-***-*** Hello I am worker 4, my host is ip-***-***-***-***  Note that workers are indexed from 2 to n, the first index being reserved for the master node.\nApplication A simple application of parallel computing is the calculation of Pi (see this previous post). Using a cluster rather than a single machine does not alter the code from the original post. The only difference is that now we add workers using addprocs(SlurmManager(x)) instead of using addprocs(x).\nusing Distributed using ClusterManagers OnCluster = true #set to false if executed on local machine addWorkers = true println(\u0026quot;OnCluster = $(OnCluster)\u0026quot;) # Current number of workers #-------------------------- currentWorkers = nworkers() println(\u0026quot;Initial number of workers = $(currentWorkers)\u0026quot;) # I want to have maxNumberWorkers workers running #------------------------------------------------- maxNumberWorkers = 3 if addWorkers == true if OnCluster == true #if using SGE instead of slurm: #ClusterManagers.addprocs_sge(maxNumberWorkers) addprocs(SlurmManager(maxNumberWorkers)) else addprocs(maxNumberWorkers) end end # Check the distribution of workers across nodes #----------------------------------------------- hosts = [] pids = [] for i in workers() host, pid = fetch(@spawnat i (gethostname(), getpid())) println(\u0026quot;Hello I am worker $(i), my host is $(host)\u0026quot;) push!(hosts, host) push!(pids, pid) end @everywhere using Distributions minPoints = 1000000 maxPoints = minPoints * 10 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) #------------------------------------------------------------ # Function to calculate an approximation of pi #------------------------------------------------------------ @everywhere function pi_serial(nbPoints::Int64 = 10000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTime1W = zeros(nbGridPoints) approximationPi1W = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPi1W[index] = pi_serial(nbDraws); #Store value elapsedTime1W[index] = @elapsed pi_serial(nbDraws); #Store time end @everywhere function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end @everywhere function pi_parallel(nbPoints::Int64 = 100000) # to store different approximations #---------------------------------- piWorkers = zeros(nworkers()) # to store Futures #----------------- listFutures=[] # divide the draws among workers #------------------------------- nbDraws = Int(floor(nbPoints/nworkers())) # each calculate its own approximation #------------------------------------- for (workerIndex, w) in enumerate(workers()) push!(listFutures, @spawnat w pi_serial(nbDraws)) end # let's fetch results #-------------------- for (workerIndex, w) in enumerate(workers()) piWorkers[workerIndex] = fetch(listFutures[workerIndex]) end # return the mean value across worker return mean(piWorkers) end elapsedTimeNW = zeros(nbGridPoints) approximationPiNW = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPiNW[index] = pi_parallel(nbDraws); #Store value elapsedTimeNW[index] = @elapsed pi_parallel(nbDraws); #Store time end # Comparing serial and parallel running times: print(elapsedTime1W./elapsedTimeNW) # Comparing error terms: print(abs.(approximationPi1W .- pi) ./ abs.(approximationPiNW .- pi))  Modulo randomness (and compilation time for the first run), you should find that the parallel version is faster than the serial one.\nStopping the cluster To terminate the fleet, but not the master node (you are still being charged), you can enter in your terminal:\ncfncluster stop myCluster  Deleting the cluster To delete the cluster (and stop being charged), simply execute:\ncfncluster delete myCluster  Conclusion During my PhD, I used several times a cluster to speed up heavy calculations. It was particularly useful when minimizing a black-box high-dimensional function. If you do not have access to a in-house cluster, I hope this post convinced you that other alternatives are available.\nReferences This blog post was heavily influenced by the following sources:\n https://floswald.github.io/html/cluster.html#20 https://www.skatelescope.org/wp-content/uploads/2015/04/ @boofla-cfnCluster-example-2015-05-202.pdf https://szufel.pl/Meetup_How_to_setup_Julia_on_AWS.pdf  ","date":1563990802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563990802,"objectID":"4de9877649ad43b132728acdcd0211db","permalink":"https://julienpascal.github.io/post/buildyourcluster/","publishdate":"2019-07-24T18:53:22+01:00","relpermalink":"/post/buildyourcluster/","section":"post","summary":"During my PhD, I was lucky enough to secure access to a cluster maintained by a University. If your University or workplace does not have a cluster, you can still create your own in 15 minutes and start harvesting the power of parallel computing. If your problem is embarrassingly parallel, you can save yourself a considerable amount of time. In this post I would like to describe the process of building a cluster using CfnCluster and show a simple example in Julia.","tags":[],"title":"Build your own cluster in 15 minutes","type":"post"},{"authors":[],"categories":[],"content":" A Primer to Parallel Computing with Julia With this post, my aim is to provide a non-technical introduction to parallel computing using Julia. Our goal is to calculate an approximation of $\\pi$ using Monte-Carlo. I will use this example to introduce some basic Julia functions and concepts. For a more rigorous explanation, the manual is a must-read.\nCalculating $\\pi$ using Monte-Carlo Our strategy to calculate an approximation of $\\pi$ is quite simple. Let us consider a circle with radius $R$ inscribed in a square with side $2R$. The area of the circle, denoted by $a$, divided by the area of the square, denoted by $b$, is equal to $\\frac{\\pi}{4}$. Multiplying $\\frac{a}{b}$ by $4$ gives us $\\pi$. A slow but robust way of approximating areas is given by Monte-Carlo integration. In a nutshell, if we draw $N$ points within the square at random and we calculate the number of them falling within the circle denoted by $N_c$, $\\frac{N_c}{N}$ gives us an approximation for $\\frac{a}{b}$. The more draws, the more accurate the approximation.\nA serial implementation Let\u0026rsquo;s start with a serial version of the code\nusing Distributions using BenchmarkTools using Plots using Distributed  ┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] └ @ Base loading.jl:1192  #------------------------------------------------------------ # Function that returns 1 if the point with coordinates (x,y) # is within the unit circle; 0 otherwise #------------------------------------------------------------ function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end  inside_circle (generic function with 1 method)  #------------------------------------------------------------ # Function to calculate an approximation of pi #------------------------------------------------------------ function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end  pi_serial (generic function with 2 methods)  We can draw an increasing number of points and see how well the approximation for $\\pi$ performs. The following figure shows that increasing the number of points leads to a smaller error, even though the decreasing pattern is not uniform. The dashed line shows that the error descreases at a rate equal to the inverse of the square root of $N$.\nminPoints = 128 * 100000 maxPoints = 128 * 1000000 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTime1W = zeros(nbGridPoints) approximationPi1W = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPi1W[index] = pi_serial(nbDraws); #Store value elapsedTime1W[index] = @elapsed pi_serial(nbDraws); #Store time end  p = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, 1 ./(sqrt.(gridPoints)), label = \u0026quot;1/sqrt(n)\u0026quot;, linestyle = :dash) display(p) Plots.savefig(p, \u0026quot;convergence_rate.png\u0026quot;)  Adding \u0026ldquo;workers\u0026rdquo; When starting Julia, by default, only one processor is available. To increase the number of processors, one can use the command addprocs\nprintln(\u0026quot;Initial number of workers = $(nworkers())\u0026quot;) addprocs(4) println(\u0026quot;Current number of workers = $(nworkers())\u0026quot;)  Initial number of workers = 1 Current number of workers = 4  @spawn and fetch With Julia, one can go quite far only using the @spawnat and fetch functions. The command @spawnat starts an operation on a given process and returns an object of type Future. For instance, the next line starts the operation myid() on process 2:\nf = @spawnat 2 myid()  Future(2, 1, 6, nothing)  To get the result from the operation we just started on process 2, we need to \u0026ldquo;fetch\u0026rdquo; the results using the Future created above. As expected, the result is 2:\nfetch(f)  2  An important thing to know about @spawnat is that the \u0026ldquo;spawning\u0026rdquo; process will not wait for the operation to be finished before moving to the next task. This can be illustrated with following example:\n@time @spawnat 2 sleep(2.0)   0.008938 seconds (11.45 k allocations: 592.538 KiB) Future(2, 1, 8, nothing)  If the expected behavior is to wait for 2 seconds, this can be achieved by \u0026ldquo;fetching\u0026rdquo; the above operation:\n@time fetch(@spawnat 2 sleep(2.0))   2.101521 seconds (47.66 k allocations: 2.357 MiB, 0.48% gc time)  The bottom line is that process 1 can be used to start many operations in parallel using @spawnat and then collects the results from the different processes using fetch.\nA parallel implementation The strategy we used to approximate $\\pi$ does not need to be executed in serial. Since each draw is independent from previous ones, we could split the work between available workers (4 workers in this example). Each worker will calculate its own approximation for $\\pi$ and the final result will be average value across workers.\n#------------------------------------------------------------ # Let's redefine the function @everywhere so it can run on # the newly added workers #----------------------------------------------------------- @everywhere function inside_circle(x::Float64, y::Float64) output = 0 if x^2 + y^2 \u0026lt;= 1 output = 1 end return output end  @everywhere using Distributions #------------------------------------------------------------ # Let's redefine the function @everywhere so it can run on # the newly added workers #----------------------------------------------------------- @everywhere function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0)) #draw NbPoints from within the square centered in 0 #with side length equal to 2 xDraws = rand(d, nbPoints) yDraws = rand(d, nbPoints) sumInCircle = 0 for (xValue, yValue) in zip(xDraws, yDraws) sumInCircle+=inside_circle(xValue, yValue) end return 4*sumInCircle/nbPoints end  @everywhere function pi_parallel(nbPoints::Int64 = 128 * 1000) # to store different approximations #---------------------------------- piWorkers = zeros(nworkers()) # to store Futures #----------------- listFutures=[] # divide the draws among workers #------------------------------- nbDraws = Int(nbPoints/4) # each calculate its own approximation #------------------------------------- for (workerIndex, w) in enumerate(workers()) push!(listFutures, @spawnat w pi_serial(nbDraws)) end # let's fetch results #-------------------- for (workerIndex, w) in enumerate(workers()) piWorkers[workerIndex] = fetch(listFutures[workerIndex]) end # return the mean value across worker return mean(piWorkers) end  minPoints = 128 * 100000 maxPoints = 128 * 1000000 gridPoints = collect(minPoints:minPoints:maxPoints) nbGridPoints = length(gridPoints) elapsedTimeNW = zeros(nbGridPoints) approximationPiNW = zeros(nbGridPoints) for (index, nbDraws) in enumerate(gridPoints) approximationPiNW[index] = pi_parallel(nbDraws); #Store value elapsedTimeNW[index] = @elapsed pi_parallel(nbDraws); #Store time end  Serial vs parallel comparisons In terms of accuracy, the serial and the parallel codes generate the same results (modulo randomness). In terms of speed, the parallel version is up to 2.5 times faster. The more points are drawn, the higher the speed-gains. This example shows the well-established fact that the advantages of parallel computing start to kick-in when the underlying tasks are time-consuming in the first place.\np = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, abs.(approximationPiNW .- pi), label = \u0026quot;Parallel\u0026quot;) Plots.title!(\u0026quot;Error\u0026quot;) Plots.xlabel!(\u0026quot;nb Draws\u0026quot;) Plots.ylabel!(\u0026quot;Error\u0026quot;) display(p) Plots.savefig(p,\u0026quot;error_comparison.png\u0026quot;)  p = Plots.plot(gridPoints, elapsedTime1W, label = \u0026quot;Serial\u0026quot;) Plots.plot!(gridPoints, elapsedTimeNW, label = \u0026quot;Parallel\u0026quot;) Plots.plot!(gridPoints, elapsedTime1W./elapsedTimeNW, label = \u0026quot;Speed-up\u0026quot;) Plots.xlabel!(\u0026quot;nb Draws\u0026quot;) Plots.ylabel!(\u0026quot;Time (s)\u0026quot;) display(p) Plots.savefig(\u0026quot;Speed_gains.png\u0026quot;)  ","date":1552931602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552931602,"objectID":"56a6614a63bc5a3030cb63aa8903ae8d","permalink":"https://julienpascal.github.io/post/primerparallel/","publishdate":"2019-03-18T18:53:22+01:00","relpermalink":"/post/primerparallel/","section":"post","summary":"A Primer to Parallel Computing with Julia With this post, my aim is to provide a non-technical introduction to parallel computing using Julia. Our goal is to calculate an approximation of $\\pi$ using Monte-Carlo. I will use this example to introduce some basic Julia functions and concepts. For a more rigorous explanation, the manual is a must-read.\nCalculating $\\pi$ using Monte-Carlo Our strategy to calculate an approximation of $\\pi$ is quite simple.","tags":[],"title":"A Primer to Parallel Computing with Julia","type":"post"},{"authors":null,"categories":null,"content":"We analyse the consequences of the minimum wage on employment and sorting in a model of the labor market with search frictions, heterogeneous workers and firms, and business cycle fluctuations. This is a joint project with Jeremy Lise and Jean-Marc Robin.\n","date":1552583467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552583467,"objectID":"84e3bc7e75d0d96edd8e02c77c55385b","permalink":"https://julienpascal.github.io/project/minwage/","publishdate":"2019-03-14T18:11:07+01:00","relpermalink":"/project/minwage/","section":"project","summary":"We analyse the consequences of the minimum wage on employment and sorting in a model of the labor market with search frictions, heterogeneous workers and firms, and business cycle fluctuations.","tags":["WIP"],"title":"Labor Policy in a Dynamic Search-Matching Model with Heterogeneous Workers and Firms","type":"project"},{"authors":null,"categories":null,"content":"We develop a theoretical framework to evaluate the contribution of different payroll tax schedules to business cycle fluctuations. We build and estimate a dynamic model of the labor market with search frictions, heterogeneous workers, an aggregate TFP shock and a non-linear payroll tax schedule.\nWe estimate the model using Italian administrative data for the period 1977-2012 and use our estimated framework to quantitatively evaluate how different payroll tax schedules can amplify business cycle shocks for different types of workers.\nThis is a joint project with Nicolò Dalvit.\n\n","date":1552497078,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552497078,"objectID":"f0eb6212aae6aa66301eaf6a155ac2bd","permalink":"https://julienpascal.github.io/project/taxation_labor/","publishdate":"2019-03-13T18:11:18+01:00","relpermalink":"/project/taxation_labor/","section":"project","summary":"We analyse the impact of labor income tax in an heterogeneous workers framework with search frictions and aggregate shocks.","tags":["WIP"],"title":"Labor Tax in a Dynamic Search-and-Matching Model","type":"project"},{"authors":null,"categories":null,"content":"I exploit a spatial discontinuity introduced by a French reform in September 2015 to measure the links between commuting costs, employment and location decisions. In the municipalities benefiting from the reform, I find that the reform led to a 2% decrease in the number of unemployed workers registered in the French unemployment agency. The employment effect is concentrated on long-term unemployed workers. I build a simple spatial search-and-matching to underline the mechanism at play.\n\n\n","date":1552410667,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552410667,"objectID":"e505c91dcb71dc0cf22f71fcd9e26221","permalink":"https://julienpascal.github.io/project/spatialeq/","publishdate":"2019-03-12T18:11:07+01:00","relpermalink":"/project/spatialeq/","section":"project","summary":"I exploit a spatial discontinuity introduced by a French reform in September 2015 to measure the links between commuting costs, employment and location decisions.","tags":["WIP"],"title":"Spatial Equilibrium and Commuting Costs","type":"project"},{"authors":null,"categories":null,"content":"This paper analyzes the determinants of labor income shocks along the business cycle. My main finding is that sorting between firms and workers is a key component of idiosyncratic risk. Labor income shocks are analyzed through the lenses of a dynamic search-and-matching model, which I estimate using US data. Because of search frictions and mismatches between firms and workers, the laissez-faire equilibrium is not necessarily optimal. My results underline that the government can tame business cycle fluctuations by designing a simple unemployment insurance scheme improving sorting between firms and workers.\n\n","date":1552324267,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552324267,"objectID":"0e3c0c721bd8e6e7f0a2ae34bb2564bc","permalink":"https://julienpascal.github.io/project/laborincomeshocks/","publishdate":"2019-03-11T18:11:07+01:00","relpermalink":"/project/laborincomeshocks/","section":"project","summary":"I explore the determinants of labor income shocks along the business cycle with a frictional model of the labor market.","tags":["WIP"],"title":"Labor Income Shocks Along the Business Cycle","type":"project"},{"authors":[],"categories":[],"content":" NOTE\nThis post is outdated. With the advent of Julia 1.0, the workflow for creating packages was significantly altered. An excellent guide can be found here.\nIn this post, my goal is to briefly explain how to create an unregistered Julia package for Julia 0.6.4, how to synchronize it with your Github account, and how to start testing your code automatically using TRAVIS CI. I started writing this post as a reminder to myself. I am posting it here with the hope that it may be useful for someone else. More on this topic can be found by reading the official Julia\u0026rsquo;s manual.\nWhy Creating a Package? A package to share academic work My research projects often involve data manipulation and/or implementing algorithms. I discovered that writing my codes in the form of a package helps me in producing better and reusable code. Creating a package to share your academic work is also very much in line with the idea that scientific research should be reproducible. Users can download your work and install the required dependencies using a single line :\ngit.clone(\u0026quot;https://github.com/YourGithubUsername/YourPackage.jl.git\u0026quot;)  Continuous Integration Another major advantage of creating a package is that it makes your life much easier when it comes to testing your code automatically using TRAVIS CI. TRAVIS CI is a continuous integration system, which considerably helps in detecting and resolving bugs at an early stage.\nStep-by-step tutorial In what follows, I am assuming you are using Linux, with julia version 0.6 installed. If you are using a different version, just replace v0.6 by the number corresponding to your current version of julia. You also need to have the package PkgDev installed.\nStep 1: Generate your package The following two lines will create a directory called \u0026quot;MyPackage.jl\u0026quot; with an MIT License, in Julia\u0026rsquo;s package location:\nusing PkgDev PkgDev.generate(\u0026quot;MyPackage.jl\u0026quot;,\u0026quot;MIT\u0026quot;)  By convention, Julia repository names and with .jl. If you change your working directory to your newly created package (cd ~/.julia/v0.6/MyPackage), you will notice that the following files and directories have been created:\n\\src The \\src folder will contain your source code. By default, it contains a file \u0026ldquo;MyPackage.jl\u0026rdquo;, which you will use to load other packages and to include .jl files that you created. In this file, you also state the functions and types you want to export. As an example, you may consult the package Distributions.\n\\test This folder contains a file runtests.jl, in which you can include unit-tests. Within julia, you can simply run your series of unit-tests with the command:\nPkg.test(\u0026quot;MyPackage\u0026quot;)  REQUIRE This file is used to specify the required dependencies. When a user Pkg.clone() your package, Julia\u0026rsquo;s package manager will make sure that these requirements are met. For instance, let\u0026rsquo;s say that your package relies on the version 0.6 of Julia (or higher) and the package JSON. The REQUIRE file will be the following :\njulia 0.6 JSON  README.md You can use this file to add a description of you package.\nLICENSE.md To guide you in the choice of a licence, you may want to consult the following website: https://choosealicense.com/\nStep 2: Set-up your working environment This step is optional. While you may want to develop you package directly from Julia\u0026rsquo;s package directory (~/.julia/v0.6 if you are using julia v0.6), I personally find it unpleasant. I usually create a symlink to a more convenient location:\nln -s ~/.julia/v0.6/MyPackage your/convenient/directory/MyPackage  After running this line in the terminal, you can start working on your package directly from your/convenient/directory.\nStep 3: Synchronize with GitHub The following step will synchronize your package with your GitHub account. After creating a repository named \u0026ldquo;MyPackage.jl\u0026rdquo; on GitHub, enter the following commands in the terminal:\ngit add -A git commit -m \u0026quot;First commit\u0026quot; git remote add origin https://github.com/YourGithubUsername/MyPackage.jl.git git push -u origin master  Going to the page https://github.com/YourGithubUsername/MyPackage.jl.git, you should now see folders and files mentioned above. Some extra files are also going to be there, for instance .gitignore or appveyor.yml. You can ignore them for the time being. After this initial commit, you are almost all set and you can use the usual GitHub workflow. A good idea though is to enable TRAVIS CI for the repository just you created.\nStep 4: Set-up TRAVIS CI From your GitHub account, sign in to either:\n TravisCI.org if your repository is public TravisCI.com if your repository is private  On TRAVIS CI, go to your profile page. Enable your repository \u0026ldquo;YourGithubUsername/MyPackage.jl\u0026rdquo; by flicking the switch one. Every time you push a new commit, your set of tests, launched by the file /test/runtests.jl, will be automatically executed on a separate virtual environment. If one of your tests fails, you will be notified by e-mail and (most of the time) you will be able to spot the origin of the error quite easily.\n","date":1528295678,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528295678,"objectID":"a2695d3b77c42a7382e2c7a98d4099a5","permalink":"https://julienpascal.github.io/post/julia_package/","publishdate":"2018-06-06T15:34:38+01:00","relpermalink":"/post/julia_package/","section":"post","summary":"NOTE\nThis post is outdated. With the advent of Julia 1.0, the workflow for creating packages was significantly altered. An excellent guide can be found here.\nIn this post, my goal is to briefly explain how to create an unregistered Julia package for Julia 0.6.4, how to synchronize it with your Github account, and how to start testing your code automatically using TRAVIS CI. I started writing this post as a reminder to myself.","tags":[],"title":"How to Create a Julia Package","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" In my previous post, I discussed how the the simulated method of moments can be used to estimate parameters without using the likelihood function. This method is useful because many \u0026ldquo;real-life\u0026rdquo; applications result in untractable likelihood functions. In this post, I use the same toy example (estimation of the mean of a mutlivariate normal random variable) and show how to use the parallel computing capabilities of Julia and MomentOpt to speed-up the estimation.\nAdding workers In this example, the goal is to estimate the mean of 4-dimension normal random variable with unit variance, without using any information on the likelihood. If you start Julia with several processors, MomentOpt will notice it and execute the code in parallel. The first step is to add \u0026ldquo;workers\u0026rdquo; to Julia. A rule of thumb is to use as many workers as you have processors on your system (4 in my case).\n# Current number of workers #-------------------------- currentWorkers = nprocs() println(\u0026quot;Initial number of workers = $(currentWorkers)\u0026quot;) # I want to have 4 workers running #-------------------------------- maxNumberWorkers = 4 while nprocs() \u0026lt; maxNumberWorkers addprocs(1) end # check the number of workers: #---------------------------- currentWorkers = nprocs() println(\u0026quot;Number of workers = $(currentWorkers)\u0026quot;) Initial number of workers = 1 Number of workers = 4  @everywhere When running Julia with several workers, you have to add the macro @everywhere when loading packages and defining functions. More details on parallel computing with Julia can be found here.\n#--------------------------------------------------------------------------------------------------------- # Julien Pascal # https://julienpascal.github.io/ # last edit: 06/06/2018 # # Julia script that shows how the simulated method of moments can be used in # a simple setting: estimation of the mean of a Normal r.v. # This version was built to be executed with several processors # For instance, start julia with: julia -p 4 # # I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl # # Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl #---------------------------------------------------------------------------------------------------------- @everywhere using MomentOpt @everywhere using GLM @everywhere using DataStructures @everywhere using DataFrames @everywhere using Plots #plotlyjs() @everywhere pyplot() #------------------------------------------------ # Options #------------------------------------------------- # Boolean: do you want to save the plots to disk? savePlots = true #------------------------ # initialize the problem: #------------------------ # Specify the initial values for the parameters, and their support: #------------------------------------------------------------------ pb = OrderedDict(\u0026quot;p1\u0026quot; =\u0026gt; [0.2,-3,3] , \u0026quot;p2\u0026quot; =\u0026gt; [-0.2,-2,2], \u0026quot;p3\u0026quot; =\u0026gt; [0.1,0,10], \u0026quot;p4\u0026quot; =\u0026gt; [-0.1,-10,0]) # Specify moments to be matched + subjective weights: #---------------------------------------------------- trueValues = OrderedDict(\u0026quot;mu1\u0026quot; =\u0026gt; [-1.0] , \u0026quot;mu2\u0026quot; =\u0026gt; [1.0], \u0026quot;mu3\u0026quot; =\u0026gt; [5.0], \u0026quot;mu4\u0026quot; =\u0026gt; [-4.0]) moms = DataFrame(name=[\u0026quot;mu1\u0026quot;,\u0026quot;mu2\u0026quot;,\u0026quot;mu3\u0026quot;, \u0026quot;mu4\u0026quot;],value=[-1.0,1.0, 5.0, -4.0], weight=ones(4)) # objfunc_normal(ev::Eval) # # GMM objective function to be minized. # It returns a weigthed distance between empirical and simulated moments # @everywhere function objfunc_normal(ev::Eval; verbose = false) start(ev) # when running in parallel, display worker's id: #----------------------------------------------- if verbose == true if nprocs() \u0026gt; 1 println(myid()) end end # extract parameters from ev: #---------------------------- mu = collect(values(ev.params)) # compute simulated moments #-------------------------- # Monte-Carlo: #------------- ns = 10000 #number of i.i.d draws from N([mu], sigma) #initialize a multivariate normal N([mu], sigma) #mu is a four dimensional object #sigma is set to be the identity matrix sigma = [1.0 ;1.0; 1.0; 1.0] # draw ns observations from N([mu], sigma): randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma)) # calculate the mean of the simulated data simM = mean(rand(randMultiNormal,ns),2) # store simulated moments in a dictionary simMoments = Dict(:mu1 =\u0026gt; simM[1], :mu2 =\u0026gt; simM[2], :mu3 =\u0026gt; simM[3], :mu4 =\u0026gt; simM[4]) # Calculate the weighted distance between empirical moments # and simulated ones: #----------------------------------------------------------- v = Dict{Symbol,Float64}() for (k, mom) in dataMomentd(ev) # If weight for moment k exists: #------------------------------- if haskey(MomentOpt.dataMomentWd(ev), k) # divide by weight associated to moment k: #---------------------------------------- v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2 else v[k] = ((simMoments[k] .- mom) ) .^2 end end # Set value of the objective function: #------------------------------------ setValue(ev, mean(collect(values(v)))) # also return the moments #----------------------- setMoment(ev, simMoments) # flag for success: #------------------- ev.status = 1 # finish and return finish(ev) return ev end # Initialize an empty MProb() object: #------------------------------------ mprob = MProb() # Add structural parameters to MProb(): # specify starting values and support #-------------------------------------- addSampledParam!(mprob,pb) # Add moments to be matched to MProb(): #-------------------------------------- addMoment!(mprob,moms) # Attach an objective function to MProb(): #---------------------------------------- addEvalFunc!(mprob, objfunc_normal) # estimation options: #-------------------- # number of iterations for each chain niter = 1000 # number of chains # nchains = nprocs() nchains = 4 opts = Dict(\u0026quot;N\u0026quot;=\u0026gt;nchains, \u0026quot;maxiter\u0026quot;=\u0026gt;niter, \u0026quot;maxtemp\u0026quot;=\u0026gt; 5, \u0026quot;coverage\u0026quot;=\u0026gt;0.025, \u0026quot;sigma_update_steps\u0026quot;=\u0026gt;10, \u0026quot;sigma_adjust_by\u0026quot;=\u0026gt;0.01, \u0026quot;smpl_iters\u0026quot;=\u0026gt;1000, \u0026quot;parallel\u0026quot;=\u0026gt;true, \u0026quot;maxdists\u0026quot;=\u0026gt;[0.05 for i in 1:nchains], \u0026quot;mixprob\u0026quot;=\u0026gt;0.3, \u0026quot;acc_tuner\u0026quot;=\u0026gt;12.0, \u0026quot;animate\u0026quot;=\u0026gt;false) #--------------------------------------- # Let's set-up and run the optimization #--------------------------------------- # set-up BGP algorithm: MA = MAlgoBGP(mprob,opts) # run the estimation: @time MomentOpt.runMOpt!(MA) # show a summary of the optimization: @show MomentOpt.summary(MA)  Inference When using the BGP algorithm, inference can be done using the first chain only. Other chains are used to explore the state space and help to exit potential local minima, but they are not meant to be used for inference. I discard the first 10th observations to get rid of the influence of the starting values. Visual inspection of the first chain suggests that the stationary part of the Markov chain was reached at this stage. I then report the quasi posterior mean and median for each parameter. As reported below, we are quite close to the true values.\n# Plot histograms for the first chain, the one with which inference should be done. # Other chains are used to explore the space and avoid local minima #------------------------------------------------------------------------------- p1 = histogram(MA.chains[1]) display(p1) if savePlots == true savefig(p1, joinpath(pwd(),\u0026quot;histogram_chain1.svg\u0026quot;)) end # Plot the realization of the first chain #---------------------------------------- p2 = plot(MA.chains[1]) if savePlots == true savefig(p2, joinpath(pwd(),\u0026quot;history_chain_1.svg\u0026quot;)) end display(p2) # Realization of the first chain: #------------------------------- dat_chain1 = MomentOpt.history(MA.chains[1]) # discard the first 10th of the observations (\u0026quot;burn-in\u0026quot; phase): #-------------------------------------------------------------- dat_chain1[round(Int, niter/10):niter, :] # keep only accepted draws: #-------------------------- dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ] # create a list with the parameters to be estimated parameters = [Symbol(String(\u0026quot;mu$(i)\u0026quot;)) for i=1:4] # list with the corresponding priors: #------------------------------------ estimatedParameters = [Symbol(String(\u0026quot;p$(i)\u0026quot;)) for i=1:4] # Quasi Posterior mean and quasi posterior median for each parameter: #------------------------------------------------------------------- for (estimatedParameter, param) in zip(estimatedParameters, parameters) println(\u0026quot;Quasi posterior mean for $(String(estimatedParameter)) = $(mean(dat_chain1[estimatedParameter]))\u0026quot;) println(\u0026quot;Quasi posterior median for $(String(estimatedParameter)) = $(median(dat_chain1[estimatedParameter]))\u0026quot;) println(\u0026quot;True value = $(trueValues[String(param)][])\u0026quot;) end # Output: #-------- # Quasi posterior mean for p1 = -0.9160461484604642 # Quasi posterior median for p1 = -0.9589739759449558 # True value = -1.0 # Quasi posterior mean for p2 = 0.9888798123473025 # Quasi posterior median for p2 = 1.0675028518780796 # True value = 1.0 # Quasi posterior mean for p3 = 4.922658319685393 # Quasi posterior median for p3 = 4.989662707150382 # True value = 5.0 # Quasi posterior mean for p4 = -3.898597557236236 # Quasi posterior median for p4 = -3.968649064061086 # True value = -4.0  Figure 1         History of chain 1    Figure 2         Histograms for chain 1    Safety checks In this toy example, we know that the conditions for global identification are met. However, in more complicated applications, global identification may be hard to prove analytically. A common practice is to make sure that the objective function is \u0026ldquo;well-behaved\u0026rdquo; in a neighborhood of the solution using slices. The graph below shows that there is no flat region in the neighborhood of the solution, suggesting (at least) local identification of the parameters.\n# plot slices of objective function # grid with 20 points #----------------------------------- s = doSlices(mprob,20) # plot slices of the objective function: #--------------------------------------- p = MomentOpt.plot(s,:value) display(p) if savePlots == true Plots.savefig(p, joinpath(pwd(),\u0026quot;slices_Normal.svg\u0026quot;)) end # Produce more precise plots with respect to each parameter: #----------------------------------------------------------- for symbol in parameters p = MomentOpt.plot(s,symbol) display(p) if savePlots == true Plots.savefig(p, joinpath(pwd(),\u0026quot;slices_Normal_$(String(symbol)).svg\u0026quot;)) end end  Figure 3         Slices of the objective function    Parallel versus Serial Here is benchmark of running the code above in serial versus in parallel (starting julia with 4 workers):\n Serial: 639.661831 seconds (12.52 M allocations: 1.972 GiB, 97.50% gc time) Parallel: 372.454707 seconds (279.32 M allocations: 14.843 GiB, 2.19% gc time)  Computing time is approximately divided by 2 when executing the parallel version.\nNotebook A jupyter notebook containing the code in this post (with some slight modifications) can be downloaded here.\n","date":1528295169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528295169,"objectID":"a160ada05108f0a665014ad22feafebf","permalink":"https://julienpascal.github.io/post/smm_parallel/","publishdate":"2018-06-06T15:26:09+01:00","relpermalink":"/post/smm_parallel/","section":"post","summary":"In my previous post, I discussed how the the simulated method of moments can be used to estimate parameters without using the likelihood function. This method is useful because many \u0026ldquo;real-life\u0026rdquo; applications result in untractable likelihood functions. In this post, I use the same toy example (estimation of the mean of a mutlivariate normal random variable) and show how to use the parallel computing capabilities of Julia and MomentOpt to speed-up the estimation.","tags":[],"title":"The Simulated Method of Moments: a Parallel Implementation","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" As Thomas Sargent said:\n \u0026ldquo;A rational expectations equilibrium model is a likelihood function\u0026rdquo;\n However in many cases, the likelihood function is too complicated to be written down in closed form. To estimate the structural parameters of a given model, one can still use Monte-Carlo methods. In this post, I would like to describe the simulated method of moments (SMM), which is a widely used simulation-based estimation technique.\nA Simple Setting I want to illustrate the SMM in one of the simplest settings you could think of: the estimation of the mean of a normal density with known variance. Let\u0026rsquo;s say we have access to a (bi-dimensional) time series and we suspect it to be normally distributed with mean $[a,\\,b]\u0026lsquo;$ and variance the identity matrix $\\mathcal{N}([a,\\,b]\u0026lsquo;,\\,I_2)$. Let\u0026rsquo;s pretend that we have no idea on how to write down the associated likelihood function. The good news is that if we have access to a \u0026ldquo;black box\u0026rdquo; that generates $i.i.d$ draws from the law $\\mathcal{N}([c,\\,d]\u0026lsquo;,\\,I_2)$, it is enough for us to do inference.\nSMM is GMM The SMM estimator can be viewed as an extension of GMM. The difference being that the function mapping the set of parameters to moments has no closed-form expression. Mathematically, we want to minimize the following objective function:\nwhere $m^*$ is a vector empirical moments, $m(\\theta)$ a vector of the same moments calculated using simulated data when the structural parameters are equal to $\\theta$, and $W$ a weighing matrix.\nThe SMM estimate is such that the (weighted) distance between simulated and empirical moments is minimized. This estimator is quite intuitive: under the hypothesis that the model is correctly specified, it should be able to reproduce empirical moments when parameters values are equal to the \u0026ldquo;true\u0026rdquo; ones.\nInference Under some regularity conditions (see McFadden 1989), the extra noise introduced by simulation is not problematic and inference is possible. That is, we can build a confidence intervals for the SMM estimates.\nImplementation in Julia The code below shows how one can recover the true parameters of the Normal density $\\mathcal{N}([a,\\,b]\u0026lsquo;,\\,I_2)$. I use the MomentOpt package, which relies on some refinements of the MCMC method to explore the state-space with several Markov chains in parallel. These Markov chains communicate between themselves to avoid being stuck in a local mode of the posterior distribution. In practice, I choose 5 Markov chains. Figure 1 shows the realizations for the first chain. As illustrated in Figure 2, we successfully recovered the true values for $a$ and $b$. For each chain, the quasi-posterior mean and median for $a$ and $b$ are extremely close to the true values $1$ and $-1$.\nFigure 1         History of one chain    Figure 2         Histograms for the 5 chains    #--------------------------------------------------------------------------------------------------------- # Julien Pascal # last edit: 12/02/2018 # # Julia script that shows how the simulated # method of moments can be used in a simple # setting: estimation of the mean of a Normal r.v. # # I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl # # Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl #---------------------------------------------------------------------------------------------------------- using MomentOpt using GLM using DataStructures using DataFrames using Plots #plotlyjs() pyplot() #------------------------------------------------ # Options #------------------------------------------------- # Boolean: do you want to save the plots to disk? savePlots = true # initialize the problem: #------------------------ # initial values: #---------------- pb = OrderedDict(\u0026quot;p1\u0026quot; =\u0026gt; [0.2,-3,3] , \u0026quot;p2\u0026quot; =\u0026gt; [-0.2,-2,2] ) # moments to be matched: #----------------------- moms = DataFrame(name=[\u0026quot;mu1\u0026quot;,\u0026quot;mu2\u0026quot;],value=[-1.0,1.0], weight=ones(2)) \u0026quot;\u0026quot;\u0026quot; objfunc_normal(ev::Eval) GMM objective function to be minized. It returns a weigthed distance between empirical and simulated moments copy-paste of the function objfunc_norm(ev::Eval) I only made minor modifications to the original fuction \u0026quot;\u0026quot;\u0026quot; function objfunc_normal(ev::Eval) start(ev) # extract parameters from ev: #---------------------------- mu = collect(values(ev.params)) # compute simulated moments #-------------------------- # Monte-Carlo: #------------- ns = 10000 #number of i.i.d draws from N([a,b], sigma) #initialize a multivariate normal N([a,b], sigma) #using a = mu[1], b=mu[2] sigma = [1.0 ;1.0] randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma)) simM = mean(rand(randMultiNormal,ns),2) #mean of simulated data simMoments = Dict(:mu1 =\u0026gt; simM[1], :mu2 =\u0026gt; simM[2])#store simulated moments in a dictionary # Calculate the weighted distance between empirical moments # and simulated ones: #----------------------------------------------------------- v = Dict{Symbol,Float64}() for (k, mom) in dataMomentd(ev) # If weight for moment k exists: #------------------------------- if haskey(MomentOpt.dataMomentWd(ev), k) # divide by weight associated to moment k: #---------------------------------------- v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2 else v[k] = ((simMoments[k] .- mom) ) .^2 end end # Set value of the objective function: #------------------------------------ setValue(ev, mean(collect(values(v)))) # also return the moments #----------------------- setMoment(ev, simMoments) # flag for success: #------------------- ev.status = 1 # finish and return finish(ev) return ev end # Initialize an empty MProb() object: #------------------------------------ mprob = MProb() # Add structural parameters to MProb(): # specify starting values and support #-------------------------------------- addSampledParam!(mprob,pb) # Add moments to be matched to MProb(): #-------------------------------------- addMoment!(mprob,moms) # Attach an objective function to MProb(): #---------------------------------------- addEvalFunc!(mprob, objfunc_normal) # estimation options: #-------------------- # number of iterations for each chain niter = 1000 # number of chains nchains = 5 opts = Dict(\u0026quot;N\u0026quot;=\u0026gt;nchains, \u0026quot;maxiter\u0026quot;=\u0026gt;niter, \u0026quot;maxtemp\u0026quot;=\u0026gt; 5, # choose inital sd for each parameter p # such that Pr( x \\in [init-b,init+b]) = 0.975 # where b = (p[:ub]-p[:lb])*opts[\u0026quot;coverage\u0026quot;] i.e. the fraction of the search interval you want to search around the initial value \u0026quot;coverage\u0026quot;=\u0026gt;0.025, # i.e. this gives you a 95% CI about the current parameter on chain number 1. \u0026quot;sigma_update_steps\u0026quot;=\u0026gt;10, \u0026quot;sigma_adjust_by\u0026quot;=\u0026gt;0.01, \u0026quot;smpl_iters\u0026quot;=\u0026gt;1000, \u0026quot;parallel\u0026quot;=\u0026gt;true, \u0026quot;maxdists\u0026quot;=\u0026gt;[0.05 for i in 1:nchains], \u0026quot;mixprob\u0026quot;=\u0026gt;0.3, \u0026quot;acc_tuner\u0026quot;=\u0026gt;12.0, \u0026quot;animate\u0026quot;=\u0026gt;false) # plot slices of objective function #--------------------------------- s = doSlices(mprob,30) # plot objective function over param values: #------------------------------------------- p1 = MomentOpt.plot(s,:value) if savePlots == true Plots.savefig(p1, joinpath(pwd(),\u0026quot;slices_Normal1.svg\u0026quot;)) end # plot value of moment :mu1 over param values #-------------------------------------------- p2 = MomentOpt.plot(s,:mu1) if savePlots == true Plots.savefig(p2, joinpath(pwd(),\u0026quot;slices_Normal2.svg\u0026quot;)) end # plot value of moment :mu2 over param values #-------------------------------------------- p3 = Plots.plot(s,:mu2) if savePlots == true Plots.savefig(p3, joinpath(pwd(),\u0026quot;slices_Normal3.svg\u0026quot;)) end #--------------------------------------- # Let's set-up and run the optimization #--------------------------------------- # set-up BGP algorithm: MA = MAlgoBGP(mprob,opts) # run the estimation: @time MomentOpt.runMOpt!(MA) # show a summary of the optimization: @show MomentOpt.summary(MA) # Plot histograms for chains: #---------------------------- p4 = histogram(MA.chains[1]) p5 = histogram(MA.chains[2]) p6 = histogram(MA.chains[3]) p7 = histogram(MA.chains[4]) p8 = histogram(MA.chains[5]) p9 = Plots.plot(p4, p5, p6, p7, p8, layout=(5,1), legend=false) if savePlots == true savefig(p9, joinpath(pwd(),\u0026quot;histogram.svg\u0026quot;)) end # Plot the \u0026quot;history\u0026quot; of one chain: #-------------------------------- p10 = plot(MA.chains[1]) if savePlots == true savefig(p10, joinpath(pwd(),\u0026quot;history_chain_1.svg\u0026quot;)) end # Realization of chain 1: #----------------------- dat_chain1 = MomentOpt.history(MA.chains[1]) # keep only accepted draws: #------------------------- dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ] # Quasi Posterior mean #--------------------- QP_mean_p1 = mean(dat_chain1[:p1]) QP_mean_p2 = mean(dat_chain1[:p2]) # Quasi Posterior median #----------------------- QP_median_p1 = median(dat_chain1[:p1]) QP_median_p2 = median(dat_chain1[:p2])  ","date":1518444453,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518444453,"objectID":"65cd9e8ec5e2630b42fad15e6ec31b76","permalink":"https://julienpascal.github.io/post/smm/","publishdate":"2018-02-12T15:07:33+01:00","relpermalink":"/post/smm/","section":"post","summary":"As Thomas Sargent said:\n \u0026ldquo;A rational expectations equilibrium model is a likelihood function\u0026rdquo;\n However in many cases, the likelihood function is too complicated to be written down in closed form. To estimate the structural parameters of a given model, one can still use Monte-Carlo methods. In this post, I would like to describe the simulated method of moments (SMM), which is a widely used simulation-based estimation technique.","tags":[],"title":"The Simulated Method of Moments","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" A large class of economic models involves solving for functional equations of the form:\nA well known example is the stochastic optimal growth model. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period\u0026rsquo;s output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.\nA common technique for solving this class of problem is value function iteration. While value function iteration is quite intuitive, it is not the only one available. In this post, I describe the collocation method, which transforms the problem of finding a function into a problem of finding a vector that satisfies a given set of conditions. The gain from this change of perspective is that any root-finding algorithm can then be used. In particular, one may use the Newton method, which converges at a quadratic rate in the neighborhood of the solution if the function is smooth enough.\nValue function iteration Value function iteration takes advantage of the fact that the Bellman operator $T$ is a contraction mapping on the set of continuous bounded functions on $\\mathbb R_+$ under the supremum distance\nAn immediate consequence if that the sequence $w,Tw,T^2w$,… converges uniformly to $w$ (starting with any bounded and continuous function $w$). The following code in Julia v0.6.4 illustrates the convergence of the series ${T^nw}$.\n#= Julien Pascal Code heavily based on: ---------------------- https://lectures.quantecon.org/jl/optgrowth.html by Spencer Lyon, John Stachurski I have only made minor modifications: #------------------------------------ * I added a type optGrowth * I use the package Interpolations * I calculate the expectation w.r.t the aggregate shock using a Gauss-Legendre quadrature scheme instead of Monte-Carlo =# using QuantEcon using Optim using CompEcon using PyPlot using Interpolations using FileIO type optGrowth w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool w_func::Function end function optGrowth(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true ) grid_y = collect(linspace(1e-5, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log(c) # Production f(k) = k^α w = 5 * log.(grid_y) Tw = 5 * log.(grid_y) σ = 5 * log.(grid_y) el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) w_func = x -\u0026gt; x optGrowth( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, w_func ) end \u0026quot;\u0026quot;\u0026quot; The approximate Bellman operator, which computes and returns the updated value function Tw on the grid points. #### Arguments `model` : a model of type optGrowth `Modifies model.σ, model.w and model.Tw \u0026quot;\u0026quot;\u0026quot; function bellman_operator!(model::optGrowth) # === Apply linear interpolation to w === # knots = (model.grid,) itp = interpolate(knots, model.w, Gridded(Linear())) #w_func(x) = itp[x] model.w_func = x -\u0026gt; itp[x] if model.compute_policy model.σ = similar(model.w) end # == set Tw[i] = max_c { u(c) + β E w(f(y - c) z)} == # for (i, y) in enumerate(model.grid) #Monte Carlo #----------- #objective(c) = - model.u(c) - model.β * mean(w_func(model.f(y - c) .* model.shocks)) #Gauss-Legendre #-------------- function objective(c) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end res = optimize(objective, 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = - Optim.minimum(res) end end model = optGrowth() function solve_optgrowth!(model::optGrowth; tol::AbstractFloat=1e-6, max_iter::Integer=500) w_old = copy(model.w) # Set initial condition error = tol + 1 i = 0 # Iterate to find solution while i \u0026lt; max_iter #update model.w bellman_operator!(model) error = maximum(abs, model.w - w_old) if error \u0026lt; tol break end w_old = copy(model.w) i += 1 end end #----------------------------------- # Solve by value function iteration #----------------------------------- @time solve_optgrowth!(model) # 3.230501 seconds (118.18 M allocations: 1.776 GiB, 3.51% gc time) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Approximated (VFI)\u0026quot;, linestyle=\u0026quot;--\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Exact value\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;value_function_iteration.png\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](0.00, 0.008) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;error_value_function_iteration.png\u0026quot;)  Output The following figure shows the exact function, which we know in this very specific case, and the one calculated using VFI. Both quantities are almost indistinguishable. As the illustrated in the next figure, the (absolute value) of the distance between the true and the approximated values is bounded above by $0.006$. The accuracy of the current approximation could be improved by iterating the process further. The collocation method The collocation method takes a different route. Let us remember that we are looking for a function $w$. Instead of solving for the values of $w$ on a grid and then interpolating, why not looking for a function directly? To do so, let us assume that $w$ can reasonably be approximated by a function $\\hat{w}$:\nwith $ \\phi_1(x) $ , $ \\phi_2(x) $,\u0026hellip;, $ \\phi_n(x) $ a set of linearly independent basis functions and $c_1$, $c_2$, \u0026hellip;, $c_n$ $n$ coefficient to be found. Replacing $w(x)$ with $\\hat{w(x)}$ into the functional equation and reorganizing gives:\nThis equation has to hold (almost) exactly at $n$ points (also called nodes): $y_1$, $y_2$, \u0026hellip;, $y_n$:\nThe equation above defines a system of $n$ equation with as many unknown, which can be compactly written as: $$ f(\\boldsymbol{c}) = \\boldsymbol{0} $$\nNewton or quasi-Newton can be used to solve for the root of $f$. In the code that follows, I use Broyden\u0026rsquo;s method. Let us illustrate this technique using a Chebychev polynomial basis and Chebychev nodes. In doing so, we avoid Runge\u0026rsquo;s phenomenon associated with a uniform grid.\nImplementation using CompEcon #--------------------------------------------- # Julien Pascal # Solve the stochastic optimal growth problem # using the collocation method #--------------------------------------------- using QuantEcon using Optim using CompEcon using PyPlot using Interpolations type optGrowthCollocation w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool order_approximation::Int64 #number of element in the functional basis along each dimension functional_basis_type::String #type of functional basis fspace::Dict{Symbol,Any} #functional basis fnodes::Array{Float64,1} #collocation nodes residual::Array{Float64,1} #vector of residual. Should be close to zero a::Array{Float64,1} #polynomial coefficients w_func::Function end ##################################### # Function that finds a solution # to f(x) = 0 # using Broyden's \u0026quot;good\u0026quot; method # and a simple backstepping procedure as described # in Miranda and Fackler (2009) # # input : # -------- # * x0: initial guess for the root # * f: function in f(x) = 0 # * maxit: maximum number of iterations # * tol: tolerance level for the zero # * fjavinc: initial inverse of the jacobian. If not provided, then inverse of the # Jacobian is calculated by finite differences # * maxsteps: maximum number of backsteps # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian # # output : # -------- # * x: one zero of f # * it: number of iterations necessary to reached the solution # * fjacinv: pseudo jacobian at the last iteration # * fnorm: norm f(x) at the last iteration # ####################################### function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0)); maxsteps = 5, recaculateJacobian = 1) println(\u0026quot;a0 = $(x0)\u0026quot;) fnorm = tol*2 it2 = 0 #to re-initialize the jacobian ################################ #initialize guess for the matrix ################################ fjacinv_function = x-\u0026gt; Calculus.finite_difference_jacobian(f, x) #fjacinv_function = x -\u0026gt; ForwardDiff.gradient(f, x) # If the user do not provide an initial guess for the jacobian # One is calculated using finite differences. if fjacinv == eye(length(x0)) ################################################ # finite differences to approximate the Jacobian # at the initial value # this is slow. Seems to improve performances # when x0 is of high dimension. println(\u0026quot;Calculating the Jacobian by finite differences\u0026quot;) #@time fjacinv = Calculus.finite_difference_jacobian(f, x0) @time fjacinv = fjacinv_function(x0) println(\u0026quot;Inverting the Jacobian\u0026quot;) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end println(\u0026quot;Done\u0026quot;) else println(\u0026quot;Using User's input as a guess for the Jacobian.\u0026quot;) end fval = f(x0) for it=1:maxit it2 +=1 #every 30 iterations, reinitilize the jacobian if mod(it2, recaculateJacobian) == 0 println(\u0026quot;Re-calculating the Jacobian\u0026quot;) fjacinv = fjacinv_function(x0) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end end println(\u0026quot;it = $(it)\u0026quot;) fnorm = norm(fval) if fnorm \u0026lt; tol println(\u0026quot;fnorm = $(fnorm)\u0026quot;) return x0, it, fjacinv, fnorm end d = -(fjacinv*fval) fnormold = Inf ######################## # Backstepping procedure ######################## for backstep = 1:maxsteps if backstep \u0026gt; 1 println(\u0026quot;backstep = $(backstep-1)\u0026quot;) end fvalnew = f(x0 + d) fnormnew = norm(fvalnew) if fnormnew \u0026lt; fnorm break end if fnormold \u0026lt; fnormnew d=2*d break end fnormold = fnormnew d = d/2 end #################### #################### x0 = x0 + d fold = fval fval = f(x0) u = fjacinv*(fval - fold) #Update the pseudo Jacobian: fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u)) println(\u0026quot;a$(it) = $(x0)\u0026quot;) println(\u0026quot;fnorm = $(fnorm)\u0026quot;) if isnan.(x0) == trues(length(x0)) println(\u0026quot;Error. a$(it) = NaN for each component\u0026quot;) x0 = zeros(length(x0)) return x0, it, fjacinv, fnorm end end println(\u0026quot;In function find_broyden\\n\u0026quot;) println(\u0026quot;Maximum number of iterations reached.\\n\u0026quot;) println(\u0026quot;No convergence.\u0026quot;) println(\u0026quot;Returning fnorm = NaN as a solution\u0026quot;) fnorm = NaN return x0, maxit, fjacinv, fnorm end function optGrowthCollocation(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true, order_approximation = 40, functional_basis_type = \u0026quot;chebychev\u0026quot;, ) grid_y = collect(linspace(1e-5, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log.(c) # Production f(k) = k^α el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) lower_bound_support = minimum(grid_y) upper_bound_support = maximum(grid_y) n_functional_basis = [order_approximation] if functional_basis_type == \u0026quot;chebychev\u0026quot; fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support) elseif functional_basis_type == \u0026quot;splines\u0026quot; fspace = fundefn(:spli, n_functional_basis, lower_bound_support, upper_bound_support, 1) elseif functional_basis_type == \u0026quot;linear\u0026quot; fspace = fundefn(:lin, n_functional_basis, lower_bound_support, upper_bound_support) else error(\u0026quot;functional_basis_type has to be either chebychev, splines or linear.\u0026quot;) end fnodes = funnode(fspace)[1] residual = zeros(size(fnodes)[1]) a = ones(size(fnodes)[1]) w = ones(size(fnodes)[1]) Tw = ones(size(fnodes)[1]) σ = ones(size(fnodes)[1]) w_func = x-\u0026gt; x optGrowthCollocation( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, order_approximation, functional_basis_type, fspace, fnodes, residual, a, w_func ) end function residual!(model::optGrowthCollocation) model.w_func = y -\u0026gt; funeval(model.a, model.fspace, [y])[1][1] function objective(c, y) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end # Loop over nodes for i in 1:size(model.fnodes)[1] y = model.fnodes[i,1] res = optimize(c -\u0026gt; objective(c, y), 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = model.w_func(y) model.residual[i] = - model.w[i] + model.Tw[i] end end model = optGrowthCollocation(functional_basis_type = \u0026quot;chebychev\u0026quot;) residual!(model) function solve_optgrowth!(model::optGrowthCollocation; tol=1e-6, max_iter=500) # Initialize guess for coefficients # by giving the \u0026quot;right shape\u0026quot; # --------------------------------- function objective_initialize!(x, model) #update polynomial coeffficients model.a = copy(x) model.w_func = y -\u0026gt; funeval(model.a, model.fspace, [y])[1][1] return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1])) end minx, iterations, Jac0, fnorm = find_broyden(model.a, x -\u0026gt; objective_initialize!(x, model), max_iter, tol) # Solving the model by collocation # using the initial guess calculated above #----------------------------------------- function objective_residual!(x, model) #update polynomial coeffficients model.a = copy(x) #calculate residual residual!(model) return abs.(model.residual) end minx, iterations, Jac, fnorm = find_broyden(model.a, x -\u0026gt; objective_residual!(x, model), max_iter, tol) end #----------------------------------- # Solve by collocation #----------------------------------- @time solve_optgrowth!(model) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Approximated (collocation)\u0026quot;, linestyle = \u0026quot;--\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;Exact value\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;collocation.png\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](-0.05, 0.05) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;) savefig(\u0026quot;error_collocation.png\u0026quot;)  Output The following figure shows the exact function and the one calculated using the collocation method. In terms of accuracy, both VFI and the collocation method generate reliable results.\nIn terms of speed, it turns out that the value function iteration implementation is much faster. One reason seems to be the efficiency associated with the package Interpolations: it is more than 20 times faster to evaluate $w$ using the package Interpolations rather than using the package CompEcon:\n#using Interpolations #-------------------- @time for i=1:1000000 model.w_func.(model.grid[1]) end #0.230861 seconds (2.00 M allocations: 30.518 MiB, 1.28% gc time) #using CompEcon #-------------- @time for i=1:1000000 model.w_func.(model.grid[1]) end # 4.998902 seconds (51.00 M allocations: 3.546 GiB, 13.39% gc time)  Implementation using ApproxFun Significant speed-gains can be obtained by using the package ApproxFun, as illustrated by the code below. Computing time is divided by approximately $5$ compared to the implementation using CompEcon. Yet, the value function iteration implementation is still the fastest one. One bottleneck seems to be the calculation of the Jacobian by finite differences when using Broyden\u0026rsquo;s method. It is likely that using automatic differentiation would further improve results.\n#--------------------------------------------- # Julien Pascal # Solve the stochastic optimal growth problem # using the collocation method # Implementation using ApproxFun #--------------------------------------------- using QuantEcon using Optim using CompEcon using PyPlot using Interpolations using FileIO using ApproxFun using ProfileView type optGrowthCollocation w::Array{Float64,1} β::AbstractFloat grid::Array{Float64,1} u::Function f::Function shocks::Array{Float64,1} Tw::Array{Float64,1} σ::Array{Float64,1} el_k::Array{Float64,1} wl_k::Array{Float64,1} compute_policy::Bool order_approximation::Int64 #number of element in the functional basis along each dimension functional_basis_type::String #type of functional basis fspace::Dict{Symbol,Any} #functional basis fnodes::Array{Float64,1} #collocation nodes residual::Array{Float64,1} #vector of residual. Should be close to zero a::Array{Float64,1} #polynomial coefficients fApprox::ApproxFun.Fun{ApproxFun.Chebyshev{ApproxFun.Segment{Float64},Float64},Float64,Array{Float64,1}} w_func::Function tol::Float64 end ##################################### # Function that find a solution # to f(x) = 0 # using Broyden's \u0026quot;good\u0026quot; method # and simple backstepping procedure as described # in Miranda and Fackler (2009) # # input : # -------- # * x0: initial guess for the root # * f: function in f(x) = 0 # * maxit: maximum number of iterations # * tol: tolerance level for the zero # * fjavinc: initial inverse of the jacobian. If not provided, then inverse of the # Jacobian is calculated by finite differences # * maxsteps: maximum number of backsteps # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian # # output : # -------- # * x: one zero of f # * it: number of iterations necessary to reached the solution # * fjacinv: pseudo jacobian at the last iteration # * fnorm: norm f(x) at the last iteration # ####################################### function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0)); maxsteps = 5, recaculateJacobian = 1) println(\u0026quot;a0 = $(x0)\u0026quot;) fnorm = tol*2 it2 = 0 #to re-initialize the jacobian ################################ #initialize guess for the matrix ################################ # with Calculus #-------------- fjacinv_function = x-\u0026gt; Calculus.finite_difference_jacobian(f, x) # If the user do not provide an initial guess for the jacobian # One is calculated using finite differences. if fjacinv == eye(length(x0)) ################################################ # finite differences to approximate the Jacobian # at the initial value # this is slow. Seems to improve performances # when x0 is of high dimension. println(\u0026quot;Calculating the Jacobian by finite differences\u0026quot;) #@time fjacinv = Calculus.finite_difference_jacobian(f, x0) @time fjacinv = fjacinv_function(x0) println(\u0026quot;Inverting the Jacobian\u0026quot;) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end println(\u0026quot;Done\u0026quot;) else println(\u0026quot;Using User's input as a guess for the Jacobian.\u0026quot;) end fval = f(x0) for it=1:maxit it2 +=1 #every 30 iterations, reinitilize the jacobian if mod(it2, recaculateJacobian) == 0 println(\u0026quot;Re-calculating the Jacobian\u0026quot;) fjacinv = fjacinv_function(x0) try fjacinv = inv(fjacinv) catch try println(\u0026quot;Jacobian non-invertible\\n calculating pseudo-inverse\u0026quot;) fjacinv = pinv(A) catch println(\u0026quot;Failing Calculating the pseudo-inverse. Initializing with In\u0026quot;) fjacinv = eye(length(x0)) end end end println(\u0026quot;it = $(it)\u0026quot;) fnorm = norm(fval) if fnorm \u0026lt; tol println(\u0026quot;fnorm = $(fnorm)\u0026quot;) return x0, it, fjacinv, fnorm end d = -(fjacinv*fval) fnormold = Inf ######################## # Backstepping procedure ######################## for backstep = 1:maxsteps if backstep \u0026gt; 1 println(\u0026quot;backstep = $(backstep-1)\u0026quot;) end fvalnew = f(x0 + d) fnormnew = norm(fvalnew) if fnormnew \u0026lt; fnorm break end if fnormold \u0026lt; fnormnew d=2*d break end fnormold = fnormnew d = d/2 end #################### #################### x0 = x0 + d fold = fval fval = f(x0) u = fjacinv*(fval - fold) #Update the pseudo Jacobian: fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u)) println(\u0026quot;a$(it) = $(x0)\u0026quot;) println(\u0026quot;fnorm = $(fnorm)\u0026quot;) if isnan.(x0) == trues(length(x0)) println(\u0026quot;Error. a$(it) = NaN for each component\u0026quot;) x0 = zeros(length(x0)) return x0, it, fjacinv, fnorm end end println(\u0026quot;In function find_broyden\\n\u0026quot;) println(\u0026quot;Maximum number of iterations reached.\\n\u0026quot;) println(\u0026quot;No convergence.\u0026quot;) println(\u0026quot;Returning fnorm = NaN as a solution\u0026quot;) fnorm = NaN return x0, maxit, fjacinv, fnorm end function optGrowthCollocation(;w = Array{Float64,1}[], α = 0.4, β = 0.96, μ = 0, s = 0.1, grid_max = 4, # Largest grid point grid_size = 200, # Number of grid points shock_size = 250, # Number of shock draws in Monte Carlo integral Tw = Array{Float64,1}[], σ = Array{Float64,1}[], el_k = Array{Float64,1}[], wl_k = Array{Float64,1}[], compute_policy = true, order_approximation = 40, functional_basis_type = \u0026quot;chebychev\u0026quot;, ) grid_y = collect(linspace(1e-4, grid_max, grid_size)) shocks = exp.(μ + s * randn(shock_size)) # Utility u(c) = log.(c) # Production f(k) = k^α el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2) lower_bound_support = minimum(grid_y) upper_bound_support = maximum(grid_y) n_functional_basis = [order_approximation] if functional_basis_type == \u0026quot;chebychev\u0026quot; fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support) else error(\u0026quot;functional_basis_type has to be \\\u0026quot;chebychev\\\u0026quot; \u0026quot;) end fnodes = funnode(fspace)[1] residual = zeros(size(fnodes)[1]) a = ones(size(fnodes)[1]) tol = 0.001 fApprox = (Fun(Chebyshev((minimum(grid_y))..(maximum(grid_y))), a)) #fApprox = (Fun(Chebyshev(0..maximum(model.grid)), a)) w_func = x-\u0026gt; fApprox(x) w = ones(size(fnodes)[1]) Tw = ones(size(fnodes)[1]) σ = ones(size(fnodes)[1]) optGrowthCollocation( w, β, grid_y, u, f, shocks, Tw, σ, el_k, wl_k, compute_policy, order_approximation, functional_basis_type, fspace, fnodes, residual, a, fApprox, w_func, tol ) end function residual!(model::optGrowthCollocation) model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a)) model.w_func = x-\u0026gt; model.fApprox(x) function objective(c, y) expectation = 0.0 for k = 1:length(model.wl_k) expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k])) end - model.u(c) - model.β * expectation end # Loop over nodes for i in 1:size(model.fnodes)[1] y = model.fnodes[i,1] res = optimize(c -\u0026gt; objective(c, y), 1e-10, y) if model.compute_policy model.σ[i] = Optim.minimizer(res) end model.Tw[i] = - Optim.minimum(res) model.w[i] = model.w_func(y) model.residual[i] = - model.w[i] + model.Tw[i] end end function solve_optgrowth!(model::optGrowthCollocation; tol=1e-6, max_iter=500) # Initialize guess for coefficients # by giving the \u0026quot;right shape\u0026quot; # --------------------------------- function objective_initialize!(x, model) #update polynomial coeffficients model.a = copy(x) model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a)) model.w_func = x-\u0026gt; model.fApprox(x) return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1])) end minx, iterations, Jac0, fnorm = find_broyden(model.a, x -\u0026gt; objective_initialize!(x, model), max_iter, tol) # Solving the model by collocation # using the initial guess calculated above #----------------------------------------- function objective_residual!(x, model) #update polynomial coeffficients model.a = copy(x) #calculate residual residual!(model) return abs.(model.residual) end minx, iterations, Jac, fnorm = find_broyden(model.a, x -\u0026gt; objective_residual!(x, model), max_iter, tol, recaculateJacobian = 1) end #----------------------------------- # Solve by collocation #----------------------------------- model = optGrowthCollocation(functional_basis_type = \u0026quot;chebychev\u0026quot;) @time solve_optgrowth!(model) # 15.865923 seconds (329.12 M allocations: 4.977 GiB, 5.55% gc time) #------------------------------- # Compare with the true solution #------------------------------- α = 0.4 β = 0.96 μ = 0 s = 0.1 c1 = log(1 - α * β) / (1 - β) c2 = (μ + α * log(α * β)) / (1 - α) c3 = 1 / (1 - β) c4 = 1 / (1 - α * β) # True optimal policy c_star(y) = (1 - α * β) * y # True value function v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y) fig, ax = subplots(figsize=(9, 5)) ax[:set_ylim](-35, -24) ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=\u0026quot;approximate value function\u0026quot;) ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=\u0026quot;true value function\u0026quot;) fig, ax = subplots(figsize=(9, 5)) ax[:set_xlim](0.1, 4.0) ax[:set_ylim](- 0.05, 0.05) ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=\u0026quot;error\u0026quot;) ax[:legend](loc=\u0026quot;lower right\u0026quot;)  ","date":1512651029,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512651029,"objectID":"616acd9b09eaac5b85a05cdc2a187df2","permalink":"https://julienpascal.github.io/post/collocation/","publishdate":"2017-12-07T13:50:29+01:00","relpermalink":"/post/collocation/","section":"post","summary":"A large class of economic models involves solving for functional equations of the form:\nA well known example is the stochastic optimal growth model. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period\u0026rsquo;s output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.","tags":[],"title":"Solving Bellman Equations by the Collocation Method","type":"post"},{"authors":["Julien Pascal"],"categories":[],"content":" Dynare is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see Winberry and Ragot for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.\nA simple model The economy is composed of a representative agent who maximizes his expected discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,\u0026hellip;,\\infty$ $$ \\sum_{t=1}^{+\\infty}\\big(\\frac{1}{1+\\rho}\\big)^{t-1} E_t\\Big[log(C_t)-\\frac{L_t^{1+\\gamma}}{1+\\gamma}\\Big] $$\nsubject to the constraint\n$$ K_t = K_t{-_1} (1-\\delta) + w_t L_t + r_t K_t-_1 - C_t $$\nwhere\n $\\rho \\in (0,\\infty)$ is the rate of time preference $\\gamma \\in (0,\\infty)$ is a labor supply parameter $w_t$ is real wage $r_t$ is the real rental rate $K_t$ is capital at the end of the period $\\delta \\in (0,1)$ is the capital depreciation rate  The production function writes \\begin{equation} Y_t = A_t K_t-_1^\\alpha \\Big((1+g)^t \\Big)^{1-\\alpha} \\end{equation}\nwhere\n $g \\in (0,\\infty)$ is the growth rate of production $\\alpha$ and $\\beta$ are technology parameters $A_t$ is a technological shock that follows and $AR(1)$ process  \\begin{equation} \\log(A_t) = \\lambda log(A_t-_1) + e_t\\end{equation}\nwith $e_t$ an i.i.d zero-mean normally distributed error term with standard deviation $\\sigma_1$ and $\\lambda \\in (0,1)$ a parameter governing the persistence of shocks.\nFirst Order conditions The F.O.C.s of the (stationarized) model are\n$$ \\frac{1}{\\hat{C_t}} = \\frac{1}{1+\\rho} E_t \\Big( \\frac{r_t+_1 + 1 - \\delta}{\\hat{C}_t+_1 (1+g)} \\Big)$$\n$$ L_t^\\gamma = \\frac{\\hat{w}_t}{\\hat{C}_t}$$\n$$ r_t = \\alpha A_t \\Big(\\frac{\\hat{K}_t-_1}{1+g}\\Big)^{\\alpha-1}L_t^{1-\\alpha}$$\n$$ \\hat{w}_t = (1-\\alpha) A_t \\Big(\\frac{\\hat{K}_t-_1}{1+g}\\Big)^{\\alpha}L_t^{-\\alpha} $$\n$$ \\hat{K}_t + \\hat{C}_t = \\frac{\\hat{K}_t-_1}{1+g} (1-\\delta) + A_t \\Big( \\frac{\\hat{K}_t-_1}{1+g} \\Big)^{\\alpha} L_t^{1-\\alpha} $$\nwith $$ \\hat{C}_t = \\frac{C_t}{(1+g)^t}$$ $$ \\hat{K}_t = \\frac{K_t}{(1+g)^t}$$ $$ \\hat{w}_t = \\frac{w_t}{(1+g)^t}$$\nSolving and simulating the model in Dynare In Dynare, one has first to specify the endogenous variables (var), exogenous variables (varexo), and the parameters\nvar C K L w r A; varexo e; parameters rho delta gamma alpha lambda g; alpha = 0.33; delta = 0.1; rho = 0.03; lambda = 0.97; gamma = 0; g = 0.015;  In a second step, the F.O.C.s of the model has to be expressed using the command model\nmodel; 1/C=1/(1+rho)*(1/(C(+1)*(1+g)))*(r(+1)+1-delta); L^gamma = w/C; r = alpha*A*(K(-1)/(1+g))^(alpha-1)*L^(1-alpha); w = (1-alpha)*A*(K(-1)/(1+g))^alpha*L^(-alpha); K+C = (K(-1)/(1+g))*(1-delta) +A*(K(-1)/(1+g))^alpha*L^(1-alpha); log(A) = lambda*log(A(-1))+e; end;  The user must provide the analytical solution for the steady state of the model using the command steady_state_model. The command steady solves for the steady state values of the model\nsteady_state_model; A = 1; r = (1+g)*(1+rho)+delta-1; L = ((1-alpha)/(r/alpha-delta-g))*r/alpha; K = (1+g)*(r/alpha)^(1/(alpha-1))*L; C = (1-delta)*K/(1+g) +(K/(1+g))^alpha*L^(1-alpha)-K; w = C; end; steady;  The command shocks defines the type of shock to be simulated\nshocks; var e; stderr 0.01; end; check;  A first order expansion around the steady state is obtained using the command stoch_simul(order=1) This function computes impulse response functions (IRF) and returns various descriptive statistics (moments, variance decomposition, correlation and autocorrelation coefficients)\nThe IRF produced by Dynare should be pretty similar to the following graph: ","date":1501332263,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501332263,"objectID":"d5946fc40e4b8cec00c21670c8ac01d1","permalink":"https://julienpascal.github.io/post/rbc_dynare/","publishdate":"2017-07-29T13:44:23+01:00","relpermalink":"/post/rbc_dynare/","section":"post","summary":"Dynare is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see Winberry and Ragot for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.\nA simple model The economy is composed of a representative agent who maximizes his expected discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,\u0026hellip;,\\infty$ $$ \\sum_{t=1}^{+\\infty}\\big(\\frac{1}{1+\\rho}\\big)^{t-1} E_t\\Big[log(C_t)-\\frac{L_t^{1+\\gamma}}{1+\\gamma}\\Big] $$","tags":[],"title":"Solving a simple RBC model in Dynare","type":"post"},{"authors":["Christian Daude","Julien Pascal"],"categories":null,"content":"","date":1488898314,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488898314,"objectID":"48aece6ec07e069260731c2bbb63b8e5","permalink":"https://julienpascal.github.io/publication/efficiencycontestability/","publishdate":"2017-03-07T15:51:54+01:00","relpermalink":"/publication/efficiencycontestability/","section":"publication","summary":"This paper explores some of the potential determinants of efficiency and contestability in the banking systems of major emerging countries, using a sample of 24 countries over the period 2004 -2013. Efficiency is estimated using both stochastic frontier and data envelopment analyses. Market contestability is measured with the Panzar-Rosse H-statistic. Potential drivers of efficiency and market contestability are then discussed.","tags":["Banking","Efficiency","Competition","Emerging markets"],"title":"Efficiency and contestability in emerging market banking systems","type":"publication"},{"authors":null,"categories":null,"content":" Introduction  Which neighborhoods to visit when in Paris? Where to live in Paris?  Answers depend on your preferences\nThis project  Cluster neighborhoods in Paris and analyze cluster properties  Data and Methodology Data Sources  Foursquare: geolocalized data on venues Wikipedia: data on neighborhoods (called \u0026ldquo;arrondissements\u0026rdquo;)  Methodology  Similarity based on venue types (restaurants, cafés, hotels, etc.) k-means clustering using scikit-learn  Results Five clusters identified Five clusters identified  cluster 1 (in red): 5th, 7th, 8th, 14th, 15th, 16th and 17th arrondissements cluster 2 (in purple): 12th arrondissement cluster 3 (in blue): 1st, 2nd, 3rd, 4th, 6th, 9th and 10th cluster 4 (in light green): 11th, 18th, 19th, and 20th cluster 5 (in orange): 13th arrondissement  Cluster properties Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Suggestions based on preferences  cluster 1: for a “French” experience, in a mostly quiet and residential area cluster 2: for fresh air and sport (near the Bois de Vincennes) cluster 3: a mix of art, museums and diverse cuisine cluster 4: for the best bars and pizza experience in town cluster 5: Asian cuisine aficionados  Conclusion  Which neighborhoods to visit when in Paris? Where to live in Paris?\n k-means clustering combined with Fousquare API generates new insights\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d5665609e9967856be1aaea60f3bcdb9","permalink":"https://julienpascal.github.io/slides/neighbourhoods_paris/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/neighbourhoods_paris/","section":"slides","summary":"Introduction  Which neighborhoods to visit when in Paris? Where to live in Paris?  Answers depend on your preferences\nThis project  Cluster neighborhoods in Paris and analyze cluster properties  Data and Methodology Data Sources  Foursquare: geolocalized data on venues Wikipedia: data on neighborhoods (called \u0026ldquo;arrondissements\u0026rdquo;)  Methodology  Similarity based on venue types (restaurants, cafés, hotels, etc.) k-means clustering using scikit-learn  Results Five clusters identified Five clusters identified  cluster 1 (in red): 5th, 7th, 8th, 14th, 15th, 16th and 17th arrondissements cluster 2 (in purple): 12th arrondissement cluster 3 (in blue): 1st, 2nd, 3rd, 4th, 6th, 9th and 10th cluster 4 (in light green): 11th, 18th, 19th, and 20th cluster 5 (in orange): 13th arrondissement  Cluster properties Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Suggestions based on preferences  cluster 1: for a “French” experience, in a mostly quiet and residential area cluster 2: for fresh air and sport (near the Bois de Vincennes) cluster 3: a mix of art, museums and diverse cuisine cluster 4: for the best bars and pizza experience in town cluster 5: Asian cuisine aficionados  Conclusion  Which neighborhoods to visit when in Paris?","tags":null,"title":"Capstone Project - The Battle of Neighborhoods","type":"slides"},{"authors":null,"categories":null,"content":" Rental Housing Market and Directed Search Introduction Context  Housing shortages in many major cities Rents have surged in big cities worldwide   The median American rent payment rose 61% in real terms between 1960 and 2016 while the median tenant’s income grew by 5%\n Rents have outpaced household disposable income in Paris Questions  Do we observe asymmetric bargaining power between tenants and landlords?  Data Data Sources  Web scraping: online platform that collects online ads for the Paris market between April and May 2019 I have data on the two sides of the rental housing market:  supply: apartment features, text description and photos demand: number of contacts received by landlords (through the online platform)   Descriptive statistics Ads per city Apartment features Apartment features by city Population (2017) Number of ads Median rent per m² Aesthetic score  Aesthetic score calculated based on photos I use a convolutional neural network (CNN) based on the work of Talebi and Milanfar (2018) CNN assigns a score to each photo Ad aesthetic score = median score  Number of photos per ad Density aesthetic score Selected sample of photos in the top 1% Selected sample of photos in the bottom 1% What features are important to predict rent? Linear model $$y_{i} = \\alpha + \\boldsymbol{x_{i}}^{\u0026lsquo;} \\boldsymbol{\\beta} + \\varepsilon_{i}$$\n $\\boldsymbol{y_{i}}$: rent (in €) $\\boldsymbol{x_{i}}$: apartment features $\\alpha$: constant $\\varepsilon_{i}$: error term  Rent and apartment characteristics Residual price dispersion and number of contacts residual price dispersion = predicted price - actual price\n residual price dispersion \u0026lt; 0: More expensive than expected residual price dispersion \u0026gt; 0: Cheaper than expected  Number of contacts per ad  I have data on the number of contacts per ad (through the online platform) Data is truncated: # of contacts observed only when number of contacts $\\geq$ 10  Percentage of ads with more than 10 contacts Number of contacts per ad Deviation from predicted price and number of contacts per ad Truncated regression  Interpretation: \u0026downarrow; monthly rent by 1 euro \u0026rightarrow; number contacts \u0026uparrow; by approximately 1.8%.  Rent movements Rent decreases  I have data on rent decreases 7% of ads in the sample decreased their advertised rent Common strategy for landlords:  set a price above the market price decrease to market price after a \u0026ldquo;wait and see\u0026rdquo; period   Rent decreases Days before a rent decrease T-tests  discounted ads were:  more likely to be overpriced (column 1) received less contacts (column 2) more likely to be managed by real estate agencies (column 3)   Summary  Cheaper (than expected) apartments attract more tenants A simple directed search model predicts this pattern  Work in progress  Landlords\u0026rsquo; pricing strategy: learning or \u0026ldquo;excess\u0026rdquo; bargaining power?  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a27097b0096c805fd7c11d95ac06566","permalink":"https://julienpascal.github.io/slides/rental-market/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/rental-market/","section":"slides","summary":"Rental Housing Market and Directed Search Introduction Context  Housing shortages in many major cities Rents have surged in big cities worldwide   The median American rent payment rose 61% in real terms between 1960 and 2016 while the median tenant’s income grew by 5%\n Rents have outpaced household disposable income in Paris Questions  Do we observe asymmetric bargaining power between tenants and landlords?  Data Data Sources  Web scraping: online platform that collects online ads for the Paris market between April and May 2019 I have data on the two sides of the rental housing market:  supply: apartment features, text description and photos demand: number of contacts received by landlords (through the online platform)   Descriptive statistics Ads per city Apartment features Apartment features by city Population (2017) Number of ads Median rent per m² Aesthetic score  Aesthetic score calculated based on photos I use a convolutional neural network (CNN) based on the work of Talebi and Milanfar (2018) CNN assigns a score to each photo Ad aesthetic score = median score  Number of photos per ad Density aesthetic score Selected sample of photos in the top 1% Selected sample of photos in the bottom 1% What features are important to predict rent?","tags":null,"title":"Rental Housing Market and Directed Search","type":"slides"},{"authors":null,"categories":null,"content":" Spatial Equilibrium and Commuting Costs Introduction  The \u0026ldquo;Forfait Toutes Zones\u0026rdquo; (FTZ) entered into force on September 2015 in the Paris area The costs of using public transports decreased for workers living in the outskirts of Paris (zones 3-5) Using the spatial discontinuity generated by the reform, I analyze the links between commuting costs and local employment outcomes  Context: Grand Paris Express source: https://www.societedugrandparis.fr/gpe/carte#lignes\nContext: Grand Paris Express source: https://www.societedugrandparis.fr/emploi\nData Data Sources  Monthly # of workers registered to Pole Emploi at the municipality level: Pole Emploi Demographic and economic characteristics of municipalities and IRIS: INSEE House price index at the municipality level: Chambre des Notaires de Paris Fare area at the IRIS level: Vianavigo Behavior of travelers on the public transport network: STIF  Cost change by fare area Average cost difference by fare area Fare area at the IRIS level Fare area at the municipality level Sample selection Sample selection Econometrics Main specification $$y_{it} = \\alpha_i + \\gamma_t + \\beta \\times \\delta_{i,t} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t}$$\n $\\alpha_i$: municipality fixed effects $\\gamma_t$: time fixed effects $\\delta_{i,t}$: indicator = 1 if treated and post-reform $\\boldsymbol{x_{i,t}}$: control variables $\\varepsilon_{i,t}$: error term  Other spefications Time-varying impacts $$ y_{it} = \\alpha_i + \\gamma_t + \\sum_{y=2015}^{2019} \\beta_{y} \\times \\delta_{i,t} \\times \\delta_{y} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t} $$\nDifferent \u0026ldquo;intensity of treatment\u0026rdquo; $$ y_{it} = \\alpha_i + \\gamma_t + \\beta \\times z_{i,t} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t} $$\n$$ y_{it} = \\alpha_i + \\gamma_t + \\sum_{y=2015}^{2019} \\beta_{y} \\times z_{i,t} \\times \\delta_{y} + \\boldsymbol{\\eta} \\boldsymbol{x_{i,t}}^{\u0026lsquo;} + \\varepsilon_{i,t} $$\nSpatial RDD  For results to be valid, the common trend assumption needs to hold To limit the impact of unobservable characteristics on local employment outcomes, I select municipalities close to the border separating fare areas (zones 2 and 3 in the baseline)  Results Impact of the FTZ on the total # of workers registered to Pole Emploi Impact of the FTZ on the # of workers registered to Pole Emploi in category A Impact on public transport usage Model Overview  Partial equilibrium model inspired by Brueckner et al. (1999), Postel-Vinay and Robin (2002), Wasmer and Zenou (2002) The model helps in understanding why a decrease in commuting costs may increase local employment  Model Main equation Impact of the FTZ reform in municipality $l$ on:\n job search efficiency the reservation wage incentives to relocate  Conclusion  The \u0026ldquo;Forfait Toutes Zones\u0026rdquo; (FTZ) decreased commuting costs for workers living in the outskirts of Paris (zones 3-5) Using the spatial discontinuity generated by the reform, I find that the FTZ created jobs for long-term unemployed workers in zones 3-5  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1805de5ca616d11e2bd305f0ac551778","permalink":"https://julienpascal.github.io/slides/spatial-eq/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/spatial-eq/","section":"slides","summary":"Spatial Equilibrium and Commuting Costs Introduction  The \u0026ldquo;Forfait Toutes Zones\u0026rdquo; (FTZ) entered into force on September 2015 in the Paris area The costs of using public transports decreased for workers living in the outskirts of Paris (zones 3-5) Using the spatial discontinuity generated by the reform, I analyze the links between commuting costs and local employment outcomes  Context: Grand Paris Express source: https://www.societedugrandparis.fr/gpe/carte#lignes\nContext: Grand Paris Express source: https://www.","tags":null,"title":"Spatial Equilibrium and Commuting Costs","type":"slides"}]