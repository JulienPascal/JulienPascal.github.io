<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.1.0">
  <meta name="generator" content="Hugo 0.54.0" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Julien Pascal, PhD">

  
  
  
    
  
  <meta name="description" content="Introduction In this blog post, I am going to present my recent work on the bias-corrected Monte Carlo operator, or more compactly &ldquo;bc-MC operator&rdquo;, which was recently published here in the JEDC. In this paper, I propose a new methodology to combine Monte Carlo and neural networks to solve large scale economic models. In this blog post, my goal is to give an intuitive description of this method.
Theory Structure of economic models In many cases, solving an economic model involves finding a policy function $g(.">

  
  <link rel="alternate" hreflang="en-us" href="https://julienpascal.github.io/post/bc_mc/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BSZS1F0NHL"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-BSZS1F0NHL');
    </script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://julienpascal.github.io/index.xml" type="application/rss+xml" title="Julien Pascal">
  <link rel="feed" href="https://julienpascal.github.io/index.xml" type="application/rss+xml" title="Julien Pascal">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://julienpascal.github.io/post/bc_mc/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@Juli3nPascal">
  <meta property="twitter:creator" content="@Juli3nPascal">
  
  <meta property="og:site_name" content="Julien Pascal">
  <meta property="og:url" content="https://julienpascal.github.io/post/bc_mc/">
  <meta property="og:title" content="The bias-corrected Monte Carlo operator | Julien Pascal">
  <meta property="og:description" content="Introduction In this blog post, I am going to present my recent work on the bias-corrected Monte Carlo operator, or more compactly &ldquo;bc-MC operator&rdquo;, which was recently published here in the JEDC. In this paper, I propose a new methodology to combine Monte Carlo and neural networks to solve large scale economic models. In this blog post, my goal is to give an intuitive description of this method.
Theory Structure of economic models In many cases, solving an economic model involves finding a policy function $g(."><meta property="og:image" content="https://julienpascal.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2024-05-01T08:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2024-05-01T08:00:00&#43;01:00">
  

  

  

  <title>The bias-corrected Monte Carlo operator | Julien Pascal</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Julien Pascal</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#computing">
            
            <span>Computing</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/pdf/Julien_Pascal_Academic_Resume.pdf">
            
            <span>Academic Resume</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">The bias-corrected Monte Carlo operator</h1>

  

  
    



<meta content="2024-05-01 08:00:00 &#43;0100 &#43;0100" itemprop="datePublished">
<meta content="2024-05-01 08:00:00 &#43;0100 &#43;0100" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>May 1, 2024</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=The%20bias-corrected%20Monte%20Carlo%20operator&amp;url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fbc_mc%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fjulienpascal.github.io%2fpost%2fbc_mc%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fbc_mc%2f&amp;title=The%20bias-corrected%20Monte%20Carlo%20operator"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fbc_mc%2f&amp;title=The%20bias-corrected%20Monte%20Carlo%20operator"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=The%20bias-corrected%20Monte%20Carlo%20operator&amp;body=https%3a%2f%2fjulienpascal.github.io%2fpost%2fbc_mc%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    







  









  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<h2 id="introduction">Introduction</h2>

<p>In this blog post, I am going to present my recent work on the <strong>bias-corrected Monte Carlo</strong> operator, or more compactly &ldquo;bc-MC operator&rdquo;, which was recently published <a href="https://www.sciencedirect.com/science/article/abs/pii/S0165188924000459" target="_blank">here</a> in the JEDC. In this paper, I propose a new methodology to combine <strong>Monte Carlo</strong> and <strong>neural networks</strong> to solve large scale economic models. In this blog post, my goal is to give an intuitive description of this method.</p>

<h2 id="theory">Theory</h2>

<h3 id="structure-of-economic-models">Structure of economic models</h3>

<p>In many cases, solving an economic model involves finding a policy function $g(.)$, which is a mapping from a state vector $s$ to a decision vector $a$. Usually, it is assumed that the economy is hit by random disturbances, which is usually modelled as zero-mean innovation vector $\varepsilon$, often considered to be normally distributed. The policy function $g(.)$ is such that the following stochastic equation holds:</p>

<p>$$ E_{\varepsilon}(f(s, \varepsilon)) = 0 $$</p>

<p>where $f(.)$ depends implicitly on the policy function $g(.)$ and on the particularities of the model under consideration. It looks a bit abstract, but we can usually write economic models in that form. For instance, in macroeconomic models, you usually have an Euler equation that can be written as such.</p>

<p>Here, let&rsquo;s assume that we restrict ourselves to policy functions that depend on a parameter vector $\theta$: $g(. |\theta)$. Further, let&rsquo;s assume that we use a multilayer perceptron for the policy function, so that the elements of $\theta$ are actually the weights and biases of that specific neural network. Now, the problem is in many ways much simpler, because it consists in finding a finite-size vector $\theta$, rather than finding a function, which is an infinite-dimensional object:</p>

<p>$$ E_{\varepsilon}(f(s, \varepsilon | \theta )) = 0 $$</p>

<p>We want the equation above to be true for all values of $s$. The common way to proceed is to define a grid for $s$ and to find $g(. |\theta)$ such that the previous equation holds on these grid points. For off-grid point values, interpolation schemes can be used. This is fine when $s$ is low-dimensional, but if $s$ is high-dimensional, the curse of dimensionality is going to bite: the number of grid points grows exponentially with the dimension of $s$, not linearly. The trick here is treat $s$ as a random variable, living within an ergodic set $S$ and to solve the following problem:</p>

<p>$$ E_{s} \Big( \Big[ E_{\varepsilon}(f(s, \varepsilon | \theta )) \Big]^2 \Big) = 0 $$</p>

<p>This probabilistic formulation works well even if $s$ is high-dimensional, because we have numerical methods that can handle high-dimensional expectations. In particular, we have Monte Carlo integration, which consists in approximating expectations using sample means.</p>

<p>Now define $L(\theta)$ as the left-hand side of the previous equation:</p>

<p>$$ L(\theta) = E_{s} \Big( \Big[ E_{\varepsilon}(f(s, \varepsilon | \theta )) \Big]^2 \Big) $$</p>

<p>One way to find $\theta$ is to use the gradient descent algorithm, which is an iterative method. We start with a random guess for $\theta$, denoted $\theta_{i}$, and we update the guess using the information contained in the gradient of $L$ evaluated at $\theta_i$ denoted by $\nabla_{\theta} L(\theta_i)$, which is the direction of the steepest ascent. Because we are minimizing a function, we take a small step $\gamma$ in the opposite direction of the gradient:</p>

<p>$$ \theta_{i+1} = \theta_{i} - \gamma \nabla_{\theta} L(\theta_{i}) $$</p>

<p>Below is an example of gradient descent for the function $x^2 + 2 y^2$. As expected, we iteratively converge to the minimum of this simple function, at $(0,0)$.</p>

<pre><code class="language-python"># Illustration of gradient descent using Python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import Image

# Define the function and its gradient
def f(x, y, a=1, b=2):
    return a*x**2 + b*y**2

def grad_f(x, y, a=1, b=2):
    return a*2*x, b*2*y

# Parameters
alpha = 0.1  # Learning rate
n_iterations = 25 #Number of steps

# Initial point
x, y = -10, -8
trajectory = []

# Gradient Descent Algorithm
for _ in range(n_iterations):
    trajectory.append((x, y))
    dx, dy = grad_f(x, y)
    x -= alpha * dx
    y -= alpha * dy

# Setting up the visualization
x_vals = np.linspace(-10, 10, 400)
y_vals = np.linspace(-10, 10, 400)
X, Y = np.meshgrid(x_vals, y_vals)
Z = f(X, Y)

fig, ax = plt.subplots(figsize=(8, 8))
CS = ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis')
ax.plot(0, 0, 'ro')  # Mark the minimum

# Animation function
def update(frame):
    ax.cla()
    ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis')
    ax.plot(0, 0, 'ro')  # Minimum
    # Current point and arrow
    x, y = trajectory[frame]
    dx, dy = grad_f(x, y)
    # Direction of update
    ax.arrow(x, y, -alpha * dx, -alpha * dy, head_width=0.5, head_length=0.5, fc='k', ec='k')
    ax.set_xlim([-10, 10])
    ax.set_ylim([-10, 10])
    ax.set_title(f&quot;Gradient Descent Algorithm for $x^2 + 2y^2$. Iteration {frame + 1}&quot;)

ani = FuncAnimation(fig, update, frames=n_iterations, repeat=False)
ani.save('gradient_descent.gif', writer='imagemagick', fps=1)
plt.close()

Image(url='gradient_descent.gif')  
</code></pre>

<p><img src="gradient_descent.gif"/></p>

<h3 id="the-bc-mc-operator">The bc-MC operator</h3>

<p>As it is now clear from the previous visual illustration, knowledge of the gradient of the loss function is enough to find numerically a solution of our economic model, at least a local one (for the gradient descent algorithm to converge to a global minimum, we usually need the function to be convex over its domain).</p>

<p>One difficulty here is that we cannot compute the expectations appearing in $L(\theta) = E_{s} \Big( \Big[ E_{\varepsilon}(f(s, \varepsilon | \theta )) \Big]^2 \Big)$. However, as suggested before, we can use Monte Carlo integration, which consists in using sample means to approximate expectations:</p>

<p>$$ L^{B}_{M,N}(\theta) = \frac{1}{M} \sum_{m=1}^{M}\Big[ \frac{1}{N} \sum_{n=1}^{N} f(s_m, \varepsilon_n | \theta ) \Big]^2 $$</p>

<p>Clearly, this is an <a href="https://en.wikipedia.org/wiki/Estimator" target="_blank">estimator</a> of $L(\theta)$ that makes a lot of sense. However, because of some unpleasant arithmetic regarding the <a href="https://journals.sagepub.com/doi/abs/10.1177/0008068319750115" target="_blank">estimation the square of the mean</a>, this is a biased one.</p>

<p>Start with the formula for the variance $Var(X) = E(X^2) - E(X)^2 \Leftrightarrow E(X^2) = E(X)^2 + Var(X)$. Using this formula</p>

<p>$$ E_{\varepsilon}\Big[ \Big(\frac{1}{N} \sum_{n=1}^{N} f(s_m, \varepsilon_n | \theta ) \Big)^2 \Big] = \Big( E_{\varepsilon}\Big[ \frac{1}{N} \sum_{n=1}^{N} f(s_m, \varepsilon_n | \theta ) \Big] \Big)^2 + Var_{\varepsilon} \Big( \frac{1}{N} \sum_{n=1}^{N} f(s_m, \varepsilon_n | \theta ) \Big) $$</p>

<p>Using the linearity of the expectation operator, and the fact that $Var(a X) = a^2 V(X)$, one gets:</p>

<p>$$ E_{\varepsilon}\Big[ \Big(\frac{1}{N} \sum_{n=1}^{N} f(s_m, \varepsilon_n | \theta ) \Big)^2 \Big] = \mu_{s_m}^2 + \frac{\sigma_{s_m}^2}{N} $$</p>

<p>In words, one gets the square of the mean, plus a bias term equal to the variance of $f$ (conditional on $s = s_m$) divided by $N$. Fortunately for us, there exists an unbiased estimator of the variance, given by the sample variance. An easy way to neutralize the bias is to subtract the sample variance (divided by N):</p>

<p>$$ L^{U}_{M,N}(\theta) = \frac{1}{M} \sum_{m=1}^{M }\Big( \Big[ \frac{1}{N}  \sum_{n=1}^{N} f(s_m, \varepsilon_n | \theta ) \Big]^2
 - \frac{S_m^2}{N} \Big) $$</p>

<p>The term &ldquo;bc-MC operator&rdquo; comes from bias correction.</p>

<p>The previous expression can be rewritten as:</p>

<p>$$ L^{U}_{M,N}(\theta) = \frac{2}{MN(N-1)} \sum_{m=1}^{M} \sum_{1 \leq i &lt; j}^{N} f(s_m, \varepsilon^{i}_m | \theta ) f(s_m, \varepsilon^{j}_m | \theta ) $$</p>

<p>where $\varepsilon^{i}$ and $\varepsilon^{j}$ are i.i.d. shocks with the same distribution as $\varepsilon$. This expression is more convenient because it can be written as a sparse matrix multiplication:</p>

<p>$$ L^{U}_{M,N}(\theta) = \frac{2}{MN(N-1)} f&rsquo; \Lambda f $$</p>

<p>where $\Lambda$ is a sparse matrix. More specifically</p>

<ul>
<li>$f$ is MN column vector such that $f&rsquo; = (f(s_1, \varepsilon^{1}_1), &hellip;, f(s_1, \varepsilon^{N}_1), f(s_2, \varepsilon^{1}_2), &hellip;, f(s_2, \varepsilon^{N}_2), &hellip;, f(s_M, \varepsilon^{N}_M))$</li>
<li>$\Lambda = I_{M} \otimes U_{N}$ is $MN \times MN$ sparse matrix, with $ I_{M}$ the $M \times M$ identity matrix and $U_{N}$ a $N \times N$ upper triangular matrix whose diagonal elements are equal to zero, while the entries above the main diagonal are all equal to one. The symbol $\otimes$ denotes the Kronecker product.</li>
</ul>

<p>This expression can be very efficiently calculated using Python and Pytorch, with no loops involved.</p>

<h3 id="minimum-variance-unbiased-estimator">Minimum Variance Unbiased Estimator</h3>

<p>Actually, under certain conditions on the economic model under consideration, I show that $L^{U}_{M,N}(\theta)$ it the Minimum Variance Unbiased Estimator (MVUE) of the loss function $L(\theta)$. That is, if one is willing to only consider unbiased estimators of $L(\theta)$, then $L^{U}_{M,N}(\theta)$ is the one with the lowest variance. In the paper, I use the <a href="https://en.wikipedia.org/wiki/Lehmann%E2%80%93Scheff%C3%A9_theorem" target="_blank">Lehmann-Scheff√© theorem</a>. This theorem states that any estimator that is unbiased for a given unknown quantity and that depends on the data only through a complete, sufficient statistic is the unique best unbiased estimator of that quantity.</p>

<p>But there is another simpler proof, if one is willing to only consider estimators that <strong>linearly</strong> combine estimators of the square of means (conditional on $s=s_m$), denoted by $E_m$</p>

<p>$$ L_{a} = \sum_{m=1}^{M} a_m E_m $$</p>

<p>It is easy to show that for $L_{a}$ to be the MVUE of $L(\theta)$, its must be a linear combination of MVUEs for the square of means, denoted by $\tilde{E}_m$</p>

<p>$$ L_{a} = \sum_{m=1}^{M} a_m \tilde{E}_m $$</p>

<p>The proof relies on the <a href="https://pages.stat.wisc.edu/~shao/stat610/stat610-04.pdf" target="_blank">following lemma</a>: &ldquo;An unbiased estimator $T(X)$ of $h(\theta)$ is UMVUE iff $T(X)$ is uncorrelated with all unbiased estimators of 0&rdquo;.</p>

<p>It is clear here that taking unbiased estimators for $E_m$ will result in $L_{a}$ being unbiased as well, by linearity of the expectation operator. Then, we can consider an unbiased estimator of $0$, denoted by $U$:</p>

<p>$$ Cov(\sum_{m=1}^{M} a_m \tilde{E}_m, U) =  \sum_{m=1}^{M} a_m cov(\tilde{E}_m, U) = 0 $$</p>

<p>where $cov(\tilde{E}_m, U) = 0$, because each $\tilde{E}_m$ is itself a MVUE by assumption. We still have to determine the optimal values for the $a_m$, under the constraint that they all sum up to one. Now, as long as the estimators $\tilde{E}_m$ have equal variance, the optimal way to select the coefficient $a_m$ is to use equal weighting: $a_m = \frac{1}{M}$ (otherwise, one may use <a href="https://en.wikipedia.org/wiki/Inverse-variance_weighting" target="_blank">inverse-variance weighting</a>, as we do with weighted least squares):</p>

<p>$$ L_{a} = \frac{1}{M} \sum_{m=1}^{M} \tilde{E}_m $$</p>

<p>For an i.i.d sample normally distributed with finite variance, the <a href="https://journals.sagepub.com/doi/abs/10.1177/0008068319750115" target="_blank">MVUE of the square of the mean</a> is the square of the empirical mean, minus the sample variance divided by the sample size. Said differently, we are back to the expression for the bc-MC operator, exactly as before:</p>

<p>$$ L_{a} =  \frac{1}{M} \sum_{m=1}^{M }\Big( \Big[ \frac{1}{N}  \sum_{n=1}^{N} f(s_m, \varepsilon_n | \theta ) \Big]^2
 - \frac{S_m^2}{N} \Big) $$</p>

<h3 id="choice-of-the-hyperparameters-m-and-n">Choice of the hyperparameters $M$ and $N$</h3>

<p>One question that remains open is the optimal choice of the hyperparameters $M$ and $N$. In an ideal world in which computations are costless, we would like to take $M$ and $N$ to infinity. But in practice, the bc-MC estimator involves evaluating $f(s_m, \varepsilon_n)$ MN times, before combining these values using the formula $\frac{2}{MN(N-1)} f&rsquo; \Lambda f $. A natural starting point for a &ldquo;computational budget&rdquo; consists in considering a multiple of $MN$, even if more sophisticated techniques could be used.</p>

<p>If you do that, you will find the variance of the loss (for a given $\theta$) can be written as $\frac{1}{MN} \times h(M, N, \theta) $, where $h(M, N, \theta)$ can be estimated using simulated data from the model (propositions 4-6 in the paper). Said differently, for a given &ldquo;computational budget&rdquo;, we can find the best combination of $M$ and $N$ such that $h(M, N, \theta)$ is minimized. We can train the neural network (update the value for $\theta$), while simultaneously changing the values for $M$ and $N$.</p>

<p>Note that if $N=2$ all the time, then we have</p>

<p>$$ L^{U}_{M,N}(\theta) = \frac{1}{M} \sum_{m=1}^{M} f(s_m, \varepsilon^{1}_m | \theta ) f(s_m, \varepsilon^{2}_m | \theta ) $$</p>

<p>which is an estimator that was already suggested in <a href="https://www.sciencedirect.com/science/article/abs/pii/S0304393221000799" target="_blank">this paper</a>.</p>

<h3 id="review-of-applications">Review of applications</h3>

<p>In the paper, I start with the really simple <a href="https://github.com/JulienPascal/bc-MC_Operator/tree/main/6.Brock_Mirman_Colab" target="_blank">Brock-Mirman Stochastic Growth Model from 1972</a>. For this model, we get a closed-form solution for the policy function and we can very sharply characterize the best choice for the hyperparameters $M$ and $N$. So it is good warm-up.</p>

<p>A more interesting application is <a href="https://github.com/JulienPascal/bc-MC_Operator/blob/main/9.large_scale_model_2/bc-MC-consumption-savings_large_scale_1.ipynb" target="_blank">consumption-savings model with a borrowing constraint</a>.</p>

<p>But you can also apply this methodology to <a href="https://github.com/JulienPascal/bc-MC_Operator/blob/main/10.OLG_model/OLG.ipynb" target="_blank">OLG models</a> with a large number of different age groups.</p>

<p>Probably the next step would be to use the methodology for a Krusell-Smith model.</p>

<h2 id="references">References</h2>

<ul>
<li>Julien Pascal, Artificial neural networks to solve dynamic programming problems: A bias-corrected Monte Carlo operator,
Journal of Economic Dynamics and Control, Volume 162, 2024, 104853, ISSN 0165-1889,
<a href="https://doi.org/10.1016/j.jedc.2024.104853" target="_blank">https://doi.org/10.1016/j.jedc.2024.104853</a>. (<a href="https://www.sciencedirect.com/science/article/pii/S0165188924000459" target="_blank">https://www.sciencedirect.com/science/article/pii/S0165188924000459</a>)</li>
</ul>

<p>To cite this work:</p>

<pre><code>@article{PASCAL2024104853,
title = {Artificial neural networks to solve dynamic programming problems: A bias-corrected Monte Carlo operator},
journal = {Journal of Economic Dynamics and Control},
volume = {162},
pages = {104853},
year = {2024},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2024.104853},
url = {https://www.sciencedirect.com/science/article/pii/S0165188924000459},
author = {Julien Pascal}
}
</code></pre>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/neural-networks/">Neural Networks</a>
  
  <a class="badge badge-light" href="/tags/monte-carlo/">Monte Carlo</a>
  
</div>




    
      






  







<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  
  <img class="portrait mr-3" src="/author/admin/avatar_hua874ac19f86782cda4b3a3191cd90706_32633_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
  

  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/authors/admin">Julien Pascal, PhD</a></h5>
    <h6 class="card-subtitle">Economist</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/Juli3nPascal" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://scholar.google.com/citations?user=GLpf1WoAAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="noopener">
          <i class="ai ai-google-scholar"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/JulienPascal" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://www.linkedin.com/in/julien-pascal-62a322aa/" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://medium.com/@julien.pascal" target="_blank" rel="noopener">
          <i class="fab fa-medium"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/publication/bc-mc-2024/">Artificial Neural Networks to solve dynamic programming problems: a bias-corrected Monte Carlo operator</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2017 Julien Pascal &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/julia.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/shell.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d7381f2d79e6271d4da28f474f49096c.js"></script>

  </body>
</html>

