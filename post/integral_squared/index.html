<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.1.0">
  <meta name="generator" content="Hugo 0.54.0" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Julien Pascal, PhD">

  
  
  
    
  
  <meta name="description" content="Introduction Recently, I came across a simple question that led me through a quite interesting rabbit hole. The question is very simple (and very niche), but the analysis involves key results from statistical theory and numerical analysis (the Central Limit Theorem, the Delta method, and the second-order Delta method, Monte Carlo integration).
Here is the seemingly innocent (and slightly bizarre) question:
 Context: One is interested in evaluating the square of an expectation $E\Big[g(X) \Big]^2 = \big(\int_{a}^{b} g(x) f_X(x) dx \Big)^2 = \mu^2$, where $\mu$ is an unknown quantity &ldquo;close to zero&rdquo;.">

  
  <link rel="alternate" hreflang="en-us" href="https://julienpascal.github.io/post/integral_squared/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-114454001-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://julienpascal.github.io/index.xml" type="application/rss+xml" title="Julien Pascal">
  <link rel="feed" href="https://julienpascal.github.io/index.xml" type="application/rss+xml" title="Julien Pascal">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://julienpascal.github.io/post/integral_squared/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@Juli3nPascal">
  <meta property="twitter:creator" content="@Juli3nPascal">
  
  <meta property="og:site_name" content="Julien Pascal">
  <meta property="og:url" content="https://julienpascal.github.io/post/integral_squared/">
  <meta property="og:title" content="Numerical Approximation of Expectations Squared | Julien Pascal">
  <meta property="og:description" content="Introduction Recently, I came across a simple question that led me through a quite interesting rabbit hole. The question is very simple (and very niche), but the analysis involves key results from statistical theory and numerical analysis (the Central Limit Theorem, the Delta method, and the second-order Delta method, Monte Carlo integration).
Here is the seemingly innocent (and slightly bizarre) question:
 Context: One is interested in evaluating the square of an expectation $E\Big[g(X) \Big]^2 = \big(\int_{a}^{b} g(x) f_X(x) dx \Big)^2 = \mu^2$, where $\mu$ is an unknown quantity &ldquo;close to zero&rdquo;."><meta property="og:image" content="https://julienpascal.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-09-03T18:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2022-09-03T18:00:00&#43;01:00">
  

  

  

  <title>Numerical Approximation of Expectations Squared | Julien Pascal</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Julien Pascal</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#computing">
            
            <span>Computing</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/pdf/Julien_Pascal_Academic_Resume.pdf">
            
            <span>Academic Resume</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Numerical Approximation of Expectations Squared</h1>

  

  
    



<meta content="2022-09-03 18:00:00 &#43;0100 &#43;0100" itemprop="datePublished">
<meta content="2022-09-03 18:00:00 &#43;0100 &#43;0100" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Sep 3, 2022</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    16 min read
  </span>
  

  
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Numerical%20Approximation%20of%20Expectations%20Squared&amp;url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fintegral_squared%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fjulienpascal.github.io%2fpost%2fintegral_squared%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fintegral_squared%2f&amp;title=Numerical%20Approximation%20of%20Expectations%20Squared"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fintegral_squared%2f&amp;title=Numerical%20Approximation%20of%20Expectations%20Squared"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Numerical%20Approximation%20of%20Expectations%20Squared&amp;body=https%3a%2f%2fjulienpascal.github.io%2fpost%2fintegral_squared%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    







  









  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<h2 id="introduction">Introduction</h2>

<p>Recently, I came across a simple question that led me through a quite interesting rabbit hole. The question is very simple (and very niche), but the analysis involves key results from statistical theory and numerical analysis (the Central Limit Theorem, the Delta method, and the second-order Delta method, Monte Carlo integration).</p>

<p>Here is the seemingly innocent (and slightly bizarre) question:</p>

<ul>
<li><strong>Context:</strong> One is interested in evaluating the square of an expectation $E\Big[g(X) \Big]^2 = \big(\int_{a}^{b} g(x) f_X(x) dx \Big)^2 = \mu^2$, where $\mu$ is an unknown quantity <strong>&ldquo;close to zero&rdquo;</strong>. Note that the integral squared can be rewritten as $\big(\int_{a}^{b} g(x) f_X(x) dx \Big)^2  = \int_{a}^{b} \int_{a}^{b} g(x) g(y) f_X(x) f_Y(y) dx dy $. Transforming the square of an integral into a double integral is a standard trick, that is for instance used when calculating the <a href="https://en.wikipedia.org/wiki/Gaussian_integral" target="_blank">Gaussian integral</a>. Let&rsquo;s say one want to numerically approximate $\mu^2$ using <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration" target="_blank">Monte Carlo integration</a>.</li>
<li><strong>Question:</strong> Is it better to approximate $\mu^2$ using the square of the Monte Carlo estimator $\big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2$, or the Monte Carlo estimator based on the double integral: $\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) $? Here the sample ${x_i}$ and ${y_i}$ are i.i.d random variables with density f(.). The random variables ${x_i}$ and ${y_i}$ have the same density function, but they are drawn independently from each other.</li>
</ul>

<h2 id="i-consistency">I. Consistency</h2>

<p>Notice that both estimators are <a href="https://en.wikipedia.org/wiki/Consistent_estimator" target="_blank">consistent</a>. That is, as we increase $N$, the estimators converges in probability to the true value of the parameter $\mu^2$.</p>

<h3 id="estimator-big-frac-1-n-sum-i-1-n-f-x-i-big-2">Estimator $\big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2$</h3>

<p><a href="https://www.mit.edu/~kircher/sim.pdf" target="_blank">We know that Monte Carlo estimator are consistent</a>: So $\frac{1}{N} \sum_{i=1}^{N} f(x_i)  \overset{p}{\to} \mu$. Because the function $x \rightarrow x^2$ is continuous on $R$, the <a href="https://en.wikipedia.org/wiki/Continuous_mapping_theorem" target="_blank">continuous mapping theorem</a> implies that  $\Big(\frac{1}{N} \sum_{i=1}^{N} f(x_i)\Big)^2  \overset{p}{\to} \mu^2$.</p>

<h3 id="estimator-frac-1-n-sum-i-1-n-f-x-i-f-y-i">Estimator $\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) $</h3>

<p>Once again, we know that Monte Carlo estimator are consistent. So $\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) \overset{p}{\to} \int_{a}^{b} \int_{a}^{b} g(x) g(y) f_X(x) f_Y(y) dx dy = \big(\int_{a}^{b} g(x) f_X(x) dx \Big)^2  = \mu^2$.</p>

<p>Hence, both estimators get closer to the true value as we increase the number of draws N.</p>

<h2 id="ii-bias">II. Bias</h2>

<p>Interestingly, $\big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2$ has a small sample bias, while $\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) $ does not.</p>

<h3 id="estimator-big-frac-1-n-sum-i-1-n-f-x-i-big-2-1">Estimator $\big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2$</h3>

<p>$$ E[ \big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2] = \big(E [\frac{1}{N} \sum_{i=1}^{N} f(x_i)] \Big)^2 + Var(\frac{1}{N} \sum_{i=1}^{N} f(x_i))$$</p>

<p>$$ E[ \big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2] = \mu^2 + \frac{\sigma^2}{N} $$</p>

<p>$$ Bias(\big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2) = \frac{\sigma^2}{N} $$</p>

<h3 id="estimator-frac-1-n-sum-i-1-n-f-x-i-f-y-i-1">Estimator $\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) $</h3>

<p>$$ E [\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i)] = \frac{1}{N} \sum_{i=1}^{N} E[f(x_i) f(y_i)]  $$</p>

<p>$$ = \frac{1}{N} \sum_{i=1}^{N} E[f(x_i)] E[ f(y_i)]  $$</p>

<p>$$ = \mu^2  $$</p>

<p>$$ Bias(\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) ) = 0 $$</p>

<p>Where, going from the first line to the second is based on the fact that $X$ and $Y$ are independent random variables. However, the bias is just one part of the whole story. To see which estimator is better, we also need to investigate their variance.</p>

<h2 id="iii-variance-case-mu-0">III. Variance. Case $\mu &gt; 0$</h2>

<h3 id="estimator-big-frac-1-n-sum-i-1-n-f-x-i-big-2-2">Estimator $\big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2$</h3>

<p>Note that the value $\bar{X<em>n} = \frac{1}{N} \sum</em>{i=1}^{N} f(x_i) $ is the sample mean. The <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank">Central Limit Theorem</a> implies that:</p>

<p>$$\sqrt{N} \frac{(\bar{X_n} - \mu)}{\sigma} \overset{d}{\to} N(0,1)$$</p>

<p>To get the (asymptotic) variance of $\bar{X_n}^2$, one may use the <a href="https://en.wikipedia.org/wiki/Delta_method" target="_blank">delta method</a>. The delta method states that if there is a sequence of random variables $X_n$ satisfying:</p>

<p>$$\sqrt{N} (\bar{X_n} - \theta) \overset{d}{\to} N(0,\sigma^2)$$</p>

<p>with $\theta$ and $\sigma^2$ finite valued constants. Assume that $g$ is a function with continuous first derivative, satisfying the property that $g&rsquo;(\theta)$ is <strong>non-zero valued</strong>. Then,</p>

<p>$$\sqrt{N} (g(\bar{X_n}) - g(\theta)) \overset{d}{\to} N(0,\sigma^2 [g&rsquo;(\theta)]^2)$$</p>

<p>Here $g(x) = x^2$, $g&rsquo;(x) = 2x$, so the delta method implies that:</p>

<p>$$\sqrt{N} (\bar{X_n}^2 - \mu^2) \overset{d}{\to} N(0,4\mu^2 \sigma^2)$$.</p>

<h3 id="estimator-frac-1-n-sum-i-1-n-f-x-i-f-y-i-2">Estimator $\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) $</h3>

<p>Let&rsquo;s use the notation $\bar{XY}_n = \frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) $. Once again, the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank">Central Limit Theorem</a> implies that:</p>

<p>$$\sqrt{N} (\bar{XY}_n - \mu^2) \overset{d}{\to} N(0, \sigma_{XY})$$</p>

<p>with $\sigma_{XY} = \sqrt{Var(\bar{XY}_n)}$.</p>

<p>If $X$ and $Y$ are independent random variables, then $Var(XY) = Var(X)Var(Y) + Var(X)(E[Y]^2) + Var(Y)(E[X]^2)$.</p>

<p>Here $Var(X) = Var(Y) = \sigma$, and $E[X] = E[Y] = \mu$, so the above equation simplifies to:</p>

<p>$$ Var(XY) = \sigma^4 + 2 \mu^2 \sigma^2$$.</p>

<h3 id="comparison-var-bar-xy-n-vs-var-bar-x-n-2">Comparison $Var(\bar{XY}_n)$ vs $Var(\bar{X}_n^2)$</h3>

<p>For large values of $\mu$, the estimator $\bar{XY}_n$ has smaller variance. However, when $\mu$ is small, $\bar{X_n}$ has a smaller variance. This property is illustrated below by plotting the function  $\mu \rightarrow (\sigma^4 + 2\mu^2\sigma^2) - 4\mu^2\sigma^2$ for different choices of $\sigma$.</p>

<pre><code class="language-julia">using Plots
using LaTeXStrings
using Distributions
gr()

N = 100
grid_sigma = collect(range(0, 1.0, length=N))
grid_mu = collect(range(0, 1.0, length=N))
grid_mu_smaller = collect(range(0, 0.25, length=N))
diff = zeros(N,N)


f(mu, sigma) = (sigma.^4 .+ (2.0.*mu.^2).*(sigma.^2)) .- 4.0 .*(mu.^2).*(sigma.^2)

#p = plot(grid_mu, grid_sigma, (x,y) -&gt; f(x,y), st = :contourf, xlabel=L&quot;\mu&quot;, ylabel=L&quot;\sigma&quot;, fill=true)
p0 = plot(grid_mu, x -&gt; 0.0, label=L&quot;0&quot;, linestyle=:dash, ylabel=L&quot;(\sigma^4 + 2\mu^2\sigma^2) - 4\mu^2\sigma^2&quot;)
plot!(p0, grid_mu, x -&gt; f(x,1.0), xlabel=L&quot;\mu&quot;, label=L&quot;f(\mu, \sigma=1.0)&quot;)
plot!(p0, grid_mu, x -&gt; f(x,0.75), xlabel=L&quot;\mu&quot;, label=L&quot;f(\mu, \sigma=0.75)&quot;)
plot!(p0, grid_mu, x -&gt; f(x,0.5), xlabel=L&quot;\mu&quot;, label=L&quot;f(\mu, \sigma=0.5)&quot;)

p1 = plot(grid_mu_smaller, x -&gt; 0.0, label=L&quot;0&quot;, linestyle=:dash, ylabel=L&quot;(\sigma^4 + 2\mu^2\sigma^2) - 4\mu^2\sigma^2&quot;)
plot!(p1, grid_mu_smaller, x -&gt; f(x,0.25), xlabel=L&quot;\mu&quot;, label=L&quot;f(\mu, \sigma=0.25)&quot;)
plot!(p1, grid_mu_smaller, x -&gt; f(x,0.20), xlabel=L&quot;\mu&quot;, label=L&quot;f(\mu, \sigma=0.20)&quot;)
plot!(p1, grid_mu_smaller, x -&gt; f(x,0.15), xlabel=L&quot;\mu&quot;, label=L&quot;f(\mu, \sigma=0.15)&quot;)

plot(p0, p1)
</code></pre>

<p><img src="Integral_Squared_6_0.svg" alt="svg" /></p>

<p>In the block of code below, I estimate $\bar{XY}_n$ and $\bar{X}_n^2$ in two cases. First, I assume that $X$ is normally distributed with mean $\mu$ and unit variance. In this case, we do not even need the central limit theorem, because the sum of iid Normal variables is also normally distributed. In the second case, I assume that $X$ is uniformly distributed $Uni(\mu-1, \mu+1)$, in which case the mean of $X$ is equal to $\mu$.</p>

<p>As illustrated below, in both instances $\bar{X}_n^2$ has a smaller variance than $\bar{XY}_n$. The figure also underlines that even when $X$ is not normally distributed, the CLT kicks in and the above analysis provides a good estimate of the small sample behavior of our estimators.</p>

<pre><code class="language-julia">N = 1000 #number of draws
N_replicate = 5000 #number of replications for the distribution of the estimator
list_plots = [] #to store plots
sigma = 1.0 #variance when simulating from Normal

for (index_distribution, distribution) in enumerate([&quot;Normal&quot;, &quot;Uniform&quot;])
    for (index_mu, mu) in enumerate([0.20, 0.10, 0.05])

        #println(mu)

        if distribution == &quot;Normal&quot;
            d_x = Normal(mu, sigma)
            d_x1 = Normal(mu, sigma)
            d_x2 = Normal(mu, sigma)
        else
            d_x = Uniform(-1,1) + mu
            d_x1 = Uniform(-1,1) + mu
            d_x2 = Uniform(-1,1) + mu
        end

        # f(x) = x
        distristribution_square = zeros(N_replicate) # (mean(x))^2
        distristribution_square_centered = zeros(N_replicate) # (mean(x))^2 - mu^2
        distristribution_product_centered = zeros(N_replicate) # (mean(x1 x2) - mu^2

        distristribution_square_std = zeros(N_replicate)
        distristribution_square_mean = zeros(N_replicate)

        distristribution_product_std = zeros(N_replicate)
        distristribution_product_mean = zeros(N_replicate)

        for i=1:N_replicate

            draws_x = rand(d_x, N)
            draws_x1 = rand(d_x1, N)
            draws_x2 = rand(d_x2, N)

            distristribution_square[i] = mean(draws_x .- mu)^2
            distristribution_square_centered[i] = sqrt(N)*(mean(draws_x)^2 - mu^2)
            distristribution_product_centered[i] = sqrt(N)*(mean(draws_x1.*draws_x2) - mu^2)

            distristribution_square_std[i] = std(draws_x)
            distristribution_square_mean[i] = mean(draws_x)

            distristribution_product_std[i] = std(draws_x1.*draws_x2)
            distristribution_product_mean[i] = mean(draws_x1.*draws_x2)

        end

        # Empirical sigmas
        mu_X1X2 = mean(distristribution_product_mean)
        sigma_X1X2 = mean(distristribution_square_std)
        var_n0_empirical = sigma_X1X2^4 + sigma_X1X2^2*(2*mu_X1X2^2)

        mu_X_squared = mean(distristribution_square_mean)
        sigma_X_squared = mean(distristribution_square_std)
        var_n1_empirical = (4*mu_X_squared^2*sigma_X_squared^2)

        # Theoretical sigmas when using Normal draws
        var_n0 = sigma^2 * sigma^2 + sigma^2*(2*mu^2) #var(distristribution_product_centered) #(sigma^2)
        var_n1 = (4*mu^2*sigma^2)

        pdf_normal_0 = Normal(0, sqrt(var_n0_empirical))
        pdf_normal_1 = Normal(0, sqrt(var_n1_empirical))

        # Plotting
        show_legend = false
        if index_mu == 2 &amp;&amp; index_distribution == 1
            show_legend = true
        end
        title_name = &quot;&quot;
        if index_distribution == 1
            title_name = &quot;N($(mu), $(sigma))&quot;
        else
            title_name = &quot;U(-1, 1) + $(mu)&quot;
        end

        p1 = histogram(distristribution_product_centered, label=L&quot;$\sqrt{N}(\bar{X_1 X_2}_{n} - \mu^2 ) $&quot;, normalize=true, alpha=0.5)
        histogram!(p1, distristribution_square_centered, label=L&quot;$\sqrt{N}(\bar{X}_{n}^2 - \mu^2 )$&quot;, normalize=true, alpha=0.5, title=title_name)

        plot!(p1, minimum(distristribution_product_centered):0.1:maximum(distristribution_product_centered), x-&gt; pdf(pdf_normal_0, x), label=L&quot;$N(0, \sigma^4 + 2\mu^2\sigma^2)$&quot;)
        plot!(p1, minimum(distristribution_square_centered):0.1:maximum(distristribution_square_centered), x-&gt; pdf(pdf_normal_1, x), label=L&quot;$N(0, 4 \mu^2 \sigma^2)$&quot;, legend = show_legend)
        push!(list_plots, p1)

    end

end

plot(list_plots[1], list_plots[2], list_plots[3], list_plots[4], list_plots[5], list_plots[6])
plot!(size=(1000,600))
</code></pre>

<p><img src="Integral_Squared_8_0.svg" alt="svg" /></p>

<h2 id="iv-variance-case-mu-0">IV. Variance. Case $\mu = 0$</h2>

<p>The limit case $\mu = 0$ deserves a special treatment. Indeed, for the delta method to applies, the condition that $gâ€²(\theta)$ is <strong>non-zero valued</strong> must hold. However with $g(\theta) = \theta^2$, $g&rsquo;(0) = 2\times0=0$.</p>

<p>In that context, the second-order delta method applies:</p>

<h3 id="second-order-delta-method">Second order delta method</h3>

<p>Consider a sequence of random variables $X_n$ satisfying:</p>

<p>$$\sqrt{N} (\bar{X_n} - \theta) \overset{d}{\to} N(0,\sigma^2)$$</p>

<p>with $\theta$ and $\sigma^2$ finite valued constants. Assume that $g$ is a function with continuous first and second derivatives, satisfying the property that $g&rsquo;(\theta) = 0$ and $g&rdquo;(\theta) \neq 0$. Then:</p>

<p>$$N (g(\bar{X_n}) - g(\theta)) \overset{d}{\to} \sigma^2 \frac{g&rdquo;(\theta)}{2} \chi^2(1)$$</p>

<p>So applying the second order delta method with $g&rsquo;(\theta) = \theta^2$ and $\mu=0$ gives us:</p>

<p>$$N (\bar{X_n}^2) \overset{d}{\to} \sigma^2 \chi^2(1)$$</p>

<p>Using the fact that $Var(\chi^2(1)) = 2$ and rearranging terms gives:</p>

<p>$$ Var(\bar{X_n}^2) \approx \frac{2 \sigma^4}{N^2}$$</p>

<p>This approximation should get increasingly better as $N$ increases.</p>

<h3 id="comparison-var-bar-xy-n-vs-var-bar-x-n-2-1">Comparison $Var(\bar{XY}_n)$ vs $Var(\bar{X}_n^2)$</h3>

<p>When $\mu=0$:</p>

<p>$$Var(\bar{XY}_n) = \frac{\sigma^4}{N}$$</p>

<p>and</p>

<p>$$Var(\bar{X_n}^2) = \frac{2\sigma^4}{N^2}$$</p>

<p>Because of the $N^2$ at the denominator, we see that $\bar{X_n}^2$ converges much faster than $\bar{XY}_n$. This property is illustrated on the plot below, in which I do the same exercice as before, but with $\mu=0$. Note that I show the histogram of $\sqrt{N}(\bar{X_1 X_2}_{n} - \mu^2 )$ versus $N(\bar{X}^2_{n} - \mu^2 )$ (scaling with $\sqrt{N}$ versus $N$).</p>

<pre><code class="language-julia">N = 1000#number of draws
N_replicate = 5000 #number of replications for the distribution of the estimator
list_plots = []
sigma=1.0 #variance when simulating from Normal

for (index_distribution, distribution) in enumerate([&quot;Normal&quot;, &quot;Uniform&quot;])
    for (index_mu, mu) in enumerate([0.0])

        #println(mu)

        if distribution == &quot;Normal&quot;
            d_x = Normal(mu, sigma)
            d_x1 = Normal(mu, sigma)
            d_x2 = Normal(mu, sigma)
        else
            d_x = Uniform(-1,1) + mu
            d_x1 = Uniform(-1,1) + mu
            d_x2 = Uniform(-1,1) + mu
        end

        # f(x) = x
        distristribution_square = zeros(N_replicate) # (mean(x))^2
        distristribution_square_centered = zeros(N_replicate) # (mean(x))^2 - mu^2
        distristribution_product_centered = zeros(N_replicate) # (mean(x1 x2) - mu^2

        distristribution_square_std = zeros(N_replicate)
        distristribution_square_var = zeros(N_replicate)
        distristribution_square_mean = zeros(N_replicate)

        distristribution_product_std = zeros(N_replicate)
        distristribution_product_mean = zeros(N_replicate)

        for i=1:N_replicate

            draws_x = rand(d_x, N)
            draws_x1 = rand(d_x1, N)
            draws_x2 = rand(d_x2, N)

            distristribution_square[i] = mean(draws_x .- mu)^2
            distristribution_square_centered[i] = N*(mean(draws_x)^2 - mu^2)/sigma
            distristribution_product_centered[i] = sqrt(N)*(mean(draws_x1.*draws_x2) - mu^2)

            distristribution_square_std[i] = std(draws_x)
            distristribution_square_var[i] = var(draws_x)
            distristribution_square_mean[i] = mean(draws_x)

            distristribution_product_std[i] = std(draws_x1.*draws_x2)
            distristribution_product_mean[i] = mean(draws_x1.*draws_x2)

        end

        # Empirical sigmas
        mu_X1X2 = mean(distristribution_product_mean)
        sigma_X1X2 = mean(distristribution_product_std)

        mu_X_squared = mean(distristribution_square_mean)
        var_X_squared = mean(distristribution_square_var)

        var_n0_empirical = var_X_squared^2 + sigma_X1X2^2*(2*mu_X1X2^2)

        pdf_normal_0 = Normal(0, sqrt(var_n0_empirical))
        pdf_chisq_1 = Chisq(1)

        # Plotting
        show_legend = false
        if index_mu == 1 &amp;&amp; index_distribution == 2
            show_legend = true
        end
        title_name = &quot;&quot;
        if index_distribution == 1
            title_name = &quot;N($(mu), $(sigma))&quot;
        else
            title_name = &quot;U(-1, 1) + $(mu)&quot;
        end

        p1 = histogram(distristribution_product_centered, label=L&quot;$\sqrt{N}(\bar{X_1 X_2}_{n} - \mu^2 ) $&quot;, normalize=true, alpha=0.5)
        histogram!(p1,distristribution_square_centered, label=L&quot;$N(\bar{X}_{n}^2 - \mu^2)/\sigma^2$&quot;, normalize=true, alpha=0.5, title=title_name)

        plot!(p1, minimum(distristribution_product_centered):0.1:maximum(distristribution_product_centered), x-&gt; pdf(pdf_normal_0, x), label=L&quot;$N(0, \sigma^4 + 2\mu^2\sigma^2)$&quot;)
        plot!(p1, 0.001:0.01:maximum(distristribution_square_centered), x-&gt; pdf(pdf_chisq_1, x), label=L&quot;$\chi^2(1)$&quot;, legend = show_legend)
        push!(list_plots, p1)

    end

end

plot(list_plots[1], list_plots[2])
plot!(size=(1000,300))
</code></pre>

<p><img src="Integral_Squared_11_0.svg" alt="svg" /></p>

<h2 id="v-mean-squared-error">V. Mean Squared Error</h2>

<p>To is a measure the quality of an estimator, it is common to use the mean squared error (MSE). In plain English, the mean squared error measures the average squared difference between the estimated values and the actual value:</p>

<p>$$ MSE(\hat{\theta}) = E_{\theta} [ (\hat{\theta} - \theta)^2] $$</p>

<p>The MSE has the appealing property that it is equal to the variance of the estimator, plus the bias squared:</p>

<p>$$ MSE(\hat{\theta}) = V [ \hat{\theta} ] + Bias(\hat{\theta})^2$$</p>

<p>Fortunately, we have already done the heavy lifting by calculating the biases and the variances of the estimators.
Collecting the previous results, we have.</p>

<h3 id="case-mu-0">Case $\mu &gt;0 $</h3>

<p>$$ MSE(\bar{X}^2_n) = \frac{\sigma_f^4 + 4N \mu^2 \sigma_f^2}{N^2} $$</p>

<p>$$ MSE(\bar{XY}_n) = \frac{\sigma_f^4}{N} $$</p>

<h3 id="case-mu-0-1">Case $\mu = 0$</h3>

<p>$$ MSE(\bar{X}^2_n) = \frac{3 \sigma_f^4}{N^2}$$</p>

<p>$$ MSE(\bar{XY}_n) = \frac{\sigma_f^4}{N} $$</p>

<p>It is easy to see that when $\mu$ is close or equal to 0, it is better to use $\bar{X}^2_n$ instead $\bar{XY}_n$. In the next section, I check that numerically with some examples.</p>

<h2 id="vi-numerical-illustration">VI. Numerical illustration</h2>

<h3 id="case-mu-0-2">Case $\mu = 0$</h3>

<p>The takeaway of the previous sections is that if we suspect $\mu$ in a neighborhood of 0, one is better off using $\bar{X_n}^2$ instead of $\bar{XY}_n$.</p>

<p>Now I compare the accuracy of both approaches for a numerical integration exercice. For instance, let&rsquo;s approximate the integral:</p>

<p>$$ \Big(\int_{-\infty}^{+\infty} x^3 \frac{1}{\sqrt{2\pi}}  \exp(\frac{-1}{2}x^2) dx \Big)^2 $$</p>

<p>The true value of the integral is known to be <strong>zero</strong> (odd central moment of a normally distributed random variable). The left panel in the next graph shows the mean squared error for $\bar{X}^2_n$ and $\bar{XY}_n$. As expected given the previous sections, $\bar{X}^2_n$ provides a much better approximation to $\mu^2$ than $\bar{XY}_n$.</p>

<pre><code class="language-julia">grid_N = [100, 1000, 10000, 100000, 1000000]
N_replicate = 1000 #number of replications for the distribution of the estimator

# To store stats values
distristribution_mean = zeros(length(grid_N), N_replicate)
distristribution_var = zeros(length(grid_N), N_replicate)
distristribution_var_product = zeros(length(grid_N), N_replicate)
distristribution_square_mean = zeros(length(grid_N), N_replicate)
distristribution_product_mean = zeros(length(grid_N), N_replicate)

# Store stats on distribution of values for each replication

distristribution_mean_var = zeros(length(grid_N))
distristribution_mean_var_product = zeros(length(grid_N))
distristribution_square_average = zeros(length(grid_N))
distristribution_square_var = zeros(length(grid_N))

distristribution_product_average = zeros(length(grid_N))
distristribution_product_var = zeros(length(grid_N))

# To store stats on Mean Squared Error (MSE)
distristribution_square_MSE_average = zeros(length(grid_N))
distristribution_square_MSE_var = zeros(length(grid_N))

distristribution_product_MSE_average = zeros(length(grid_N))
distristribution_product_MSE_var = zeros(length(grid_N))


mu = 0
sigma = 1.0 #variance when simulating from Normal
f(x) = x.^3

for (index_N, N) in enumerate(grid_N)

    d_x = Normal(mu, sigma)
    d_x1 = Normal(mu, sigma)
    d_x2 = Normal(mu, sigma)

    for i=1:N_replicate

        draws_x = rand(d_x, N)
        draws_x1 = rand(d_x1, N)
        draws_x2 = rand(d_x2, N)

        distristribution_mean[index_N, i] = mean(f(draws_x)) #mean f(x)
        distristribution_var[index_N, i] = var(f(draws_x)) #variance of f(x)
        distristribution_var_product[index_N, i] = var(f(draws_x1).*f(draws_x2)) #variance of f(x1)*f(x2)
        distristribution_square_mean[index_N, i] = mean(f(draws_x))^2
        distristribution_product_mean[index_N, i] = mean(f(draws_x1).*f(draws_x2))

    end

    #Stats across different replications
    # Variance estimator
    # E(x^2)
    distristribution_mean_var[index_N] = mean(distristribution_var[index_N, :]) #average of variances
    distristribution_mean_var_product[index_N] = mean(distristribution_var_product[index_N, :]) #average of variances of products
    distristribution_square_var[index_N] = var(distristribution_square_mean[index_N, :])
    distristribution_square_average[index_N] = mean(distristribution_square_mean[index_N, :])

    # E(xy)
    # prevent negative values using abs.
    distristribution_product_var[index_N] = var(distristribution_product_mean[index_N, :])
    distristribution_product_average[index_N] = mean(distristribution_product_mean[index_N, :])

    # Mean squared error
    # Note that here the true value is 0
    # E(x^2)
    distristribution_square_MSE_var[index_N] = var((distristribution_square_mean[index_N, :]).^2)
    distristribution_square_MSE_average[index_N] = mean((distristribution_square_mean[index_N, :]).^2)

    # E(xy)
    distristribution_product_MSE_var[index_N] = var(distristribution_product_mean[index_N, :].^2)
    distristribution_product_MSE_average[index_N] = mean(distristribution_product_mean[index_N, :].^2)


end


</code></pre>

<pre><code class="language-julia"># Plot for the expected value
p0 = plot(log10.(grid_N), log10.(distristribution_square_MSE_average), label=L&quot;$\bar{X}^2_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;)
plot!(log10.(grid_N), log10.(distristribution_product_MSE_average),  label =L&quot;$\bar{XY}_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;, ylabel=L&quot;$log_{10}(MSE)$&quot;)
plot!(log10.(grid_N), log10.((distristribution_mean_var.^2 )./(grid_N)), label=L&quot;$\sigma_f^4/N$&quot;, linestyle=:dash, linewidth=2.0)
plot!(log10.(grid_N), log10.((3.0 .* distristribution_mean_var.^2 )./(grid_N.^2)), label=L&quot;$3\sigma_f^4/N^2$&quot;, title=&quot;Mean Squared Error&quot;, linestyle=:dash, linewidth=2.0)

# Plot for the variance
p1 = plot(log10.(grid_N), log10.(distristribution_square_var), label=L&quot;$\bar{X}^2_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;)
plot!(p1, log10.(grid_N), log10.(distristribution_product_var),  label =L&quot;$\bar{XY}_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;)
# Variance absolute value of $\bar{XY}_n$
# Using Jensen's inequality, it is slightly below the variance of $\bar{XY}_n$
# plot!(p1, log10.(grid_N), log10.(var(abs.(distristribution_product_mean), dims=2)),  label =L&quot;$|\bar{XY}_n|$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;)
plot!(p1, log10.(grid_N), log10.((2.0 .* distristribution_mean_var.^2 )./(grid_N.^2)), label=L&quot;$2\sigma_f^4/N^2$&quot;, linestyle=:dash, linewidth=2.0)
plot!(p1, log10.(grid_N), log10.((distristribution_mean_var.^2 )./(grid_N)), label=L&quot;$\sigma_f^4/N$&quot;, title=&quot;Variance Estimator&quot;, ylabel=L&quot;$log_{10}(Variance)$&quot;, linestyle=:dash, linewidth=2.0)


plot(p0, p1)
plot!(size=(800,400))

</code></pre>

<p><img src="Integral_Squared_15_0.svg" alt="svg" /></p>

<p>The log scale of the previous graph does not do justice to the difference that exists between both estimators.
So in the next graph, I keep a linear scale. The blue line is the average value for a given $N$. This graph makes obvious $\bar{X}_n$ converges much quicker to $\mu^2$ than $\bar{XY}_n$.</p>

<pre><code class="language-julia">grid_N = collect(range(100, 100000, step=1000))
N_replicate = 100 #number of replications for the distribution of the estimator

distristribution_square_mean = zeros(length(grid_N), N_replicate)
distristribution_product_mean = zeros(length(grid_N), N_replicate)

# Distribution of values for each replication
distristribution_square_average = zeros(length(grid_N))
distristribution_square_var = zeros(length(grid_N))

distristribution_product_average = zeros(length(grid_N))
distristribution_product_var = zeros(length(grid_N))


mu = 0
sigma=1.0 #variance when simulating from Normal
f(x) = x.^3

for (index_N, N) in enumerate(grid_N)

    d_x = Normal(mu, sigma)
    d_x1 = Normal(mu, sigma)
    d_x2 = Normal(mu, sigma)

    for i=1:N_replicate

        draws_x = rand(d_x, N)
        draws_x1 = rand(d_x1, N)
        draws_x2 = rand(d_x2, N)

        distristribution_square_mean[index_N, i] = mean(f(draws_x))^2
        distristribution_product_mean[index_N, i] = mean(f(draws_x1).*f(draws_x2))

    end

    #Stats across different replications
    # E(x^2)
    distristribution_square_average[index_N] = mean(distristribution_square_mean[index_N, :])

    # E(xy)
    distristribution_product_average[index_N] = mean(distristribution_product_mean[index_N, :])

end

</code></pre>

<pre><code class="language-julia">min_val = minimum(distristribution_product_mean)
max_val = maximum(distristribution_product_mean)
p0 = scatter(grid_N, distristribution_square_mean, legend=false,title=L&quot;$\bar{X}^2_n$&quot;, xlabel=L&quot;$N$&quot;, alpha=0.5, ylims=(min_val, max_val ))
plot!(p0, grid_N, distristribution_square_average, label=&quot;mean&quot;, ylims=(min_val, max_val ), linewidth=2.0, color=&quot;blue&quot;, xrotation = 15)
p1 = scatter(grid_N, distristribution_product_mean, legend=false,title=L&quot;$\bar{XY}_n$&quot;, xlabel=L&quot;$N$&quot;, ylims=(min_val, max_val ))
plot!(p1, grid_N, distristribution_product_average, label=&quot;mean&quot;, ylims=(min_val, max_val ), linewidth=2.0, color=&quot;blue&quot;, xrotation = 15)
plot(p0, p1)
plot!(size=(800,600))
</code></pre>

<p><img src="Integral_Squared_18_0.svg" alt="svg" /></p>

<h3 id="case-mu-small">Case $\mu$ small</h3>

<p>In practice, we do not know for sure that $\mu$ is exactly equal to zero. However, we know that $\mu$ is &ldquo;close to zero&rdquo;. Are we still better off using $\bar{X}^2_n$ instead of $\bar{XY}_n$?</p>

<pre><code class="language-julia">grid_N = [10, 100, 1000, 10000, 100000, 1000000]
#grid_N = [10, 100, 1000, 10000]
N_replicate = 1000 #number of replications for the distribution of the estimator

# To store stats values
distristribution_mean = zeros(length(grid_N), N_replicate)
distristribution_var = zeros(length(grid_N), N_replicate)
distristribution_var_product = zeros(length(grid_N), N_replicate)
distristribution_square_mean = zeros(length(grid_N), N_replicate)
distristribution_product_mean = zeros(length(grid_N), N_replicate)

# Store stats on distribution of values for each replication
distristribution_mean_average = zeros(length(grid_N))
distristribution_mean_var = zeros(length(grid_N))
distristribution_mean_var_product = zeros(length(grid_N))
distristribution_square_average = zeros(length(grid_N))
distristribution_square_var = zeros(length(grid_N))

distristribution_product_average = zeros(length(grid_N))
distristribution_product_var = zeros(length(grid_N))

# To store stats on Mean Squared Error (MSE)
distristribution_square_MSE_average = zeros(length(grid_N))
distristribution_square_MSE_var = zeros(length(grid_N))

distristribution_product_MSE_average = zeros(length(grid_N))
distristribution_product_MSE_var = zeros(length(grid_N))


true_value = sqrt(0.01)
mu = true_value  #mean normal
sigma = 1.0 #variance when simulating from Normal
f(x) = x

for (index_N, N) in enumerate(grid_N)

    d_x = Normal(mu, sigma)
    d_x1 = Normal(mu, sigma)
    d_x2 = Normal(mu, sigma)

    for i=1:N_replicate

        draws_x = rand(d_x, N)
        draws_x1 = rand(d_x1, N)
        draws_x2 = rand(d_x2, N)

        distristribution_mean[index_N, i] = mean(f(draws_x)) #mean f(x)
        distristribution_var[index_N, i] = var(f(draws_x)) #variance of f(x)
        distristribution_var_product[index_N, i] = var(f(draws_x1).*f(draws_x2)) #variance of f(x1)*f(x2)
        distristribution_square_mean[index_N, i] = mean(f(draws_x))^2
        distristribution_product_mean[index_N, i] = mean(f(draws_x1).*f(draws_x2))

    end

    #Stats across different replications
    # Average of mean
    distristribution_mean_average[index_N] = mean(mean(distristribution_mean[index_N, :]))

    # Variance estimator
    # E(x^2)
    distristribution_mean_var[index_N] = mean(distristribution_var[index_N, :]) #average of variances
    distristribution_mean_var_product[index_N] = mean(distristribution_var_product[index_N, :]) #average of variances of products
    distristribution_square_var[index_N] = var(distristribution_square_mean[index_N, :])
    distristribution_square_average[index_N] = mean(distristribution_square_mean[index_N, :])

    # E(xy)
    # prevent negative values using abs.
    distristribution_product_var[index_N] = var(distristribution_product_mean[index_N, :])
    distristribution_product_average[index_N] = mean(distristribution_product_mean[index_N, :])

    # Mean squared error
    # Note that here the true value is 0
    # E(x^2)
    distristribution_square_MSE_var[index_N] = var((distristribution_square_mean[index_N, :] .- true_value^2).^2)
    distristribution_square_MSE_average[index_N] = mean((distristribution_square_mean[index_N, :] .- true_value^2).^2)

    # E(xy)
    distristribution_product_MSE_var[index_N] = var((distristribution_product_mean[index_N, :] .- true_value^2).^2)
    distristribution_product_MSE_average[index_N] = mean((distristribution_product_mean[index_N, :] .- true_value^2).^2)


end


</code></pre>

<pre><code class="language-julia"># Plot for the expected value
p0 = plot(log10.(grid_N), log10.(distristribution_square_MSE_average), label=L&quot;$\bar{X}^2_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;)
plot!(log10.(grid_N), log10.(distristribution_product_MSE_average),  label =L&quot;$\bar{XY}_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;, ylabel=L&quot;$log_{10}(MSE)$&quot;)
plot!(log10.(grid_N), log10.((distristribution_mean_var.^2 )./(grid_N)), label=L&quot;$\sigma_f^4/N$&quot;, linestyle=:dash, linewidth=2.0)
plot!(log10.(grid_N), log10.((3.0 .* distristribution_mean_var.^2 )./(grid_N.^2)), label=L&quot;$3\sigma_f^4/N^2$&quot;, title=&quot;Mean Squared Error&quot;, linestyle=:dash, linewidth=2.0)
plot!(log10.(grid_N), log10.((distristribution_mean_var.^2 + 4 .* grid_N .* distristribution_square_average .* distristribution_mean_var)./(grid_N.^2)), label=L&quot;$\frac{\sigma_f^4 + 4N \mu^2 \sigma_f^2}{N^2}$&quot;, title=&quot;Mean Squared Error&quot;, linestyle=:dash, linewidth=2.0)


# Plot for the variance
p1 = plot(log10.(grid_N), log10.(distristribution_square_var), label=L&quot;$\bar{X}^2_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;)
plot!(p1, log10.(grid_N), log10.(distristribution_product_var),  label =L&quot;$\bar{XY}_n$&quot;, xlabel=L&quot;$log_{10}(N)$&quot;)
plot!(p1, log10.(grid_N), log10.((2.0 .* distristribution_mean_var.^2 )./(grid_N.^2)), label=L&quot;$2\sigma_f^4/N^2$&quot;, linestyle=:dash, linewidth=2.0)
plot!(p1, log10.(grid_N), log10.((distristribution_mean_var.^2 )./(grid_N)), label=L&quot;$\sigma_f^4/N$&quot;, title=&quot;Variance Estimator&quot;, ylabel=L&quot;$log_{10}(Variance)$&quot;, linestyle=:dash, linewidth=2.0)
plot!(log10.(grid_N), log10.((4 .* distristribution_square_average .* distristribution_mean_var)./(grid_N)), label=L&quot;$\frac{4 \mu^2 \sigma_f^2}{N^2}$&quot;, title=&quot;Mean Squared Error&quot;, linestyle=:dash, linewidth=2.0)


plot(p0, p1)
plot!(size=(800,400))

</code></pre>

<p><img src="Integral_Squared_21_0.svg" alt="svg" /></p>

<p>Below is another example with a linear scale:</p>

<pre><code class="language-julia">grid_N = collect(range(10, 1000, step=10))
N_replicate = 100 #number of replications for the distribution of the estimator

distristribution_square_mean = zeros(length(grid_N), N_replicate)
distristribution_product_mean = zeros(length(grid_N), N_replicate)

# Distribution of values for each replication
distristribution_square_average = zeros(length(grid_N))
distristribution_square_var = zeros(length(grid_N))

distristribution_product_average = zeros(length(grid_N))
distristribution_product_var = zeros(length(grid_N))


mu = true_value
sigma=1.0 #variance when simulating from Normal
f(x) = x

for (index_N, N) in enumerate(grid_N)

    d_x = Normal(mu, sigma)
    d_x1 = Normal(mu, sigma)
    d_x2 = Normal(mu, sigma)

    for i=1:N_replicate

        draws_x = rand(d_x, N)
        draws_x1 = rand(d_x1, N)
        draws_x2 = rand(d_x2, N)

        distristribution_square_mean[index_N, i] = mean(f(draws_x))^2
        distristribution_product_mean[index_N, i] = mean(f(draws_x1).*f(draws_x2))

    end

    #Stats across different replications
    # E(x^2)
    distristribution_square_average[index_N] = mean(distristribution_square_mean[index_N, :])

    # E(xy)
    distristribution_product_average[index_N] = mean(distristribution_product_mean[index_N, :])

end

</code></pre>

<pre><code class="language-julia">min_val = minimum(distristribution_product_mean)
max_val = maximum(distristribution_product_mean)
p0 = scatter(grid_N, distristribution_square_mean, legend=false,title=L&quot;$\bar{X}^2_n$&quot;, xlabel=L&quot;$N$&quot;, alpha=0.5, ylims=(min_val, max_val ))
plot!(p0, grid_N, distristribution_square_average, label=&quot;mean&quot;, ylims=(min_val, max_val ), linewidth=3.0, color=&quot;blue&quot;, xrotation = 15)
plot!(grid_N, x -&gt; mu^2, label=L&quot;\mu^2&quot;, linestyle = :dash, linewidth=3.0)
p1 = scatter(grid_N, distristribution_product_mean, legend=false,title=L&quot;$\bar{XY}_n$&quot;, xlabel=L&quot;$N$&quot;, ylims=(min_val, max_val ))
plot!(p1, grid_N, distristribution_product_average, label=&quot;mean&quot;, ylims=(min_val, max_val ), linewidth=3.0, color=&quot;blue&quot;, xrotation = 15)
plot!(p1, grid_N, x -&gt; mu^2, label=L&quot;\mu^2&quot;, linestyle = :dash, linewidth=3.0)
plot(p0, p1)
plot!(size=(800,600))
</code></pre>

<p><img src="Integral_Squared_24_0.svg" alt="svg" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>Based on a comparison of the mean squared error, $\big(\frac{1}{N} \sum_{i=1}^{N} f(x_i) \Big)^2$ is a <strong>better</strong> estimator than $\frac{1}{N} \sum_{i=1}^{N} f(x_i) f(y_i) $
<strong>when $\mu^2$ is small</strong>.</p>

<p>Why should we care? In some applications, we aim at finding a function $f$ that minimizes the square of an expectation: $\big(E[f(X)]\big)^2$. Numerical methods can find $\hat{f}$ such that $\big(E[\hat{f}(X)]\big)^2 \approx 0$. In general, no closed-form solution is available for the expectation, so one must use numerical methods. For instance, Monte Carlo integration. Hence, the question of $\bar{X}_n^2$ vs $\bar{XY}_n$ appears quite naturally in that context.</p>

<h2 id="appendix">Appendix</h2>

<pre><code class="language-julia">versioninfo()
</code></pre>

<pre><code>Julia Version 1.7.1
Commit ac5cc99908 (2021-12-22 19:35 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-12.0.1 (ORCJIT, skylake)
Environment:
  JULIA_NUM_THREADS = 4
</code></pre>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/statistics/">Statistics</a>
  
  <a class="badge badge-light" href="/tags/expectations/">Expectations</a>
  
</div>




    
      






  







<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  
  <img class="portrait mr-3" src="/author/admin/avatar_hu3ebf584d0e96ded762042713d38b4f6f_6501895_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
  

  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/authors/admin">Julien Pascal, PhD</a></h5>
    <h6 class="card-subtitle">Economist</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/Juli3nPascal" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/JulienPascal" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://www.linkedin.com/in/julien-pascal-62a322aa/" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://medium.com/@julien.pascal" target="_blank" rel="noopener">
          <i class="fab fa-medium"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>



      
      
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2017 Julien Pascal &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/julia.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/shell.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d7381f2d79e6271d4da28f474f49096c.js"></script>

  </body>
</html>

