<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.1.0">
  <meta name="generator" content="Hugo 0.54.0" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Julien Pascal, PhD">

  
  
  
    
  
  <meta name="description" content="Introduction Artificial Neural networks (ANN) are very trendy at the moment, and rightly so.
They are being used everywhere in big tech companies. For instance, when you use Google translate, or when recommandations appear on your Netflix feed, complex artificial neural networks are being used behind the scene. Behind the success of Alpha Go at the game of Go against Lee Sedol, an ANN was used to identify the next best move.">

  
  <link rel="alternate" hreflang="en-us" href="https://julienpascal.github.io/post/ann_1/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'G-BSZS1F0NHL', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://julienpascal.github.io/index.xml" type="application/rss+xml" title="Julien Pascal">
  <link rel="feed" href="https://julienpascal.github.io/index.xml" type="application/rss+xml" title="Julien Pascal">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://julienpascal.github.io/post/ann_1/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@Juli3nPascal">
  <meta property="twitter:creator" content="@Juli3nPascal">
  
  <meta property="og:site_name" content="Julien Pascal">
  <meta property="og:url" content="https://julienpascal.github.io/post/ann_1/">
  <meta property="og:title" content="Artificial Neural Networks as universal function approximators | Julien Pascal">
  <meta property="og:description" content="Introduction Artificial Neural networks (ANN) are very trendy at the moment, and rightly so.
They are being used everywhere in big tech companies. For instance, when you use Google translate, or when recommandations appear on your Netflix feed, complex artificial neural networks are being used behind the scene. Behind the success of Alpha Go at the game of Go against Lee Sedol, an ANN was used to identify the next best move."><meta property="og:image" content="https://julienpascal.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2021-11-28T18:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2021-11-28T18:00:00&#43;01:00">
  

  

  

  <title>Artificial Neural Networks as universal function approximators | Julien Pascal</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Julien Pascal</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#computing">
            
            <span>Computing</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/pdf/Julien_Pascal_Academic_Resume.pdf">
            
            <span>Academic Resume</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Artificial Neural Networks as universal function approximators</h1>

  

  
    



<meta content="2021-11-28 18:00:00 &#43;0100 CET" itemprop="datePublished">
<meta content="2021-11-28 18:00:00 &#43;0100 CET" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Nov 28, 2021</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min read
  </span>
  

  
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Artificial%20Neural%20Networks%20as%20universal%20function%20approximators&amp;url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fann_1%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fjulienpascal.github.io%2fpost%2fann_1%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fann_1%2f&amp;title=Artificial%20Neural%20Networks%20as%20universal%20function%20approximators"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fjulienpascal.github.io%2fpost%2fann_1%2f&amp;title=Artificial%20Neural%20Networks%20as%20universal%20function%20approximators"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Artificial%20Neural%20Networks%20as%20universal%20function%20approximators&amp;body=https%3a%2f%2fjulienpascal.github.io%2fpost%2fann_1%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    







  









  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<h2 id="introduction">Introduction</h2>

<p><strong>Artificial Neural networks</strong> (ANN) are very trendy at the moment, and rightly so.</p>

<p>They are being used everywhere in big tech companies. For instance, when you use Google translate, or when recommandations appear on your Netflix feed, complex artificial neural networks are being used behind the scene. Behind the success of <a href="https://en.wikipedia.org/wiki/AlphaGo" target="_blank">Alpha Go</a> at the game of <a href="https://www.washingtonpost.com/news/innovations/wp/2016/03/15/what-alphagos-sly-move-says-about-machine-creativity/" target="_blank">Go against Lee Sedol</a>, an ANN was used to identify the next best move.</p>

<p>ANN are also increasingly being used in Economic modeling, as exemplified by two recent publications:</p>

<ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0304393221000799" target="_blank">Deep learning for solving dynamic economic models</a></li>
<li><a href="https://www.nber.org/system/files/working_papers/w26302/w26302.pdf" target="_blank">Financial Frictions and the Wealth Distribution</a></li>
</ul>

<p>In this blog article, I discuss the reasons behind the popularity of neural networks</p>

<p><strong>Spoiler alert:</strong> It has to do with ANN being <strong>universal function approximators.</strong></p>

<p>As usual, I like to include Julia code to illustrate how things work in practice. My tool of choice is <a href="https://julialang.org/" target="_blank">Julia</a> because
it is really fast and it is an increasingly popular programming language. For machine learning tasks, <a href="https://github.com/FluxML/Flux.jl" target="_blank">Flux.jl</a> is a really good option, so let&rsquo;s use it as well. You can download the code <a href="https://github.com/JulienPascal/ANN_Flux" target="_blank">here</a>.</p>

<h2 id="i-some-theory">I. Some theory</h2>

<h3 id="architecture">Architecture</h3>

<p>If you ended up here, you probably have already some knowledge about what an artificial neural network is. So I will be brief. In a nutshell, a neural network is made of several interconnected layers. Each layer is constituted of nodes. Nodes between adjacent layers exchange information between each others. The way the nodes communicate between each others is captured by parameter values associated to each nodes.</p>

<p>See the graph below:</p>

<p><img src="ANN.png" alt="Drawing" style="width: 400px;"/></p>

<p>Source: <a href="https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Artificial_neural_network.svg" target="_blank">https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Artificial_neural_network.svg</a></p>

<p>An artificial neural network mimics at a high level what the brain does. The brain is composed of neurons and neurons are connected to each others via synapses. Our brain is very good at recognizing patterns, so one might hope that an artificial neural network could be a good pattern-recognition machine.</p>

<p>In practice, it is the case. Even better, we have some theorems that tell us that ANN are really, really good.</p>

<h3 id="universal-approximation-theorems">Universal approximation theorems</h3>

<p>Let me describe two important papers. Below, I reproduce some selected parts of their abstracts:</p>

<hr />

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/0893608089900208" target="_blank">Hornik Stinchcombe and White (1989)</a></p>

<p>&ldquo;This paper rigorously establishes that <strong>standard multilayer feedforward networks with as few as one hidden layer</strong> using arbitrary squashing functions <strong>are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy</strong>, provided sufficiently many hidden units are available. In this sense, <strong>multilayer feedforward networks are a class of universal approximators.</strong>&ldquo;</p>

<p><a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf" target="_blank">Barron (1993)</a></p>

<p>&ldquo;<strong>It  is  shown  that  feedforward networks with one layer of sigmoidal nonlinearities achieve integrated  squared  error  of order  O(1/n),  where  n  is  the  number  of  nodes.</strong>  [&hellip;]  For the class of functions  examined  here, the approximation rate  and  the  parsimony  of  the  parameterization  of  the  networks  are
<strong>surprisingly  advantageous  in high-dimensional settings.</strong>&ldquo;</p>

<hr />

<p>The paper by <strong>Hornik Stinchcombe and White (1989)</strong> tells us that a very large class of functions can be approximated by an ANN with the architecture we presented above. The underlying function we aim to approximate is only required to be &ldquo;Borel measurable&rdquo; (from one finite dimensional space to another), which contains pretty much all the useful functions you use in Economics (continuous functions from one finite dimensional space to another are Borel measurable functions).</p>

<p>The paper by <strong>Barron (1993)</strong> tells us that ANN are particularly good approximators when working with many dimensions.
Said differently, ANN can help to mitigate the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank">curse of dimensionality</a>. One way to understand the curse of dimensionality is that the number of points needed to approximate a function grows exponentially with the number of dimensions, not linearly. We would like to explain complex phenomenon, with many dimensions and interactions, but traditional approximation methods generally do poorly in such settings.</p>

<p>Put together, these results tell us that ANN are very good function approximators, even when the number of dimensions is high.</p>

<hr />

<hr />

<h2 id="ii-application">II. Application</h2>

<p>Now let&rsquo;s see two applications. To warm up, we will start with a smooth and nice function to approximate.
Then we will move to a more complex function.</p>

<h3 id="ii-a-easy-function">II. A. Easy function</h3>

<p>Let&rsquo;s load some useful packages and define the function we would like to approximate</p>

<pre><code class="language-julia"># Dependencies
using Flux
using Plots
using LinearAlgebra
using ProgressMeter
using Statistics
using LaTeXStrings
using Surrogates
gr()

# Define function that we would like to learn with our neural network
f(x) = x[1].^2 + x[2].^2
</code></pre>

<pre><code>f (generic function with 1 method)
</code></pre>

<p>A function is an infinite-dimensional object. But we need a finite number of values to train our neural network.
To that, let&rsquo;s create a sample of points from an interval (I use <a href="https://en.wikipedia.org/wiki/Sobol_sequence" target="_blank">Sobol sampling</a>) and then
evaluate the value of the true function for these points.</p>

<pre><code class="language-julia">n_samples = 100
lower_bound = [-1.0, -1.0]
upper_bound = [1.0, 1.0]

xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())
rawInputs = xys
rawOutputs = [[f(xy)] for xy in xys] # Compute outputs for each input
trainingData = zip(rawInputs, rawOutputs);
</code></pre>

<p>Now is the fun part of deciding the architecture of our ANN.
I choose two hidden layers. The number of nodes for the first layer is
imposed by the dimension of the input (a 2d vector), as well the dimension of
the final node (a scalar). We still have to choose the number of nodes in between.
For the first hidden layer I choose 784 nodes, and 50 for the second hidden layer.
To be fair, these choices are a bit random (I was influenced by the <a href="https://fluxml.ai/Flux.jl/stable/training/training/" target="_blank">Flux.jl tutorial here</a>). Feel free to experiment with different values.</p>

<pre><code class="language-julia"># Define the neural network layers (this defines a function called model(x))
# Specify our model
dim_input = 2
dim_ouptut = 1
Q1 = 784;
Q2 = 50;

# Two inputs, one output
model = Chain(Dense(2,Q1,relu),
            Dense(Q1,Q2,relu),
            Dense(Q2,1,identity));
</code></pre>

<p>Next we define a <strong>loss function</strong>, which measures the accuracy of the approximation.
The smaller the loss, the better. We use the <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank">mean squared error</a> loss function. The name of the game is to find the parameter values that minimize the loss function.
One way to do minimize the loss function is to use the <strong>gradient descent algorithm</strong>.</p>

<p>Here is an intuitive explanation of gradient descent. Imagine that you are on a top of a mountain at that there is a
lot of fog that prevents you from seeing far away. You really want to go down. What should you do?</p>

<p>One strategy is to look at where you stand and evaluate the direction of the steepest descent in the <strong>neighborhood of your location</strong> (you can&rsquo;t see far away). Then take a step in that direction. Then repeat the process. If the mountain is &ldquo;well-behaved&rdquo; (it has no local minima), you will manage to go down the mountain, even though you were just using local information at every step. (Go the very bottom of this blog post to see an illustration of gradient descent on a really easy problem).</p>

<pre><code class="language-julia"># Define loss function and weights
loss(x, y) = Flux.Losses.mse(model(collect(x)), y)

lr = 0.001 # learning rate

# V1. Gradient descent
opt = Descent(lr)

# V2. ADAM
#decay = 0.9
#momentum =0.999
#opt = ADAM(lr, (decay, momentum))

epochs = 1000 # Define the number of epochs
trainingLosses = zeros(epochs);# Initialize a vector to keep track of the training progress
</code></pre>

<p>Next is the most rewarding step: <strong>the training part</strong>. The following block of code
does gradient descent. The function <code>Flux.train!</code> uses all the observations
we have in our sample once. Because one iteration is not enough to reach a minimum,
we repeat the process several <code>epochs</code>. After each epoch, we calculate the mean squared
error to see how well the model does.</p>

<pre><code class="language-julia">ps = Flux.params(model) #initialize weigths
p = Progress(epochs; desc = &quot;Training in progress&quot;); # Creates a progress bar
showProgress = true

# Training loop
@time for ii in 1:epochs

    Flux.train!(loss, ps, trainingData, opt)

    # Update progress indicator
    if showProgress
        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])
        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)
    end

end
</code></pre>

<pre><code>┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell.
│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`.
│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.
└ @ ProgressMeter /home/julien/.julia/packages/ProgressMeter/Vf8un/src/ProgressMeter.jl:620
[32mTraining in progress100%|███████████████████████████████| Time: 0:00:24[39m
[39m  loss:     0.00017162366528025578[39m
[39m  logloss:  -3.7654228272565655[39m


 24.753884 seconds (41.00 M allocations: 37.606 GiB, 6.56% gc time, 0.48% compilation time)
</code></pre>

<p>The next plot shows a surface plot for the original function and the value returned by the ANN (the dots).
Results are quite good. The top right graph displays the value of the loss function as the training takes place.
Gradient descent seems to work well, because the loss function decreases in a nice and monotonic manner. The bottom plot displays the surface plot for the trained ANN.</p>

<pre><code class="language-julia">nb_points = 100
grid_x = collect(range(lower_bound[1], upper_bound[1], length= nb_points))
grid_y = collect(range(lower_bound[2], upper_bound[2], length= nb_points))

grid_x_random = [xy[1] for xy in xys]
grid_y_random = [xy[2] for xy in xys]

Zs = [model([x,y])[1] for (x, y) in zip(grid_x_random, grid_y_random)];

# Plot output for trained neural network
p1 = plot(grid_x, grid_y, (x, y) -&gt; f([x,y]), label = &quot;f(x)&quot;, st=:surface)
scatter!(p1, grid_x_random, grid_y_random, Zs, label=&quot;ANN&quot;)
title!(&quot;Original function&quot;)
xlabel!(L&quot;x&quot;)
ylabel!(L&quot;y&quot;)


# Plot training loss
p2 = plot(1:epochs, log.(trainingLosses), label = &quot;&quot;, linewidth = 2)
title!(&quot;Training Loss&quot;)
xlabel!(&quot;Epoch&quot;)
ylabel!(L&quot;\log(\textrm{Loss})&quot;)

# Neural network
p3 = plot(grid_x, grid_y, (x, y) -&gt; model([x,y])[1], label = &quot;f(x)&quot;, st=:surface)
title!(&quot;Trained Neural Network&quot;)
xlabel!(L&quot;x&quot;)
ylabel!(L&quot;y&quot;)

ratio = 9/16
width = 800
pp = plot(p1, p2, p3, size = (width, width*ratio))
</code></pre>

<p><img src="ANN_1_12_0.svg" alt="svg" /></p>

<h3 id="ii-b-more-challenging-function">II.B. More challenging function</h3>

<p>Ok, so our ANN works with a simple function, which is reassuring.
Let&rsquo;s now turn to a more challenging function. For instance,
we can try to approximate the <a href="https://en.wikipedia.org/wiki/Ackley_function" target="_blank">Ackley function</a>, which
is a slightly crazy function often used to test minimization algorithms (it has a global minimum at the origin).</p>

<p>Even with a more complex function, our ANN does a great job a approximating the true function, as you can see on the graph below.</p>

<pre><code class="language-julia">function ackley(x; e = exp(1), a = 10.0, b = -0.2, c=2.0*π)
    #a, b, c = 10.0, -0.2, 2.0*π
    len_recip = inv(length(x))
    sum_sqrs = zero(eltype(x))
    sum_cos = sum_sqrs
    for i in x
        sum_cos += cos(c*i)
        sum_sqrs += i^2
    end
    return -a * exp(b * sqrt(len_recip*sum_sqrs)) - exp(len_recip*sum_cos) + a + e
end

n_samples = 1000
lower_bound = [-2.0, -2.0]
upper_bound = [2.0, 2.0]
xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())
rawInputs = xys

rawOutputs = [[ackley(xy)] for xy in xys] # Compute outputs for each input
trainingData = zip(rawInputs, rawOutputs);

# Define the neural network layers (this defines a function called model(x))
# Specify our model
Q1 = 784;
Q2 = 50;
Q3 = 10;

# Two inputs, one output
model = Chain(Dense(2,Q1,relu),
            Dense(Q1,Q2,relu),
            Dense(Q2,1,identity));

# Define loss function and weights
loss(x, y) = Flux.Losses.mse(model(collect(x)), y)
ps = Flux.params(model)

# Train the neural network
epochs = 1000
showProgress = true
lr = 0.001 # learning rate

# V1. Gradient descent
opt = Descent(lr)

# V2. ADAM
#decay = 0.9
#momentum =0.999
#opt = ADAM(lr, (decay, momentum))

trainingLosses = zeros(epochs) # Initialize vectors to keep track of training
p = Progress(epochs; desc = &quot;Training in progress&quot;) # Creates a progress bar

@time for ii in 1:epochs

    Flux.train!(loss, ps, trainingData, opt)

    # Update progress indicator
    if showProgress
        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])
        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)
    end

end


</code></pre>

<pre><code>┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell.
│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`.
│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.
└ @ ProgressMeter /home/julien/.julia/packages/ProgressMeter/Vf8un/src/ProgressMeter.jl:620
[32mTraining in progress100%|███████████████████████████████| Time: 0:04:02[39m
[39m  loss:     0.01753345824641543[39m
[39m  logloss:  -1.7561324165239636[39m


242.064635 seconds (407.63 M allocations: 375.931 GiB, 6.81% gc time, 0.04% compilation time)
</code></pre>

<pre><code class="language-julia">nb_points = 100
grid_x = collect(range(lower_bound[1], upper_bound[1], length= nb_points))
grid_y = collect(range(lower_bound[2], upper_bound[2], length= nb_points))

grid_x_random = [xy[1] for xy in xys]
grid_y_random = [xy[2] for xy in xys]

Zs = [model([x,y])[1] for (x, y) in zip(grid_x_random, grid_y_random)];

# Plot output for trained neural network
p1 = plot(grid_x, grid_y, (x, y) -&gt; ackley([x,y]), label = &quot;f(x)&quot;, st=:surface)
# Show somes points, not all of them (otherwise hard to see anything)
scatter!(p1, grid_x_random[1:100], grid_y_random[1:100], Zs[1:100], label=&quot;ANN&quot;)
title!(&quot;Original Function&quot;)
xlabel!(L&quot;x&quot;)
ylabel!(L&quot;y&quot;)


# Plot training loss
p2 = plot(1:epochs, log.(trainingLosses), label = &quot;&quot;, linewidth = 2)
title!(&quot;Training Loss&quot;)
xlabel!(&quot;Epoch&quot;)
ylabel!(L&quot;\log(\textrm{Loss})&quot;)


# Plot output for trained neural network
p3 = plot(grid_x, grid_y, (x, y) -&gt; model([x,y])[1], label = &quot;f(x)&quot;, st=:surface)
title!(&quot;Trained Neural Network&quot;)
xlabel!(L&quot;x&quot;)
ylabel!(L&quot;y&quot;)

ratio = 9/16
width = 800
pp = plot(p1, p2, p3, size = (width, width*ratio))
</code></pre>

<p><img src="ANN_1_15_0.svg" alt="svg" /></p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Artificial Neural Network are universal function approximators. This blog post showed how to start using ANN to approximate relatively simple functions.</p>

<p>When trying to solve an economic model, one often has to find an unknown function that satisfies a number of
restrictions. In a subsequent post, I wish to describe how an ANN can be used to find such an unknown function.</p>

<hr />

<hr />

<h3 id="extra-gradient-descent-visually">Extra: gradient descent visually</h3>

<p>Below is an illustration of gradient descent.
We want to find the minimum of the function <code>J(x)=x^2</code> and we start
at the point <code>-20</code>.</p>

<p>The algorithm proceeds iteratively:</p>

<ol>
<li>Calculate the <strong>gradient</strong> at the current value. This gives us the direction
of the maximum change for the function <code>J</code>.</li>
<li>Because we are looking for a <strong>minimum</strong> and not a maximum, take a step in the opposite
direction of the maximum change</li>
<li>Repeat steps 1-2</li>
</ol>

<pre><code class="language-julia">using GradDescent
#Code from here: https://jacobcvt12.github.io/GradDescent.jl/stable/
#I made only slight modifications to the original code

# objective function and gradient of objective function
J(x) = x^2
dJ(x) = 2*x

# number of epochs
epochs = 150

# instantiation of Adagrad optimizer with learning rate of 2
opt = Adagrad(η=2.0)

# initial value for x (usually initialized with a random value)
x = -20.0 #initial position on the function
values_x = zeros(epochs) #initialization
value_y = zeros(epochs) #initialization
iter_x = zeros(epochs) #initialization

for i in 1:epochs
    # Save values for plotting
    values_x[i] = x
    value_y[i] = J(x)
    iter_x[i] = i

    # calculate the gradient wrt to the current x
    g = dJ(x)

    # change to the current x
    δ = update(opt, g)
    x -= δ
end

</code></pre>

<p>As you can see on the plot below, we start from the left hand side and then we make some quite big moves
to the right. As time passes, the points go from yellow to darker colors.
After about 150 iterations, we are very close to the true minimum at 0.</p>

<pre><code class="language-julia">plot(values_x, value_y, label=&quot;J(x)&quot;)
scatter!(values_x, value_y, marker_z = iter_x, color = cgrad(:thermal, rev = true), label=&quot;Position&quot;, colorbar_title=&quot;Iteration&quot;)
xlabel!(L&quot;x&quot;)
ylabel!(L&quot;J(x)&quot;)
</code></pre>

<p><img src="ANN_1_20_0.svg" alt="svg" /></p>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/numerical/">Numerical</a>
  
  <a class="badge badge-light" href="/tags/ai/">AI</a>
  
</div>




    
      






  







<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  
  <img class="portrait mr-3" src="/author/admin/avatar_hu3ebf584d0e96ded762042713d38b4f6f_6501895_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
  

  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/authors/admin">Julien Pascal, PhD</a></h5>
    <h6 class="card-subtitle">Economist</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/Juli3nPascal" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/JulienPascal" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://www.linkedin.com/in/julien-pascal-62a322aa/" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://medium.com/@julien.pascal" target="_blank" rel="noopener">
          <i class="fab fa-medium"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>



      
      
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2017 Julien Pascal &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/julia.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/shell.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d7381f2d79e6271d4da28f474f49096c.js"></script>

  </body>
</html>

