<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julien Pascal on Julien Pascal</title>
    <link>https://julienpascal.github.io/</link>
    <description>Recent content in Julien Pascal on Julien Pascal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Julien Pascal</copyright>
    <lastBuildDate>Tue, 04 Jan 2022 18:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Artificial Neural Networks to Solve Economic Models</title>
      <link>https://julienpascal.github.io/post/ann_2/</link>
      <pubDate>Tue, 04 Jan 2022 18:00:00 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/ann_2/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&#34;https://julienpascal.github.io/post/ann_1/&#34; target=&#34;_blank&#34;&gt;a previous post&lt;/a&gt;, I discussed why Artificial Neural Networks (ANN) are very popular tools: (i) they can approximate a very large set of functions (ii) they work well in high-dimensional spaces (iii) we can train them efficiently using gradient descent (even better if you have a GPU). In the application part, I showed how to use them in practice using &lt;a href=&#34;https://julialang.org/&#34; target=&#34;_blank&#34;&gt;Julia&lt;/a&gt; and &lt;a href=&#34;https://github.com/FluxML/Flux.jl&#34; target=&#34;_blank&#34;&gt;Flux.jl&lt;/a&gt; with two toy examples.&lt;/p&gt;

&lt;p&gt;Now it&amp;rsquo;s time to be a bit more ambitious. In my previous post, we could sample from the function we wanted to approximate. However, in many instances we cannot simply do that. &lt;strong&gt;The unknown element&lt;/strong&gt; is the &lt;strong&gt;function we want to approximate&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR: ANN work well to approximate unknown functions.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The notebook for this post is available &lt;a href=&#34;https://github.com/JulienPascal/ANN_Flux&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;i-theory&#34;&gt;I. Theory&lt;/h2&gt;

&lt;p&gt;The cornerstone of Macro and Labour Economics is the dynamic optimization problem, in which an agent chooses a series of actions, taking account a set of constraints and the evolution of the state of the economy. A classical example is the a central planner deciding &lt;a href=&#34;https://en.wikipedia.org/wiki/Ramsey%E2%80%93Cass%E2%80%93Koopmans_model&#34; target=&#34;_blank&#34;&gt;how much to save and consume&lt;/a&gt; at any point in time. Another example would be &lt;a href=&#34;https://www.jstor.org/stable/2297896&#34; target=&#34;_blank&#34;&gt;a worker searching for a job and deciding which job to accept and when to resign&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These types of problems naturally lend themselves to the following &lt;a href=&#34;https://en.wikipedia.org/wiki/Bellman_equation&#34; target=&#34;_blank&#34;&gt;Bellman equation&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;$$ V(x,z) = \max_{c \in \Gamma(x,z)}[ F(x,c,z) + \beta E[ V(T(x,c), z&amp;rsquo;) ]$$&lt;/p&gt;

&lt;p&gt;where $x$ is a known state variable and $z$ is a random variable following a Markov process. Both $x$ and $z$ are vectors. The function $\Gamma(.)$ represents the set of possible actions given a current state and the function $T(.)$ captures the transition from one state to another when an action is taken. The variable $\beta$ is a discount factor strictly smaller than one and the $E$ represent an expectation operator.&lt;/p&gt;

&lt;p&gt;I do not want to delve too much into the fascinating theory of optimal control. The point is that the unknown is the function $V(.)$. Given the good properties of ANN mentioned above, one may be tempted to use them to approximate the true $V(.)$. And this is exactly what we are going to do.&lt;/p&gt;

&lt;h2 id=&#34;ii-application&#34;&gt;II. Application&lt;/h2&gt;

&lt;p&gt;For the application, let&amp;rsquo;s focus on the model of the labour market of &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/aer.20131118&#34; target=&#34;_blank&#34;&gt;Lise and Robin 2017&lt;/a&gt;.Why? Because I have worked a lot with this model &lt;a href=&#34;https://www.theses.fr/2020IEPP0013&#34; target=&#34;_blank&#34;&gt;during my PhD&lt;/a&gt; and also because the model has really nice properties. The model features &lt;strong&gt;heterogeneous firms, workers&lt;/strong&gt; (one can think of large versus small firms and workers with different skills) and &lt;strong&gt;business cycle fluctuations&lt;/strong&gt; (booms and busts). Despite the underlying complexity, the model lends itself to a pretty simple Bellman equation:&lt;/p&gt;

&lt;p&gt;$$ S(x,y,z) = s(x,y,z) + \frac{1 - \delta}{1 + r} E_{z&amp;rsquo;|z} [ max(S(x,y,z&amp;rsquo;), 0) ] $$&lt;/p&gt;

&lt;p&gt;where $x$ is the worker&amp;rsquo;s type; $y$ the firm&amp;rsquo;s type; $z$ the aggregate (stochastic) state of the economy; $\delta$ the probability that a current job is going to be destroyed next period for exogenous reasons and $r$ the interest rate. The function $s(x,y,z)$ captures the value of net output when a worker of type $x$ works with a firm of type $y$ when the state of the economy is $z$. If the job is not productive enough, workers and firms decide to separate, which happens when $S(x,y,z) &amp;lt; 0$. This is why there is a max operator within the expectation operator.&lt;/p&gt;

&lt;p&gt;What we want to do is to use an ANN such that:&lt;/p&gt;

&lt;p&gt;$$ ANN(x,y,z|\theta) = s(x,y,z) + \frac{1 - \delta}{1 + r} E_{z&amp;rsquo;|z} [ ANN(x,y,z&amp;rsquo;|\theta), 0) ] $$&lt;/p&gt;

&lt;p&gt;Note that this equation can be rewritten as:&lt;/p&gt;

&lt;p&gt;$$ ANN(x,y,z|\theta) - s(x,y,z) - \frac{1 - \delta}{1 + r} E_{z&amp;rsquo;|z} [ ANN(x,y,z&amp;rsquo;|\theta), 0) ] = 0$$&lt;/p&gt;

&lt;p&gt;Our approximation is not going to be 100 percent perfect. But we can hope to find a good value of $\theta$ by minimizing the Mean Squared Error&lt;/p&gt;

&lt;p&gt;$$ \frac{1}{n}\sum(y_i - \hat{y}_i)^2 $$&lt;/p&gt;

&lt;p&gt;where $y_i$ is 0 everywhere and $\hat{y}_i = ANN(x_i,y_i,z_i|\theta) - \frac{1 - \delta}{1 + r} E_{z_i&amp;rsquo;|z_i} [ ANN(x_i,y_i,z_i&amp;rsquo;|\theta), 0) ] - s(x_i,y_i,z_i)$. Here the index $i$ captures the idea that we are going to draw several points from the space $X \times Y \times Z$&lt;/p&gt;

&lt;h3 id=&#34;ii-a-preliminaries&#34;&gt;II. A. Preliminaries&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s load useful packages and define a structure (using &lt;a href=&#34;https://github.com/mauro3/Parameters.jl&#34; target=&#34;_blank&#34;&gt;Parameters.jl&lt;/a&gt;) to hold parameter values and primitive functions of the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Random
using DataFrames
using DataStructures
using Statistics
using Plots
using IterTools
using Flux
using Flux.Data: DataLoader
using ProgressMeter
using Parameters
using Expectations
using Distributions
using Surrogates
using LinearAlgebra
using LaTeXStrings
using Interpolations
using BenchmarkTools
using Parameters
using StatsPlots
using Kronecker
using SparseArrays
using CUDA
gr()

if CUDA.has_cuda()
    device = gpu
    @info &amp;quot;GPU available&amp;quot;
else
    device = cpu
    @info &amp;quot;GPU not available&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Info: GPU not available
└ @ Main In[120]:30
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@with_kw struct Params
    x_min::Float64 = 0.0
    x_max::Float64 = 1.0
    y_min::Float64 = 0.0
    y_max::Float64 = 1.0
    nx::Int64 = 21 # grid points for human capital
    ny::Int64 = 21 # grid points for firm productivity
    dt::Float64 = 1.0/52.0 # length of a period in years
    r::Float64 = (1.0 + 0.05)^dt - 1.0 # interest rate
    delta::Float64 = 0.0127012273 #job destrution rate
    epsilon::Float64 = 0.001 # distance to stay away from bounds of the interval
    x_grid = collect(range(x_min + epsilon, x_max - epsilon, length = nx))
    y_grid = collect(range(y_min + epsilon, y_max - epsilon, length = nx))
    # log of aggregate shocks are AR(1):
    # ln(zt) = rho*ln(zt-1) + e_k
    # with e_k distributed as N(0,psi^2)
    # and psi = sigma*sqrt(1-rho²)
    rho::Float64 = 0.9997369438 #persitence parameter
    sigma::Float64 = 0.0714488990 #volatility parameter
    psi::Float64 = sigma*sqrt(1.0 - rho^2) #std dev. of innovation term
    distrib_innovation::Any = Normal(0, psi) #0 mean and std = psi
    nb_nodes::Int64 = 10 #number nodes for the expectation
    E::Any = expectation(distrib_innovation, n = nb_nodes) #expectation operator
    nodes_E = nodes(E) #nodes for expectation
    weigths_E = weights(E) #weights for expectation
    f0::Float64     =    6.0873503685 # market production parameter
    f1::Float64     =    0.0025488557 # market production parameter
    f2::Float64     =    2.0529182143 # market production parameter
    f3::Float64     =    -0.1400252578 # market production parameter
    f4::Float64     =    8.0349795180 # market production parameter
    f5::Float64     =    -1.9072145913 # market production parameter
    f6::Float64     =    6.5961298483 # market production parameter
    b0::Float64     = 0.7 # home production parameter
    p_xyz::Function = (x, y, z) -&amp;gt; f0*z*(f1 + f2*x + f3*y + f4*(x^2) + f5*(y^2) + f6*x*y)*dt #value of market production
    b_x::Function = (x) -&amp;gt; b0*p_xyz(x, x, 1.0) #value of market production
    s_xyz::Function = (x,y,z) -&amp;gt; p_xyz(x,y,z) - b_x(x) #surplus
    # VECTORIZED FUNCTIONS
    # input of the form:
    # row: observation
    # column: dimension
    p_xyz_v::Function = x -&amp;gt; f0.*x[:,3].*(f1 .+ f2.*x[:,1] .+ f3.*x[:,2] .+ f4.*(x[:,1].^2) .+ f5.*(x[:,2].^2) .+ f6.*x[:,1].*x[:,2]).*dt #value of market production
    b_x_v::Function = x -&amp;gt; b0.*f0.*1.0.*(f1 .+ f2.*x[:,1] .+ f3.*x[:,1] .+ f4.*(x[:,1].^2) .+ f5.*(x[:,1].^2) .+ f6.*x[:,1].*x[:,1]).*dt #value of market production
    s_xyz_v::Function = x -&amp;gt; p_xyz_v(x) .- b_x_v(x) #surplus
    # VECTORIZED FUNCTIONS
    # input of the form:
    # row: dimension
    # column: observation
    p_xyz_r::Function = x -&amp;gt; f0.*x[3, :].*(f1 .+ f2.*x[1, :] .+ f3.*x[2,:] .+ f4.*(x[1,:].^2) .+ f5.*(x[2,:].^2) .+ f6.*x[1,:].*x[2,:]).*dt #value of market production
    b_x_r::Function = x -&amp;gt; b0.*f0.*1.0.*(f1 .+ f2.*x[1,:] .+ f3.*x[1,:] .+ f4.*(x[1,:].^2) .+ f5.*(x[1,:].^2) .+ f6.*x[1,:].*x[1,:]).*dt #value of market production
    s_xyz_r::Function = x -&amp;gt; p_xyz_r(x) .- b_x_r(x) #surplus
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p = Params();
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ii-b-value-function-iteration&#34;&gt;II. B. Value function iteration&lt;/h3&gt;

&lt;p&gt;To check the accuracy of the ANN approach, we will also use another &amp;ldquo;legacy&amp;rdquo; method to find another approximation for $S(x,y,z)$. The traditional way is to use &lt;strong&gt;Value Function Iteration (VFI)&lt;/strong&gt;. For a detailed treatment, the textbook of &lt;a href=&#34;https://www.google.com/search?q=Recursive+methods+in+economic+dynamics&amp;amp;client=ubuntu&amp;amp;hs=q3T&amp;amp;sa=X&amp;amp;channel=fs&amp;amp;sxsrf=AOaemvLsI7n9lOLie8RSVfOPi0l2weq7Pg:1641306887162&amp;amp;tbm=isch&amp;amp;source=iu&amp;amp;ictx=1&amp;amp;fir=GH2dBz2E9vFPDM%252CzAM-vlBogjsh7M%252C%252Fm%252F0cgvb6c&amp;amp;vet=1&amp;amp;usg=AI4_-kQJc5H3CV04Yfys38GSbnPN50KV5A&amp;amp;ved=2ahUKEwi-xOfPqJj1AhWySvEDHWK1BeUQ_B16BAgbEAI&amp;amp;biw=1232&amp;amp;bih=626&amp;amp;dpr=1.5#imgrc=GH2dBz2E9vFPDM&#34; target=&#34;_blank&#34;&gt;Stockey and Lucas&lt;/a&gt; is a must-read. For explanations with codes, I suggest &lt;a href=&#34;https://julia.quantecon.org/dynamic_programming/optgrowth.html&#34; target=&#34;_blank&#34;&gt;QuantEcon&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;VFI, as suggested by the name, is an iterated procedure that converges to the true function. One starts by assuming a function $V^{(0)}$. Then one applies the &amp;ldquo;right-hand-side operator&amp;rdquo; of the Bellman equation to get $V^{(1)}$:&lt;/p&gt;

&lt;p&gt;$ V^{(1)}(x,y,z) = s(x,y,z) + \frac{1 - \delta}{1 + r} E_{z&amp;rsquo;|z} [ max(V^{(0)}(x,y,z&amp;rsquo;), 0) ]$&lt;/p&gt;

&lt;p&gt;In practice, one must calculate $V^{(n)}(x,y,z)$ on a grid. Off-grid points can be obtained by linear interpolation. After some time, the distance between $V^{(n)}(x,y,z)$ and $V^{(n-1)}(x,y,z)$ is small and the procedure may be stopped.&lt;/p&gt;

&lt;h4 id=&#34;grid-considerations-unconditional-distribution-of-z&#34;&gt;Grid considerations: unconditional distribution of z?&lt;/h4&gt;

&lt;p&gt;As explained above, to use VFI, one must define a grid. For the values of $x$ and $y$, representing workers&amp;rsquo; and firms&amp;rsquo; heterogeneity, any bounded interval could do the job. Let&amp;rsquo;s follow the original paper and assume that both $x$ and $y$ are in $[0,1]$.&lt;/p&gt;

&lt;p&gt;Defining a good grid for $z$ requires a bit more work. In the model of &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/aer.20131118&#34; target=&#34;_blank&#34;&gt;Lise and Robin 2017&lt;/a&gt;, the log (natural logarithm) of the aggregate state $z$ follows an AR(1) process:&lt;/p&gt;

&lt;p&gt;$log(z_{t}) = \rho log(z_{t-1}) + \epsilon_{t}$&lt;/p&gt;

&lt;p&gt;with $\epsilon_{t}$ normally distributed with variance $\psi \equiv \sigma \times \sqrt(1.0 - \rho^2)$.&lt;/p&gt;

&lt;p&gt;This implies that the unconditional distribution of $log(z)$ is normally distributed with mean 0 and variance $\sigma$. Or said differently, $z$ is $Lognormal(0, \sigma)$. In the block of code below, we check this fact by comparing some simulated data and the pdf of a $Lognormal(0, \sigma)$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Nt = 20000000 #number of draws
d = Normal(0, p.psi)
innov = rand(d, Nt)
log_z = zeros(Nt)
z = zeros(Nt)
log_z[1] = 1.0
for t=2:Nt
    log_z[t] = p.rho*log_z[t-1] + innov[t]
end
z = exp.(log_z)
d_log_normal = LogNormal(0.0, p.sigma)

density(z, label=&amp;quot;data&amp;quot;)
plot!(minimum(z):0.01:maximum(z), x -&amp;gt; pdf(d_log_normal, x), label=&amp;quot;LogNormal&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_2_11_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the time, $z$ is going to be between the 5th and the 95th percentile of $Lognormal(0, \sigma)$. Hence, we define the grid for $z$ to be $[P5, P95]$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;P5 = quantile(d_log_normal, 0.05)
P95 = quantile(d_log_normal, 0.95)
println(&amp;quot;P5: $(P5); P95: $(P95)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;P5: 0.8891200789438004; P95: 1.1247074761689282
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Sampling for training
n_samples_xy = 10; #number of draws for the x and y dimensions
n_samples_z = 10; #number of draws for the z dimension
lower_bound = [0.0, 0.0, P5]
upper_bound = [1.0, 1.0, P95]

x_grid = collect(range(lower_bound[1], upper_bound[1], length=n_samples_xy))
y_grid = collect(range(lower_bound[2], upper_bound[2], length=n_samples_xy))
z_grid = collect(range(lower_bound[3], upper_bound[3], length=n_samples_z))

nodes_xyz = (x_grid, y_grid, z_grid, ); #for package Interpolations
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;value-function-iteration&#34;&gt;Value function iteration&lt;/h4&gt;

&lt;p&gt;The following block of code uses the VFI algorithm and displays the final results. The value function $S(x,y,z)$ is a non-linear transformation of the value of net output of the job $s(x,y,z)$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;V_old = zeros(n_samples_xy, n_samples_xy, n_samples_z)
V_new = zeros(n_samples_xy, n_samples_xy, n_samples_z)

# Initial guess
for (zIndex, zValue) in enumerate(z_grid)
    for (yIndex, yValue) in enumerate(y_grid)
        for (xIndex, xValue) in enumerate(x_grid)
            V_old[xIndex, yIndex, zIndex] = p.s_xyz(xValue, yValue, zValue) + ((1.0 - p.delta)/(1.0 + p.r))*max(0.0, p.s_xyz(xValue, yValue, zValue))
        end
    end
end

itp = interpolate(nodes_xyz, V_old, Gridded(Linear()))
etp = extrapolate(itp, Line())
V_old_interpolated = (x) -&amp;gt; etp(x[1], x[2], x[3])

max_iter = 5000
tol = 10^-8

@time begin
    # Initialize
    for i = 1:max_iter

        for (zIndex, zValue) in enumerate(z_grid)
            for (yIndex, yValue) in enumerate(y_grid)
                for (xIndex, xValue) in enumerate(x_grid)
                    V_new[xIndex, yIndex, zIndex] = p.s_xyz(xValue, yValue, zValue) + ((1.0 - p.delta)/(1.0 + p.r)).*sum(p.weigths_E.*[max.(0.0, V_old_interpolated([xValue; yValue; (zValue.^p.rho).*exp.(innovation)])) for innovation in p.nodes_E])
                end
            end
        end

        # DISTANCE
        diff= maximum(abs.(V_new .- V_old))
        if mod(i, 100) == 0
            println(&amp;quot;Iter $(i) Diff : $(diff)&amp;quot;)
        end

        if diff &amp;lt; tol
            println(&amp;quot;Iter $(i) Convergence reached&amp;quot;)
            break
        end

        #UPDATE
        V_old = copy(V_new)
        itp = interpolate(nodes_xyz, V_old, Gridded(Linear()))
        etp = extrapolate(itp, Line())
        V_old_interpolated = (x) -&amp;gt; etp(x[1], x[2], x[3])

    end
end

# Plot output for trained neural network
p1 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; V_old_interpolated([x; y; 1.0])[1], label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(L&amp;quot;S(x,y,z=1)&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)

# Initialization function
p2 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; p.s_xyz(x, y, 1.0), label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(L&amp;quot;s(x,y,z=1)&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)

p3 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; V_old_interpolated([x; y; 1.0])[1], label = &amp;quot;f(x)&amp;quot;, st=:contour)
title!(L&amp;quot;S(x,y,z=1)&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)

p4 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; p.s_xyz(x, y, 1.0), label = &amp;quot;f(x)&amp;quot;, st=:contour)
title!(L&amp;quot;s(x,y,z=1)&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)

ratio = 9/16
width = 800
pp = plot(p1, p2, p3, p4, size = (width, width*ratio))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Iter 100 Diff : 0.18062930117157094
Iter 200 Diff : 0.04545706889411605
Iter 300 Diff : 0.011441422819899572
Iter 400 Diff : 0.002880196260427681
Iter 500 Diff : 0.0007251468880724588
Iter 600 Diff : 0.00018259564639322434
Iter 700 Diff : 4.598479092265961e-5
Iter 800 Diff : 1.1582340917470901e-5
Iter 900 Diff : 2.9176678424391866e-6
Iter 1000 Diff : 7.350753179480307e-7
Iter 1100 Diff : 1.8521812705785123e-7
Iter 1200 Diff : 4.6675602050072484e-8
Iter 1300 Diff : 1.1763880536364013e-8
Iter 1312 Convergence reached
 43.861711 seconds (266.61 M allocations: 12.326 GiB, 3.34% gc time, 0.15% compilation time)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_2_16_1.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ii-c-bellman-ann&#34;&gt;II.C. Bellman - ANN&lt;/h3&gt;

&lt;p&gt;When doing value function iteration, one does not have to vectorize the code. However, I have learned the hard-way that it matters quite a lot when switching to the ANN approach, especially if you aim to use a GPU. So let&amp;rsquo;s tackle this problem right now.&lt;/p&gt;

&lt;p&gt;The trick is to realize that the expectation operator is just a large weighted average, which can be represented by a matrix multiplication when grid points are chosen carefully.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider all the value of $S(x,y,z)$ from the grid, arranged in a large row vector of dimension $(1; n_x \times n_y \times n_z)$ : $$ \small \begin{pmatrix}
S(x_1,y_1, z_1) &amp;amp; S(x_2,y_1, z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_{nx},y_1, z_1) &amp;amp; S(x_{1},y_2, z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_{nx},y_{ny}, z_{nz}) \end{pmatrix}$$&lt;/p&gt;

&lt;p&gt;Using this convention, one can verify that the initial Bellman equation can be written as:&lt;/p&gt;

&lt;p&gt;$$ \small \begin{pmatrix}
S(x_1,y_1, z_1) &amp;amp; S(x_2,y_1, z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_{nx},y_{ny}, z_{nz})
\end{pmatrix} = \begin{pmatrix}s(x_1,y_1, z_1) &amp;amp; s(x_2,y_1, z_1) &amp;amp; &amp;hellip; &amp;amp; s(x_{nx},y_{ny}, z_{nz}) \end{pmatrix} + \frac{1 - \delta}{1 + r} \begin{pmatrix}
S(x_1,y_1, z_1&amp;rsquo;|z_1) &amp;amp; S(x_1,y_1, z_2&amp;rsquo;|z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_1,y_1, z_{nz&amp;rsquo;}|z_1) &amp;amp; S(x_2,y_1, z_{1&amp;rsquo;}|z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_{nx},y_{ny}, z_{nz&amp;rsquo;}|z_{nz}) \end{pmatrix} \times W
$$&lt;/p&gt;

&lt;p&gt;where $W$ is large (sparse) matrix of dimension $(n_x \times n_y \times n_z \times n_{z&amp;rsquo;}; n_x \times n_y \times n_z)$ containing the weights to approximate the expectation operator:&lt;/p&gt;

&lt;p&gt;$$ \small
W \equiv \begin{pmatrix} w1 &amp;amp; w2 &amp;amp; &amp;hellip; &amp;amp; w_{nz&amp;rsquo;} &amp;amp; 0 &amp;amp; &amp;hellip; &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; &amp;hellip; &amp;amp; 0 &amp;amp; w1 &amp;amp; w2 &amp;amp; &amp;hellip; &amp;amp; w_{nz&amp;rsquo;} &amp;amp; 0 &amp;amp; &amp;hellip; 0 \\ 0 &amp;amp; 0 &amp;amp; &amp;hellip; &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; &amp;hellip; &amp;amp; 0 &amp;amp; w1 &amp;amp; w2 &amp;amp; &amp;hellip; &amp;amp; w_{nz&amp;rsquo;} \end{pmatrix}^T
$$&lt;/p&gt;

&lt;p&gt;For this vectorized approach to work, we need to define two grids:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;one grid for the &amp;ldquo;today&amp;rsquo;s part&amp;rdquo;:
$\small \begin{pmatrix} S(x_1,y_1, z_1) &amp;amp; S(x_2,y_1, z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_{nx},y_{ny}, z_{nz}) \end{pmatrix} $ and $\begin{pmatrix}s(x_1,y_1, z_1) &amp;amp; s(x_2,y_1, z_1) &amp;amp; &amp;hellip; &amp;amp; s(x_{nx},y_{ny}, z_{nz}) \end{pmatrix}$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;one grid for the &amp;ldquo;expectation part&amp;rdquo; (the value of $z$ tomorrow depends on the value today) $\small \begin{pmatrix} S(x_1,y_1, z_1&amp;rsquo;|z_1) &amp;amp; S(x_1,y_1, z_2&amp;rsquo;|z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_1,y_1, z_{nz&amp;rsquo;}|z_1) &amp;amp; S(x_2,y_1, z_{1&amp;rsquo;}|z_1) &amp;amp; &amp;hellip; &amp;amp; S(x_{nx},y_{ny}, z_{nz&amp;rsquo;}|z_{nz}) \end{pmatrix}$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also define some grid points that are not used for the gradient descent phase (the &amp;ldquo;test set&amp;rdquo;). These points allow us to verify that we are not overfitting the training sample.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;nb_dim = 3

#-------------------------------------------------------------
# Train set
#-------------------------------------------------------------
# Grid for the LHS
# Grid (1, nx * ny * nz )
grid_1 = zeros(nb_dim, n_samples_xy*n_samples_xy*n_samples_z);

index = 1
for (xValue, yValue, zValue) in product(x_grid, y_grid, z_grid)
    grid_1[1, index] = xValue
    grid_1[2, index] = yValue
    grid_1[3, index] = zValue
    index+=1
end

# Grid for the RHS (calculate expectations)
# Grid (1, nx * ny * nz * nz&#39;)
grid_2 = zeros(nb_dim, n_samples_xy*n_samples_xy*n_samples_z*p.nb_nodes);

# Populate grid_2
index = 1
for (xValue, yValue, zValue) in product(x_grid, y_grid, z_grid)
    for (innovation_index, innovation) in enumerate(p.nodes_E)
        grid_2[1, index] = xValue
        grid_2[2, index] = yValue
        grid_2[3, index] = (zValue.^p.rho).*exp.(innovation)
        index+=1
    end
end

#-------------------------------------------------------------
# Test set
#-------------------------------------------------------------
# Sampling for test
n_samples_test = 1000
grid_test_1 = zeros(nb_dim, n_samples_test);

index = 1
for (xValue, yValue, zValue) in Surrogates.sample(n_samples_test, lower_bound, upper_bound, SobolSample())
    grid_test_1[1, index] = xValue
    grid_test_1[2, index] = yValue
    grid_test_1[3, index] = zValue
    index+=1
end

grid_test_2 = zeros(nb_dim, n_samples_xy*n_samples_xy*n_samples_z*p.nb_nodes);

# Populate grid_test_2
index = 1
for (xValue, yValue, zValue) in zip(grid_test_1[1,:], grid_test_1[2,:], grid_test_1[3,:])
    for (innovation_index, innovation) in enumerate(p.nodes_E)
        grid_test_2[1, index] = xValue
        grid_test_2[2, index] = yValue
        grid_test_2[3, index] = (zValue.^p.rho).*exp.(innovation)
        index+=1
    end
end

# To calculate the expectation
W = sparse(kronecker(Matrix(I, n_samples_xy*n_samples_xy*n_samples_z, n_samples_xy*n_samples_xy*n_samples_z), p.weigths_E));
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Define the neural network layers
# Specify our model
dim_input = 3
dim_ouptut = 1

Q1 = 300;
Q2 = 100;
Q3 = 10;

activation_f = relu;

model = Chain(Dense(dim_input,Q1,activation_f),
            Dense(Q1,Q2,activation_f),
            Dense(Q2,Q3,activation_f),
            Dense(Q3,dim_ouptut,identity));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check one pass of the gradient:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;ps = Flux.params(model)
y = zeros(1, size(grid_1, 2))
s_xyz_r = transpose(p.s_xyz_r(grid_1)) #precalculate net output on grid

@time begin
    gs = gradient(() -&amp;gt; Flux.Losses.mse(model(grid_1) - s_xyz_r + ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, model(grid_2))*W, y), ps)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;initializing-the-model&#34;&gt;Initializing the model&lt;/h4&gt;

&lt;p&gt;We can start with random coefficients and try to solve for the function S(.) directly.
However, it is better to initialize the coefficients of the ANN to match a not too far-fetched guess:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;lr = 0.001 # learning rate when batchsize = 1000
opt = ADAM(lr)

epochs_training = 100 # Define the number of epochs
trainingLosses = zeros(epochs_training); # Keep track of the training progress
testLosses = zeros(epochs_training); # Test on data not used in the training

ps = Flux.params(model) #initialize weigths
p_bar = Progress(epochs_training; desc = &amp;quot;Training in progress&amp;quot;); # Creates a progress bar
showProgress = true

y = zeros(1, size(grid_1, 2))
s_xyz_r = transpose(p.s_xyz_r(grid_1)) #precalculate net output on grid
s_xyz_r_test = transpose(p.s_xyz_r(grid_test_1)) #precalculate net output on test grid

# Training loop
@time for ii in 1:epochs_training

    gs = gradient(() -&amp;gt; Flux.Losses.mse(model(grid_1) - s_xyz_r - ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, s_xyz_r), y), ps)# compute gradient
    Flux.Optimise.update!(opt, ps, gs) # update parameters


    if showProgress
        trainingLosses[ii] = Flux.Losses.mse(model(grid_1) - s_xyz_r - ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, s_xyz_r), y)
        testLosses[ii] = Flux.Losses.mse(model(grid_test_1) - s_xyz_r_test - ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, s_xyz_r_test), y)
        next!(p_bar; showvalues = [(:epochs, ii), (:logloss, log.(trainingLosses[ii])), (:loglosstest, log.(testLosses[ii]))], valuecolor = :grey)
    end

end


&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell.
│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`.
│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.
└ @ ProgressMeter /home/julien/.julia/packages/ProgressMeter/Vf8un/src/ProgressMeter.jl:620
[32mTraining in progress100%|███████████████████████████████| Time: 0:00:07[39m
[39m  epochs:       100[39m
[39m  logloss:      -6.198231731362997[39m
[39m  loglosstest:  -6.480455135391009[39m


  7.398687 seconds (285.46 k allocations: 2.889 GiB, 5.29% gc time, 5.06% compilation time)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;init_f = (x,y,z) -&amp;gt; p.s_xyz(x,y,z) .+ ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, p.s_xyz(x,y,z))

gr()
# Plot output for trained neural network
p1 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; model([x; y; 1.0])[1], label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;ANN&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)

# Plot training loss
p2 = plot(1:epochs_training, log.(trainingLosses), label = &amp;quot;Train set&amp;quot;, linewidth = 2)
plot!(1:epochs_training, log.(testLosses), label = &amp;quot;Test set&amp;quot;, linewidth = 2)
title!(&amp;quot;Log Training Loss&amp;quot;)
xlabel!(&amp;quot;Epoch&amp;quot;)
ylabel!(L&amp;quot;\log(\textrm{Loss})&amp;quot;)

# Plot training loss
# Plot output for trained neural network
p3 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; init_f(x, y, 1.0), label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;Initial Guess&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)

# Plot training loss
# Plot output for trained neural network
p4 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; model([x; y; 1.0])[1] - init_f(x, y, 1.0), label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;Initial Guess - VFI&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)


ratio = 9/16
width = 800
pp = plot(p1, p2, p3, p4, size = (width, width*ratio))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_2_24_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;solving-the-real-problem&#34;&gt;Solving the real problem&lt;/h4&gt;

&lt;p&gt;Now that the coefficients are initialized, let&amp;rsquo;s tackle the real problem:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;lr = 0.001 # learning rate
opt = ADAM(lr)

epochs_training = 1000 # Define the number of epochs
trainingLosses = zeros(epochs_training); # Keep track of the training progress
testLosses = zeros(epochs_training); # Test on data not used in the training

ps = Flux.params(model) #initialize weigths
p_bar = Progress(epochs_training; desc = &amp;quot;Training in progress&amp;quot;); # Creates a progress bar
showProgress = true

y = zeros(1, size(grid_1, 2))
s_xyz_r = transpose(p.s_xyz_r(grid_1)) #precalculate net output on grid
s_xyz_r_test = transpose(p.s_xyz_r(grid_test_1)) #precalculate net output on test grid

# Training loop
@time for ii in 1:epochs_training

    gs = gradient(() -&amp;gt; Flux.Losses.mse(model(grid_1) - s_xyz_r - ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, model(grid_2))*W, y), ps)# compute gradient
    Flux.Optimise.update!(opt, ps, gs) # update parameters

    if showProgress
        trainingLosses[ii] = Flux.Losses.mse(model(grid_1) - s_xyz_r - ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, model(grid_2))*W, y)
        testLosses[ii] = Flux.Losses.mse(model(grid_test_1) - s_xyz_r_test - ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, model(grid_test_2))*W, y)
        next!(p_bar; showvalues = [(:epochs, ii), (:logloss, log.(trainingLosses[ii])), (:loglosstest, log.(testLosses[ii]))], valuecolor = :grey)
    end

end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell.
│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`.
│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.
└ @ ProgressMeter /home/julien/.julia/packages/ProgressMeter/Vf8un/src/ProgressMeter.jl:620
[32mTraining in progress100%|███████████████████████████████| Time: 0:08:47[39m
[39m  epochs:       1000[39m
[39m  logloss:      -7.247630080617367[39m
[39m  loglosstest:  -7.022159208051979[39m


527.918020 seconds (2.05 M allocations: 380.917 GiB, 0.84% gc time, 0.08% compilation time)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Training on the full problem is succesfull, as illustrated on the graphs below. It is hard to see meaningfull differences between
VFI and the ANN methods.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;gr()
# Plot output for trained neural network
p1 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; model([x; y; 1.0])[1], label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;ANN&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)

# Plot training loss
p2 = plot(1:epochs_training, log.(trainingLosses), label = &amp;quot;Train set&amp;quot;, linewidth = 2)
plot!(1:epochs_training, log.(testLosses), label = &amp;quot;Test set&amp;quot;, linewidth = 2)
title!(&amp;quot;Log Training Loss&amp;quot;)
xlabel!(&amp;quot;Epoch&amp;quot;)
ylabel!(L&amp;quot;\log(\textrm{Loss})&amp;quot;)

# Plot training loss
# Plot output for trained neural network
p3 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; V_old_interpolated([x; y; 1.0])[1], label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;VFI&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)

# Plot training loss
# Plot output for trained neural network
p4 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; model([x; y; 1.0])[1] - V_old_interpolated([x; y; 1.0])[1], label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;ANN - VFI&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)


ratio = 9/16
width = 800
pp = plot(p1, p2, p3, p4, size = (width, width*ratio))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_2_28_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;zValue = 1.0

p1 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; model([x; y; zValue])[1], label = &amp;quot;f(x)&amp;quot;, st=:contour)
title!(&amp;quot;ANN&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)

p2 = plot(p.x_grid, p.y_grid, (x, y) -&amp;gt; V_old_interpolated([x,y, zValue]), label = &amp;quot;f(x)&amp;quot;, st=:contour)
title!(&amp;quot;VFI&amp;quot;)
xlabel!(&amp;quot;x&amp;quot;)
ylabel!(&amp;quot;y&amp;quot;)

plot(p1, p2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_2_29_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;accuracy-tests&#34;&gt;Accuracy tests&lt;/h4&gt;

&lt;p&gt;Accuracy of VFI versus ANN approaches can be assessed by looking at off-grid points.
In the present setting, VFI outperforms the ANN approach. However, I conjecture that in a high-dimensional setting the opposite would be true (in a high-dimensional setting, VFI probably would not be possible in the first place). Also, one could increase the number of epochs and get better results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#VFI
#LHS
LHS = zeros(size(grid_test_1, 2))
index = 1
for (xValue, yValue, zValue) in zip(grid_test_1[1, :], grid_test_1[2, :], grid_test_1[3,:])
    LHS[index] = V_old_interpolated([xValue; yValue; zValue])
    index+=1
end


#RHS
RHS = zeros(size(grid_test_1, 2))
index = 1
for (xValue, yValue, zValue) in zip(grid_test_1[1, :], grid_test_1[2, :], grid_test_1[3,:])
    RHS[index] = p.s_xyz(xValue, yValue, zValue) + ((1.0 - p.delta)/(1.0 + p.r)).*sum(p.weigths_E.*[max.(0.0, V_old_interpolated([xValue; yValue; (zValue.^p.rho).*exp.(innovation)])) for innovation in p.nodes_E])
    index+=1
end

#ANN
error_ANN = model(grid_test_1) - transpose(p.s_xyz_r(grid_test_1)) - ((1.0 - p.delta)/(1.0 + p.r)).*max.(0.0, model(grid_test_2)*W)
error_ANN = transpose(error_ANN)
p1 = histogram(LHS - RHS, label=&amp;quot;errors VFI&amp;quot;)
p2 = histogram(error_ANN, label=&amp;quot;errors ANN&amp;quot;)
plot(p1, p2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_2_31_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;df = DataFrame(Method = [&amp;quot;VFI&amp;quot;, &amp;quot;ANN&amp;quot;],
                MSE = [mean(LHS .- RHS).^2, mean(error_ANN).^2],
                MDSE = [median(LHS .- RHS).^2, median(error_ANN).^2],
                MAXSE = [maximum(LHS .- RHS).^2, maximum(error_ANN).^2],
                MINSE = [minimum(LHS .- RHS).^2, minimum(error_ANN).^2])

df
&lt;/code&gt;&lt;/pre&gt;

&lt;table class=&#34;data-frame&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;Method&lt;/th&gt;&lt;th&gt;MSE&lt;/th&gt;&lt;th&gt;MDSE&lt;/th&gt;&lt;th&gt;MAXSE&lt;/th&gt;&lt;th&gt;MINSE&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;String&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;p&gt;2 rows × 5 columns&lt;/p&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;td&gt;VFI&lt;/td&gt;&lt;td&gt;1.99895e-6&lt;/td&gt;&lt;td&gt;3.47266e-7&lt;/td&gt;&lt;td&gt;0.00288945&lt;/td&gt;&lt;td&gt;1.5217e-6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;td&gt;ANN&lt;/td&gt;&lt;td&gt;7.47681e-5&lt;/td&gt;&lt;td&gt;1.37991e-5&lt;/td&gt;&lt;td&gt;0.035538&lt;/td&gt;&lt;td&gt;0.00108539&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post, I showed how ANN can be used to find unknown functions satisfying a Bellman equation.&lt;/p&gt;

&lt;p&gt;The code presented here is more a &lt;strong&gt;proof-of-concept than a production-ready release&lt;/strong&gt;. A first obvious improvement is to train the ANN on a &lt;strong&gt;GPU instead of a CPU&lt;/strong&gt;. Using the free version of &lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34;&gt;Google Colab&lt;/a&gt;, I am getting an almost 10x time improvement. A second improvement would be to use more diverse set of points for the ANN. While VFI requires a fixed grid to make sense, the ANN is a grid-less method. The training phase would benefit from more mixing in the input points.&lt;/p&gt;

&lt;p&gt;As a final remark, I chose on purpose a problem which is a bit much simpler than the full general problem, in which one must also find the policy function that comes with the unknown value function. I am wondering if some of the insights described in the &lt;a href=&#34;https://www.nature.com/articles/nature16961&#34; target=&#34;_blank&#34;&gt;Alpha Go paper&lt;/a&gt; could apply in the present setting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Artificial Neural Networks as universal function approximators</title>
      <link>https://julienpascal.github.io/post/ann_1/</link>
      <pubDate>Sun, 28 Nov 2021 18:00:00 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/ann_1/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Artificial Neural networks&lt;/strong&gt; (ANN) are very trendy at the moment, and rightly so.&lt;/p&gt;

&lt;p&gt;They are being used everywhere in big tech companies. For instance, when you use Google translate, or when recommandations appear on your Netflix feed, complex artificial neural networks are being used behind the scene. Behind the success of &lt;a href=&#34;https://en.wikipedia.org/wiki/AlphaGo&#34; target=&#34;_blank&#34;&gt;Alpha Go&lt;/a&gt; at the game of &lt;a href=&#34;https://www.washingtonpost.com/news/innovations/wp/2016/03/15/what-alphagos-sly-move-says-about-machine-creativity/&#34; target=&#34;_blank&#34;&gt;Go against Lee Sedol&lt;/a&gt;, an ANN was used to identify the next best move.&lt;/p&gt;

&lt;p&gt;ANN are also increasingly being used in Economic modeling, as exemplified by two recent publications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0304393221000799&#34; target=&#34;_blank&#34;&gt;Deep learning for solving dynamic economic models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nber.org/system/files/working_papers/w26302/w26302.pdf&#34; target=&#34;_blank&#34;&gt;Financial Frictions and the Wealth Distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this blog article, I discuss the reasons behind the popularity of neural networks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spoiler alert:&lt;/strong&gt; It has to do with ANN being &lt;strong&gt;universal function approximators.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As usual, I like to include Julia code to illustrate how things work in practice. My tool of choice is &lt;a href=&#34;https://julialang.org/&#34; target=&#34;_blank&#34;&gt;Julia&lt;/a&gt; because
it is really fast and it is an increasingly popular programming language. For machine learning tasks, &lt;a href=&#34;https://github.com/FluxML/Flux.jl&#34; target=&#34;_blank&#34;&gt;Flux.jl&lt;/a&gt; is a really good option, so let&amp;rsquo;s use it as well. You can download the code &lt;a href=&#34;https://github.com/JulienPascal/ANN_Flux&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;i-some-theory&#34;&gt;I. Some theory&lt;/h2&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;If you ended up here, you probably have already some knowledge about what an artificial neural network is. So I will be brief. In a nutshell, a neural network is made of several interconnected layers. Each layer is constituted of nodes. Nodes between adjacent layers exchange information between each others. The way the nodes communicate between each others is captured by parameter values associated to each nodes.&lt;/p&gt;

&lt;p&gt;See the graph below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ANN.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 400px;&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Artificial_neural_network.svg&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Artificial_neural_network.svg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An artificial neural network mimics at a high level what the brain does. The brain is composed of neurons and neurons are connected to each others via synapses. Our brain is very good at recognizing patterns, so one might hope that an artificial neural network could be a good pattern-recognition machine.&lt;/p&gt;

&lt;p&gt;In practice, it is the case. Even better, we have some theorems that tell us that ANN are really, really good.&lt;/p&gt;

&lt;h3 id=&#34;universal-approximation-theorems&#34;&gt;Universal approximation theorems&lt;/h3&gt;

&lt;p&gt;Let me describe two important papers. Below, I reproduce some selected parts of their abstracts:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/0893608089900208&#34; target=&#34;_blank&#34;&gt;Hornik Stinchcombe and White (1989)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;This paper rigorously establishes that &lt;strong&gt;standard multilayer feedforward networks with as few as one hidden layer&lt;/strong&gt; using arbitrary squashing functions &lt;strong&gt;are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy&lt;/strong&gt;, provided sufficiently many hidden units are available. In this sense, &lt;strong&gt;multilayer feedforward networks are a class of universal approximators.&lt;/strong&gt;&amp;ldquo;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf&#34; target=&#34;_blank&#34;&gt;Barron (1993)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;&lt;strong&gt;It  is  shown  that  feedforward networks with one layer of sigmoidal nonlinearities achieve integrated  squared  error  of order  O(1/n),  where  n  is  the  number  of  nodes.&lt;/strong&gt;  [&amp;hellip;]  For the class of functions  examined  here, the approximation rate  and  the  parsimony  of  the  parameterization  of  the  networks  are
&lt;strong&gt;surprisingly  advantageous  in high-dimensional settings.&lt;/strong&gt;&amp;ldquo;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The paper by &lt;strong&gt;Hornik Stinchcombe and White (1989)&lt;/strong&gt; tells us that a very large class of functions can be approximated by an ANN with the architecture we presented above. The underlying function we aim to approximate is only required to be &amp;ldquo;Borel measurable&amp;rdquo; (from one finite dimensional space to another), which contains pretty much all the useful functions you use in Economics (continuous functions from one finite dimensional space to another are Borel measurable functions).&lt;/p&gt;

&lt;p&gt;The paper by &lt;strong&gt;Barron (1993)&lt;/strong&gt; tells us that ANN are particularly good approximators when working with many dimensions.
Said differently, ANN can help to mitigate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_dimensionality&#34; target=&#34;_blank&#34;&gt;curse of dimensionality&lt;/a&gt;. One way to understand the curse of dimensionality is that the number of points needed to approximate a function grows exponentially with the number of dimensions, not linearly. We would like to explain complex phenomenon, with many dimensions and interactions, but traditional approximation methods generally do poorly in such settings.&lt;/p&gt;

&lt;p&gt;Put together, these results tell us that ANN are very good function approximators, even when the number of dimensions is high.&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;ii-application&#34;&gt;II. Application&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s see two applications. To warm up, we will start with a smooth and nice function to approximate.
Then we will move to a more complex function.&lt;/p&gt;

&lt;h3 id=&#34;ii-a-easy-function&#34;&gt;II. A. Easy function&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s load some useful packages and define the function we would like to approximate&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Dependencies
using Flux
using Plots
using LinearAlgebra
using ProgressMeter
using Statistics
using LaTeXStrings
using Surrogates
gr()

# Define function that we would like to learn with our neural network
f(x) = x[1].^2 + x[2].^2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;f (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A function is an infinite-dimensional object. But we need a finite number of values to train our neural network.
To that, let&amp;rsquo;s create a sample of points from an interval (I use &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobol_sequence&#34; target=&#34;_blank&#34;&gt;Sobol sampling&lt;/a&gt;) and then
evaluate the value of the true function for these points.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;n_samples = 100
lower_bound = [-1.0, -1.0]
upper_bound = [1.0, 1.0]

xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())
rawInputs = xys
rawOutputs = [[f(xy)] for xy in xys] # Compute outputs for each input
trainingData = zip(rawInputs, rawOutputs);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now is the fun part of deciding the architecture of our ANN.
I choose two hidden layers. The number of nodes for the first layer is
imposed by the dimension of the input (a 2d vector), as well the dimension of
the final node (a scalar). We still have to choose the number of nodes in between.
For the first hidden layer I choose 784 nodes, and 50 for the second hidden layer.
To be fair, these choices are a bit random (I was influenced by the &lt;a href=&#34;https://fluxml.ai/Flux.jl/stable/training/training/&#34; target=&#34;_blank&#34;&gt;Flux.jl tutorial here&lt;/a&gt;). Feel free to experiment with different values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Define the neural network layers (this defines a function called model(x))
# Specify our model
dim_input = 2
dim_ouptut = 1
Q1 = 784;
Q2 = 50;

# Two inputs, one output
model = Chain(Dense(2,Q1,relu),
            Dense(Q1,Q2,relu),
            Dense(Q2,1,identity));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we define a &lt;strong&gt;loss function&lt;/strong&gt;, which measures the accuracy of the approximation.
The smaller the loss, the better. We use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34; target=&#34;_blank&#34;&gt;mean squared error&lt;/a&gt; loss function. The name of the game is to find the parameter values that minimize the loss function.
One way to do minimize the loss function is to use the &lt;strong&gt;gradient descent algorithm&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Here is an intuitive explanation of gradient descent. Imagine that you are on a top of a mountain at that there is a
lot of fog that prevents you from seeing far away. You really want to go down. What should you do?&lt;/p&gt;

&lt;p&gt;One strategy is to look at where you stand and evaluate the direction of the steepest descent in the &lt;strong&gt;neighborhood of your location&lt;/strong&gt; (you can&amp;rsquo;t see far away). Then take a step in that direction. Then repeat the process. If the mountain is &amp;ldquo;well-behaved&amp;rdquo; (it has no local minima), you will manage to go down the mountain, even though you were just using local information at every step. (Go the very bottom of this blog post to see an illustration of gradient descent on a really easy problem).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Define loss function and weights
loss(x, y) = Flux.Losses.mse(model(collect(x)), y)

lr = 0.001 # learning rate

# V1. Gradient descent
opt = Descent(lr)

# V2. ADAM
#decay = 0.9
#momentum =0.999
#opt = ADAM(lr, (decay, momentum))

epochs = 1000 # Define the number of epochs
trainingLosses = zeros(epochs);# Initialize a vector to keep track of the training progress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next is the most rewarding step: &lt;strong&gt;the training part&lt;/strong&gt;. The following block of code
does gradient descent. The function &lt;code&gt;Flux.train!&lt;/code&gt; uses all the observations
we have in our sample once. Because one iteration is not enough to reach a minimum,
we repeat the process several &lt;code&gt;epochs&lt;/code&gt;. After each epoch, we calculate the mean squared
error to see how well the model does.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;ps = Flux.params(model) #initialize weigths
p = Progress(epochs; desc = &amp;quot;Training in progress&amp;quot;); # Creates a progress bar
showProgress = true

# Training loop
@time for ii in 1:epochs

    Flux.train!(loss, ps, trainingData, opt)

    # Update progress indicator
    if showProgress
        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])
        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)
    end

end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell.
│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`.
│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.
└ @ ProgressMeter /home/julien/.julia/packages/ProgressMeter/Vf8un/src/ProgressMeter.jl:620
[32mTraining in progress100%|███████████████████████████████| Time: 0:00:24[39m
[39m  loss:     0.00017162366528025578[39m
[39m  logloss:  -3.7654228272565655[39m


 24.753884 seconds (41.00 M allocations: 37.606 GiB, 6.56% gc time, 0.48% compilation time)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next plot shows a surface plot for the original function and the value returned by the ANN (the dots).
Results are quite good. The top right graph displays the value of the loss function as the training takes place.
Gradient descent seems to work well, because the loss function decreases in a nice and monotonic manner. The bottom plot displays the surface plot for the trained ANN.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;nb_points = 100
grid_x = collect(range(lower_bound[1], upper_bound[1], length= nb_points))
grid_y = collect(range(lower_bound[2], upper_bound[2], length= nb_points))

grid_x_random = [xy[1] for xy in xys]
grid_y_random = [xy[2] for xy in xys]

Zs = [model([x,y])[1] for (x, y) in zip(grid_x_random, grid_y_random)];

# Plot output for trained neural network
p1 = plot(grid_x, grid_y, (x, y) -&amp;gt; f([x,y]), label = &amp;quot;f(x)&amp;quot;, st=:surface)
scatter!(p1, grid_x_random, grid_y_random, Zs, label=&amp;quot;ANN&amp;quot;)
title!(&amp;quot;Original function&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)


# Plot training loss
p2 = plot(1:epochs, log.(trainingLosses), label = &amp;quot;&amp;quot;, linewidth = 2)
title!(&amp;quot;Training Loss&amp;quot;)
xlabel!(&amp;quot;Epoch&amp;quot;)
ylabel!(L&amp;quot;\log(\textrm{Loss})&amp;quot;)

# Neural network
p3 = plot(grid_x, grid_y, (x, y) -&amp;gt; model([x,y])[1], label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;Trained Neural Network&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)

ratio = 9/16
width = 800
pp = plot(p1, p2, p3, size = (width, width*ratio))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_1_12_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ii-b-more-challenging-function&#34;&gt;II.B. More challenging function&lt;/h3&gt;

&lt;p&gt;Ok, so our ANN works with a simple function, which is reassuring.
Let&amp;rsquo;s now turn to a more challenging function. For instance,
we can try to approximate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Ackley_function&#34; target=&#34;_blank&#34;&gt;Ackley function&lt;/a&gt;, which
is a slightly crazy function often used to test minimization algorithms (it has a global minimum at the origin).&lt;/p&gt;

&lt;p&gt;Even with a more complex function, our ANN does a great job a approximating the true function, as you can see on the graph below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function ackley(x; e = exp(1), a = 10.0, b = -0.2, c=2.0*π)
    #a, b, c = 10.0, -0.2, 2.0*π
    len_recip = inv(length(x))
    sum_sqrs = zero(eltype(x))
    sum_cos = sum_sqrs
    for i in x
        sum_cos += cos(c*i)
        sum_sqrs += i^2
    end
    return -a * exp(b * sqrt(len_recip*sum_sqrs)) - exp(len_recip*sum_cos) + a + e
end

n_samples = 1000
lower_bound = [-2.0, -2.0]
upper_bound = [2.0, 2.0]
xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())
rawInputs = xys

rawOutputs = [[ackley(xy)] for xy in xys] # Compute outputs for each input
trainingData = zip(rawInputs, rawOutputs);

# Define the neural network layers (this defines a function called model(x))
# Specify our model
Q1 = 784;
Q2 = 50;
Q3 = 10;

# Two inputs, one output
model = Chain(Dense(2,Q1,relu),
            Dense(Q1,Q2,relu),
            Dense(Q2,1,identity));

# Define loss function and weights
loss(x, y) = Flux.Losses.mse(model(collect(x)), y)
ps = Flux.params(model)

# Train the neural network
epochs = 1000
showProgress = true
lr = 0.001 # learning rate

# V1. Gradient descent
opt = Descent(lr)

# V2. ADAM
#decay = 0.9
#momentum =0.999
#opt = ADAM(lr, (decay, momentum))

trainingLosses = zeros(epochs) # Initialize vectors to keep track of training
p = Progress(epochs; desc = &amp;quot;Training in progress&amp;quot;) # Creates a progress bar

@time for ii in 1:epochs

    Flux.train!(loss, ps, trainingData, opt)

    # Update progress indicator
    if showProgress
        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])
        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)
    end

end


&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell.
│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`.
│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.
└ @ ProgressMeter /home/julien/.julia/packages/ProgressMeter/Vf8un/src/ProgressMeter.jl:620
[32mTraining in progress100%|███████████████████████████████| Time: 0:04:02[39m
[39m  loss:     0.01753345824641543[39m
[39m  logloss:  -1.7561324165239636[39m


242.064635 seconds (407.63 M allocations: 375.931 GiB, 6.81% gc time, 0.04% compilation time)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;nb_points = 100
grid_x = collect(range(lower_bound[1], upper_bound[1], length= nb_points))
grid_y = collect(range(lower_bound[2], upper_bound[2], length= nb_points))

grid_x_random = [xy[1] for xy in xys]
grid_y_random = [xy[2] for xy in xys]

Zs = [model([x,y])[1] for (x, y) in zip(grid_x_random, grid_y_random)];

# Plot output for trained neural network
p1 = plot(grid_x, grid_y, (x, y) -&amp;gt; ackley([x,y]), label = &amp;quot;f(x)&amp;quot;, st=:surface)
# Show somes points, not all of them (otherwise hard to see anything)
scatter!(p1, grid_x_random[1:100], grid_y_random[1:100], Zs[1:100], label=&amp;quot;ANN&amp;quot;)
title!(&amp;quot;Original Function&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)


# Plot training loss
p2 = plot(1:epochs, log.(trainingLosses), label = &amp;quot;&amp;quot;, linewidth = 2)
title!(&amp;quot;Training Loss&amp;quot;)
xlabel!(&amp;quot;Epoch&amp;quot;)
ylabel!(L&amp;quot;\log(\textrm{Loss})&amp;quot;)


# Plot output for trained neural network
p3 = plot(grid_x, grid_y, (x, y) -&amp;gt; model([x,y])[1], label = &amp;quot;f(x)&amp;quot;, st=:surface)
title!(&amp;quot;Trained Neural Network&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;y&amp;quot;)

ratio = 9/16
width = 800
pp = plot(p1, p2, p3, size = (width, width*ratio))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_1_15_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Artificial Neural Network are universal function approximators. This blog post showed how to start using ANN to approximate relatively simple functions.&lt;/p&gt;

&lt;p&gt;When trying to solve an economic model, one often has to find an unknown function that satisfies a number of
restrictions. In a subsequent post, I wish to describe how an ANN can be used to find such an unknown function.&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;extra-gradient-descent-visually&#34;&gt;Extra: gradient descent visually&lt;/h3&gt;

&lt;p&gt;Below is an illustration of gradient descent.
We want to find the minimum of the function &lt;code&gt;J(x)=x^2&lt;/code&gt; and we start
at the point &lt;code&gt;-20&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The algorithm proceeds iteratively:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calculate the &lt;strong&gt;gradient&lt;/strong&gt; at the current value. This gives us the direction
of the maximum change for the function &lt;code&gt;J&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Because we are looking for a &lt;strong&gt;minimum&lt;/strong&gt; and not a maximum, take a step in the opposite
direction of the maximum change&lt;/li&gt;
&lt;li&gt;Repeat steps 1-2&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using GradDescent
#Code from here: https://jacobcvt12.github.io/GradDescent.jl/stable/
#I made only slight modifications to the original code

# objective function and gradient of objective function
J(x) = x^2
dJ(x) = 2*x

# number of epochs
epochs = 150

# instantiation of Adagrad optimizer with learning rate of 2
opt = Adagrad(η=2.0)

# initial value for x (usually initialized with a random value)
x = -20.0 #initial position on the function
values_x = zeros(epochs) #initialization
value_y = zeros(epochs) #initialization
iter_x = zeros(epochs) #initialization

for i in 1:epochs
    # Save values for plotting
    values_x[i] = x
    value_y[i] = J(x)
    iter_x[i] = i

    # calculate the gradient wrt to the current x
    g = dJ(x)

    # change to the current x
    δ = update(opt, g)
    x -= δ
end

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see on the plot below, we start from the left hand side and then we make some quite big moves
to the right. As time passes, the points go from yellow to darker colors.
After about 150 iterations, we are very close to the true minimum at 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(values_x, value_y, label=&amp;quot;J(x)&amp;quot;)
scatter!(values_x, value_y, marker_z = iter_x, color = cgrad(:thermal, rev = true), label=&amp;quot;Position&amp;quot;, colorbar_title=&amp;quot;Iteration&amp;quot;)
xlabel!(L&amp;quot;x&amp;quot;)
ylabel!(L&amp;quot;J(x)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;ANN_1_20_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Young&#39;s method (2010) to simulate a cross-section</title>
      <link>https://julienpascal.github.io/post/young_2010/</link>
      <pubDate>Mon, 18 Jan 2021 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/young_2010/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Solving economic models involves (i) finding the &lt;strong&gt;optimal response of individuals&lt;/strong&gt; given the state of the economy (the policy functions); (ii) given the policy functions, &lt;strong&gt;simulating the model&lt;/strong&gt;. While usually one must show great ingenuity and creativity for the former, the latter is often seen as trivial and not even mentioned. However, in this notebook I describe a simulation procedure that deserves to be advertised. Namely, I describe &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v34y2010i1p36-41.html&#34; target=&#34;_blank&#34;&gt;Young&amp;rsquo;s method (2010)&lt;/a&gt; to simulate a large number (infinity) of individuals. The code can be downloaded &lt;a href=&#34;https://github.com/JulienPascal/Simulate_Cross-section&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;why-should-we-care&#34;&gt;Why should we care?&lt;/h2&gt;

&lt;p&gt;In economies with heterogeneous agents, often there is no such thing as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Representative_agent#:~:text=Economists%20use%20the%20term%20representative,the%20same%20type%20are%20identical.&#34; target=&#34;_blank&#34;&gt;representative agent&lt;/a&gt;. Generally, one must follow a large number of individuals. For instance, one may be interested in knowing how the average wage responds to an increase in labor productivity. If workers are different (in terms of skills, experience, firms in which they work, etc.), one must take into consideration changes in individuals&amp;rsquo; wages to determine how the average wage moves.&lt;/p&gt;

&lt;h2 id=&#34;the-method&#34;&gt;The method&lt;/h2&gt;

&lt;p&gt;The method avoids simulating a panel of agents. Instead, the idea is to simulate directly the distribution.
In practice, one chooses a grid $[w_1, w_2, &amp;hellip;, w_N]$&lt;/p&gt;

&lt;p&gt;If a measure $m$ of workers choose to consume $w$, with $w \in [w_{n}, w_{n+1}]$, then the mass assigned to the grid point $w_{n}$ is equal to $m \times p$ and the mass assigned to the grid point $w_{n+1}$ is $m \times (1 - p)$ with&lt;/p&gt;

&lt;p&gt;$$ p = 1 - \frac{w - w_{n}}{w_{n+1} - w_{n}} $$&lt;/p&gt;

&lt;p&gt;If $w$ is very close to $w_{n}$, then most of the mass $m$ will be assigned to this point. In the limit case, if $w$ is equal to $w_{n}$, $100$ percent of the mass is assigned to $w_{n}$.&lt;/p&gt;

&lt;p&gt;Simple, right? The code below is an implementation of &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v34y2010i1p36-41.html&#34; target=&#34;_blank&#34;&gt;Young&amp;rsquo;s method (2010)&lt;/a&gt; using &lt;a href=&#34;https://julialang.org/&#34; target=&#34;_blank&#34;&gt;Julia&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;h3 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s first start by loading a few packages and declaring auxiliaries functions. In particular, given a value $w$ we need a function that returns the closest value $w_k$, where $w_k$ is picked from given grid $w_1, w_2, &amp;hellip;, w_N$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Plots
using Distributions
using StatsBase

# Matlab like function
function linspace(z_start::Real, z_end::Real, z_n::Int64)
    return collect(range(z_start,stop=z_end,length=z_n))
end

# Not my function. Credit to: https://discourse.julialang.org/t/findnearest-function/4143/4
function closest_index(a::Vector,x::Real)

    if isempty(a) == true
      error(&amp;quot;xGrid is empty in function closest_index.&amp;quot;)
    end

    if isnan(x) == true
      error(&amp;quot;val is NaN in function closest_index.&amp;quot;)
    end

   idx = searchsortedfirst(a,x)
   if (idx==1); return idx; end
   if (idx&amp;gt;length(a)); return length(a); end
   if (a[idx]==x); return idx; end
   if (abs(a[idx]-x) &amp;lt; abs(a[idx-1]-x))
      return idx
   else
      return idx-1
   end
end

# Returns best index and best value
function closest_value_and_index(xGrid::Vector, val::Real)

    # get index
    ibest = closest_index(xGrid, val)

    # Return best value on grid, and the corresponding index
    return xGrid[ibest], ibest

end

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;approximate-a-distribution-with-a-single-mass-point&#34;&gt;Approximate a distribution with a single mass point&lt;/h3&gt;

&lt;p&gt;To warm up, let&amp;rsquo;s see how the method works when the true underlying distribution is constituted of &lt;strong&gt;a single mass point&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;## True Value
w = 2.5    #True value
mass = 1.0 #Mass at the true value

## Approximation
nW=10 #Number of grid points
w_grid=linspace(0.0, 4.0, nW) #Location of grid points
hw_grid=zeros(nW); #Initialization

w_min = minimum(w_grid) #Upper bound for the grid
w_max = maximum(w_grid) #Lower bound for the grid
nW = length(w_grid) #Number of points on the grid

# Project true value on the grid:
(wValue_proj, wIndex_proj) = closest_value_and_index(w_grid, w)

# To store the location of the value below and above the true value:
wIndex_below = 0
wIndex_above = 0

# If the true value is above the projection
if w &amp;gt;= wValue_proj
    wIndex_below = wIndex_proj
    wIndex_above = wIndex_proj + 1
# If the true value is below the projection
elseif w &amp;lt; wValue_proj
    wIndex_below = wIndex_proj -1
    wIndex_above = wIndex_proj
end

# Boundary cases
if wIndex_proj == 1
    wIndex_below = 1
    wIndex_above = 2
elseif wIndex_proj == nW
    wIndex_below = nW - 1
    wIndex_above = nW
end

# Special case 1: w &amp;lt; w_min
if w &amp;lt;= w_min
    p = 1
elseif w &amp;gt;= w_max
# Special case 2: w &amp;gt; w_max
    p = 0
else
    p = 1.0 - ((w - w_grid[wIndex_below])/(w_grid[wIndex_above] - w_grid[wIndex_below]))
    p = min(1.0, max(0.0, p))
end

# Spread the mass:
# 1. Point below
hw_grid[wIndex_below] += p*mass

# 2. Point above:
hw_grid[wIndex_above] += (1.0 - p)*mass;

p0 =bar(w_grid, hw_grid, label = &amp;quot;Mass on grid&amp;quot;)
plot!(p0, [w], seriestype = :vline, label=&amp;quot;True value&amp;quot;, linewidth = 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Young_2010_12_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the true distribution (in orange) and the approximation (in blue).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Because the true value is not a point of our grid (that would be a miracle), the mass is spread between the two closest points. However, we still get the mean right. We will see below that this property extends to distributions with supports constituted of several points:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;println(&amp;quot;True mean $(w)&amp;quot;)
println(&amp;quot;Approximate mean $(round(mean(w_grid, weights(hw_grid./sum(hw_grid))), digits = 4))&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;True mean 2.5
Approximate mean 2.5
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;approximate-a-multi-point-distribution&#34;&gt;Approximate a multi-point distribution&lt;/h3&gt;

&lt;p&gt;Now, let&amp;rsquo;s see how well &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v34y2010i1p36-41.html&#34; target=&#34;_blank&#34;&gt;Young&amp;rsquo;s method (2010)&lt;/a&gt; works when the true underlying distribution has positive mass for &lt;strong&gt;a finite number of points&lt;/strong&gt;. To keep things pretty, let&amp;rsquo;s assume that the true distribution has the shape of a Normal distribution, but we truncate the tails.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Nw_true = 10 #Number of points with positive mass
w_true_lb = 1.0 #lower bound
w_true_ub = 3.0 #upper bound
w_true = linspace(w_true_lb, w_true_ub, Nw_true)
d = truncated(Normal((w_true_lb + w_true_ub)/2), w_true_lb, w_true_ub)
mass_true = pdf.(d, w_true)
mass_true = mass_true./sum(mass_true);
p0 = bar(w_true, mass_true, label=&amp;quot;Mass true value&amp;quot;)
display(p0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Young_2010_17_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the mass for each point on the (true) grid.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;w_min = minimum(w_grid)
w_max = maximum(w_grid)
nW = length(w_grid)

hw_grid=zeros(nW);


for (wIndexTrue, w) in enumerate(w_true)

    mass = mass_true[wIndexTrue] #mass

    # Project true value on the grid:
    (wValue_proj, wIndex_proj) = closest_value_and_index(w_grid, w)

    # To store the location of the value below and above the true value:
    wIndex_below = 0
    wIndex_above = 0

    # If the true value is above the projection
    if w &amp;gt;= wValue_proj
        wIndex_below = wIndex_proj
        wIndex_above = wIndex_proj + 1
    # If the true value is below the projection
    elseif w &amp;lt; wValue_proj
        wIndex_below = wIndex_proj -1
        wIndex_above = wIndex_proj
    end

    # Boundary cases
    if wIndex_proj == 1
        wIndex_below = 1
        wIndex_above = 2
    elseif wIndex_proj == nW
        wIndex_below = nW - 1
        wIndex_above = nW
    end

    # Special case 1: w &amp;lt; w_min
    if w &amp;lt;= w_min
        p = 1
    elseif w &amp;gt;= w_max
    # Special case 2: w &amp;gt; w_max
        p = 0
    else
        p = 1.0 - ((w - w_grid[wIndex_below])/(w_grid[wIndex_above] - w_grid[wIndex_below]))
        p = min(1.0, max(0.0, p))
    end

    p = 1.0 - ((w - w_grid[wIndex_below])/(w_grid[wIndex_above] - w_grid[wIndex_below]))
    p = min(1.0, max(0.0, p))

    # Spread the mass:
    # 1. Point below
    hw_grid[wIndex_below] += p*mass

    # 2. Point above:
    hw_grid[wIndex_above] += (1.0 - p)*mass

end

bar(w_grid, hw_grid, label=&amp;quot;Mass on grid&amp;quot;)
bar!(w_true, mass_true, label=&amp;quot;Mass true value&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Young_2010_18_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the true distribution (in orange) and the approximation (in blue).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A good property of &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v34y2010i1p36-41.html&#34; target=&#34;_blank&#34;&gt;Young&amp;rsquo;s method (2010)&lt;/a&gt; is that, &lt;strong&gt;as long as the grid is wide enough&lt;/strong&gt; to so that true values fall within it, &lt;strong&gt;the mean of the true underlying distribution is preserved&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;println(&amp;quot;True mean $(round(mean(w_true, weights(mass_true./sum(mass_true))), digits = 4))&amp;quot;)
println(&amp;quot;Approximate mean $(round(mean(w_grid, weights(hw_grid./sum(hw_grid))), digits = 4))&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;True mean 2.0
Approximate mean 2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, there are some &lt;strong&gt;approximation errors&lt;/strong&gt; when &lt;strong&gt;higher moments&lt;/strong&gt; are involved such as the variance, or when calculating &lt;strong&gt;percentiles&lt;/strong&gt;. But the finest the grid, the better the approximation gets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;println(&amp;quot;True median $(round(quantile(w_true, weights(mass_true./sum(mass_true)), 0.5), digits = 4))&amp;quot;)
println(&amp;quot;Approximate median $(round(quantile(w_grid, weights(hw_grid./sum(hw_grid)), 0.5), digits = 4))&amp;quot;)

println(&amp;quot;True variance $(round(var(w_true, weights(mass_true./sum(mass_true))), digits = 4))&amp;quot;)
println(&amp;quot;Approximate variance $(round(var(w_grid, weights(hw_grid./sum(hw_grid))), digits = 4))&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;True median 1.9567
Approximate median 1.8519
True variance 0.3465
Approximate variance 0.3836
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-a-panel-of-agents&#34;&gt;Using a panel of agents&lt;/h3&gt;

&lt;p&gt;Alternatively, one may use a large number of agents to approximate the true underlying distribution. The idea is that if the number of agents is large enough, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Central_limit_theorem&#34; target=&#34;_blank&#34;&gt;central limit theorem&lt;/a&gt; will kick in. The issue is that we need a large number of agents to get the approximation right, as illustrated below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;nb_agents = 1000
w_agents = rand(d, nb_agents);

println(&amp;quot;True mean $(round(mean(w_true, weights(mass_true./sum(mass_true))), digits = 4))&amp;quot;)
println(&amp;quot;Approximate mean $(round(mean(w_agents), digits = 4))&amp;quot;)

println(&amp;quot;True median $(round(quantile(w_true, weights(mass_true./sum(mass_true)), 0.5), digits = 4))&amp;quot;)
println(&amp;quot;Approximate median $(round(quantile(w_agents, 0.5), digits = 4))&amp;quot;)

println(&amp;quot;True variance $(round(var(w_true, weights(mass_true./sum(mass_true))), digits = 4))&amp;quot;)
println(&amp;quot;Approximate variance $(round(var(w_agents), digits = 4))&amp;quot;)

p0 = bar(w_true, mass_true, label=&amp;quot;Mass true value&amp;quot;)
p1 = histogram(w_agents, label=&amp;quot;Mass panel&amp;quot;)
plot(p0,p1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;True mean 2.0
Approximate mean 1.9955
True median 1.9567
Approximate median 1.9971
True variance 0.3465
Approximate variance 0.2754
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Young_2010_25_1.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the true distribution (on the left) and the approximation (on the right).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With 1000 agents, the approximation is not exceptional. Let&amp;rsquo;s try to increase the number of agents. The following plot shows that the approximation gets better as we increase the number of agents, but a very
large number of agents is needed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;w_agents = []
grid_agents = collect(1000:1000:100000)
mean_agents = zeros(length(grid_agents))
std_agents = zeros(length(grid_agents))

for (i, nb_agents) in enumerate(grid_agents)
    push!(w_agents, rand(d, nb_agents))
    mean_agents[i] = mean(w_agents[i])
    std_agents[i] = std(w_agents[i])
end

CI = 1.960.*std_agents./sqrt.(grid_agents); #95% confidence interval. t-test is approximately a z-test because large number of agents.
p0 = plot(grid_agents, mean_agents, ribbon = CI, label = &amp;quot;approximate mean&amp;quot;)
plot!(p0,[mean(w_true, weights(mass_true./sum(mass_true)))], linetype = :hline, label = &amp;quot;true mean&amp;quot;, linestyle = :dash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Young_2010_27_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the empirical mean calculated using the panel and the true value, for different sizes of the panel.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;When possible, simulating directly a distribution instead of using a panel is a good idea. &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v34y2010i1p36-41.html&#34; target=&#34;_blank&#34;&gt;Young&amp;rsquo;s method (2010)&lt;/a&gt;
allows to do that, while preserving the mean of the true distribution.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Young, Eric R. &amp;ldquo;Solving the incomplete markets model with aggregate uncertainty using the Krusell–Smith algorithm and non-stochastic simulations.&amp;rdquo; Journal of Economic Dynamics and Control 34.1 (2010): 36-41.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Aiyagari Model with Aggregate Uncertainty</title>
      <link>https://julienpascal.github.io/post/aiyagariaggregateuncertainty/</link>
      <pubDate>Tue, 19 May 2020 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/aiyagariaggregateuncertainty/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The Bewley-Huggett-Aiyagari-Imohoroğlu economies are the workhorse of modern macroeconomics. In these economies, markets are &amp;ldquo;incomplete&amp;rdquo;. Agents cannot fully insure against risk and decide to self-insure by holding a safe asset to smooth their consumption (see &lt;a href=&#34;https://mitpress.mit.edu/books/recursive-macroeconomic-theory-fourth-edition&#34; target=&#34;_blank&#34;&gt;Ljungqvist and Sargent (2018)&lt;/a&gt; for a textbook treatment of this topic).&lt;/p&gt;

&lt;p&gt;In this post, I consider the model of Aiyagari (1994). While the original model abstracts from aggregate fluctuations, Economists have since developed several techniques to simulate out-of-steady-state dynamics for this class of models.&lt;/p&gt;

&lt;p&gt;Here, I use a methodology that is quite general. It is a &lt;strong&gt;3-step procedure&lt;/strong&gt;, which proceeds as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Solve for the non-stochastic steady-state&lt;/li&gt;
&lt;li&gt;Perturbe the model around its non-stochastic steady-state&lt;/li&gt;
&lt;li&gt;Use the perturbation to calculate out-of-steady-state dynamics&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I use the &lt;a href=&#34;https://julienpascal.github.io/post/genbkm/&#34; target=&#34;_blank&#34;&gt;BKM and GenBKM&lt;/a&gt; algorithms for step 2 and 3, which means that the only theoretical tool needed is &lt;strong&gt;backward induction&lt;/strong&gt; (i.e. knowing the value tomorrow, what is the value today?).&lt;/p&gt;

&lt;p&gt;What are the cons of the methodology presented here? First, the methodology assumes a &lt;strong&gt;small aggregate shock&lt;/strong&gt;. An implicit assumption is that aggregate shocks do not modify the value of the non-stochastic steady-state. Said differently, the cycle does not change the trend, &lt;a href=&#34;https://www.youtube.com/watch?v=K2X1GiTaxHw&#34; target=&#34;_blank&#34;&gt;which seems to be violated in some instances&lt;/a&gt;. If the shock is large, the value of steady-state may be altered and the methodology presented here is not adequate. An example of a large shock could be the disruption caused by COVID-19.&lt;/p&gt;

&lt;p&gt;This methodology also fails when the non-stochastic steady-state is not relevant for the dynamic economy. This can problematic in portfolio choice problems, in which portfolios are indeterminate when aggregate uncertainty vanishes (see &lt;a href=&#34;https://hal-sciencespo.archives-ouvertes.fr/hal-00972801/document&#34; target=&#34;_blank&#34;&gt;Coeurdacier et al (2011)&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;On the pro side, the methodology presented here is fast (orders of magnitude faster than &lt;a href=&#34;https://notes.quantecon.org/submission/5bb58d1e11611400157fdc8d&#34; target=&#34;_blank&#34;&gt;Krusell-Smith (1998)&lt;/a&gt;) and transparent. The code can be &lt;a href=&#34;https://github.com/JulienPascal/AiyagariAggregateUncertainty&#34; target=&#34;_blank&#34;&gt;downloaded here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;i-the-model&#34;&gt;I. The Model&lt;/h2&gt;

&lt;p&gt;In the model of Aiyagari (1994), there is continuum of agents each maximizing an infinite flow of discounted utility:&lt;/p&gt;

&lt;p&gt;$$ E_{0} \sum_{t=0}^{\infty} \beta^t U(c_t) $$&lt;/p&gt;

&lt;p&gt;subject to the constraint that consumption plus savings in period $t$ (the left hand side of the next equation) is equal to labor earnings plus savings inherited from the last period (the right hand side of the next equation):&lt;/p&gt;

&lt;p&gt;$$ c_t + a_{t+1} = w_t l_t + (1 + r_t) a_t $$&lt;/p&gt;

&lt;p&gt;The variable $l_t$ captures idiosyncratic risk in labor earnings and could be interpreted as unemployment risk. We also make the assumptions that consumption cannot be negative and that agents cannot borrow more than a certain amount $B$:&lt;/p&gt;

&lt;p&gt;$$ c_t \geq 0 $$&lt;/p&gt;

&lt;p&gt;$$ a_t \geq -B $$&lt;/p&gt;

&lt;p&gt;The behavior of firms can be summarized by a representative firm hiring workers and capital:&lt;/p&gt;

&lt;p&gt;$$ Y_t = z_t K_t^{\alpha} L_t^{1-\alpha} $$&lt;/p&gt;

&lt;p&gt;where $Y_t$ is total output, $K_t$ is the aggregate capital level and $L_t$ is the aggregate labor supply. The variables $w_t$ and $r_t$ are pinned down each period by the first order conditions (FOCs) of the representative firm. Note that at the non-stochastic steady-state $z_t = z_{SS} = 1$ (by definition) and both $w_t$ and $r_t$ are constant. Importantly, because agents have to take into consideration the value of $w_t$ and $r_t$ when making decisions, the cross-sectional distribution of agents across capital and idiosyncratic states matters (through the FOCs of the representative firm).&lt;/p&gt;

&lt;h2 id=&#34;ii-methodology&#34;&gt;II. Methodology&lt;/h2&gt;

&lt;p&gt;To solve for individual policy functions, I use the endogenous grid method (EGM) of Carroll (2006). The main idea of this method is to start from the end-of-period level of capital. Using the Euler equation, one may recover the beginning-of-period consumption and level of capital without using a root-finding algorithm.&lt;/p&gt;

&lt;p&gt;To determine the non-stochastic equilibrium, I solve for the fixed-point problem over the aggregate capital level $f(K*) = 0$ using &lt;a href=&#34;https://en.wikipedia.org/wiki/Brent%27s_method&#34; target=&#34;_blank&#34;&gt;Brent&amp;rsquo;s method&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To calculate the response of the economy to one-period unforeseen aggregate shock (an &amp;ldquo;MIT shock&amp;rdquo;), I use a standard backward-forward &lt;a href=&#34;https://en.wikipedia.org/wiki/Shooting_method&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;shooting&amp;rdquo; method&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;holding constant the path of aggregate capital level $(K_t)_{t=1}^{T}$, calculate the policy functions&lt;/li&gt;
&lt;li&gt;holding constant the policy functions, calculate the path of aggregate capital level $(K_t)_{t=1}^{T}$&lt;/li&gt;
&lt;li&gt;repeat until convergence of the path for aggregate capital level $(K_t)_{t=1}^{T}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To simulate out-of-steady-state dynamics, I use the &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v89y2018icp68-92.html&#34; target=&#34;_blank&#34;&gt;BKM algorithm&lt;/a&gt;, which relies on the assumption that the response of the economy to an aggregate shock $d_t$ is &lt;strong&gt;linear&lt;/strong&gt; with respect to the &lt;strong&gt;aggregate state&lt;/strong&gt; $z_t$:&lt;/p&gt;

&lt;p&gt;$$ d_t = z_t d(1, 0, 0, &amp;hellip;) + z_{t-1}d(0, 1, 0, &amp;hellip;) + z_{t-2}d(0, 0, 1, &amp;hellip;) + &amp;hellip; $$&lt;/p&gt;

&lt;p&gt;or more compactly:&lt;/p&gt;

&lt;p&gt;$$ d_t = \sum_{k=0}^{+\infty} z_{t-k} d_{k} $$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$$ d_{1} = d(1,0,0,&amp;hellip;)$$
$$ d_{2} = d(0,1,0,&amp;hellip;)$$
$$ d_{3} = d(0,0,1,&amp;hellip;)$$&lt;/p&gt;

&lt;p&gt;That is, the evolution of equilibrium variables is a moving average of past shocks. However, by calculating several trajectories after an MIT shock, one sees that linearity assumption is slightly violated. Hence, I use the &lt;a href=&#34;https://irihs.ihs.ac.at/id/eprint/4500/&#34; target=&#34;_blank&#34;&gt;GenBKM algorithm&lt;/a&gt;, which is a refinement of the BKM algorithm taking into consideration these slight violations of linearity.&lt;/p&gt;

&lt;h2 id=&#34;iii-implementation&#34;&gt;III. Implementation&lt;/h2&gt;

&lt;p&gt;These ideas are implemented using &lt;a href=&#34;https://julialang.org/&#34; target=&#34;_blank&#34;&gt;Julia&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;versioninfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Julia Version 1.3.0
Commit 46ce4d7933 (2019-11-26 06:09 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-6.0.1 (ORCJIT, skylake)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ii-a-dependencies&#34;&gt;II. A Dependencies&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions
using Plots
using DataFrames
using Random
using ForwardDiff
using LinearAlgebra
using Interpolations
using DataFrames
using Optim
Random.seed!(1234);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using NBInclude #To load stuct and functions from other notebooks
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@nbinclude(&amp;quot;utils.ipynb&amp;quot;) #mutable structs and primitive functions for the model
@nbinclude(&amp;quot;RBC.ipynb&amp;quot;)   #Aiyagari model, without the borrowing constraint (standard RBC)
@nbinclude(&amp;quot;EGM.ipynb&amp;quot;)   #implementation of the EGM method
@nbinclude(&amp;quot;SteadyState.ipynb&amp;quot;) #to calculate the non-stochastic steady-state
@nbinclude(&amp;quot;GenBKM.ipynb&amp;quot;) #to simulate the stochastic model using the GenBKM algorithm
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ii-b-steady-state&#34;&gt;II.B Steady-state&lt;/h3&gt;

&lt;h4 id=&#34;finding-the-steady-state-value-of-capital&#34;&gt;Finding the steady-state value of capital&lt;/h4&gt;

&lt;p&gt;Finding the non-stochastic equilibrium is a fixed-point problem over the aggregate capital level $f(K*) = 0$, which can be solved using &lt;a href=&#34;https://en.wikipedia.org/wiki/Brent%27s_method&#34; target=&#34;_blank&#34;&gt;Brent&amp;rsquo;s method&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p = Params() #struct with model parameters
z_ss = 1.0 #aggregate productivity at the non-stochastic steady-state
@time oo = optimize(K -&amp;gt; eq_K(K,p), 10, 100, Brent()) #solve for the steady-state value of capital using Brent method
K_star = oo.minimizer;
println(&amp;quot;Steady-state value of capital K* = $(K_star)&amp;quot;)
# Store the optimal policy function at the steady-state
g_star, c_star, g_low_star, g_high_star, success_flag= solve_EGM(x-&amp;gt;log(x), x-&amp;gt;log(x), R(K_star, z_ss, p), W(K_star, z_ss, p), p); #solve for policy functions
# Store the stationary distribution at the steady-state
t_star = make_trans_mat(g_star, p)    #generate transition matrix
d_star = get_stationary_dist(t_star); #stationary distribution
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;visualizing-transition-probabilities&#34;&gt;Visualizing transition probabilities&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;xs = [string(&amp;quot;x&amp;quot;, i) for i = 1:size(t_star,1)]
ys = [string(&amp;quot;y&amp;quot;, i) for i = 1:size(t_star,2)]
heatmap(t_star, aspect_ratio = 1, color=:plasma, clim=(0., 0.25))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_15_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the transition probabilities across capital and idiosyncratic probability states.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;visualizing-convergence-toward-the-steady-state&#34;&gt;Visualizing convergence toward the steady-state&lt;/h4&gt;

&lt;p&gt;One may visually check that the equilibrium exists and is unique:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Solve for the demand and supply of capital for different values of the interest rate
K_grid = collect(range(oo.minimizer-0.5, stop=oo.minimizer+0.5, length=20))
K_implied_grid = similar(K_grid)
R_grid = similar(K_grid)

for (K_index, K_value) in enumerate(K_grid)
    R_s, W_s = R(K_value, z_ss, p), W(K_value, z_ss, p) #calculate interest rate R and wage W
    gg, c_star, g_low, g_high, success_flag= solve_EGM(x -&amp;gt; log(x), x -&amp;gt; 2*log(x), R_s, W_s, p) #solve for policy functions
    tt = make_trans_mat(gg, p)   #generate transition matrix
    dd = get_stationary_dist(tt) #stationary distribution
    K_implied = aggregate_K(dd, p) #implied level of capital
    R_grid[K_index] = R_s #store interest rate
    K_implied_grid[K_index] = K_implied #store demand of capital
    K_grid[K_index] = K_value #store supply of capital
end

# Plot demand and supply of capital
plot(K_grid, R_grid, label = &amp;quot;Demand&amp;quot;, ylabel=&amp;quot;Interest rate&amp;quot;, xlabel=&amp;quot;Capital&amp;quot;)
plot!(K_implied_grid, R_grid, label = &amp;quot;Supply&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_18_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the demand and the supply of capital as a function of the interest rate.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;ii-c-mit-shock&#34;&gt;II.C MIT Shock&lt;/h3&gt;

&lt;h4 id=&#34;backward-and-forward-updates&#34;&gt;Backward and forward updates&lt;/h4&gt;

&lt;p&gt;The next block implements the backward-forward shooting method:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;holding the path of aggregate capital $(K_t)_{t=1}^{T}$, calculate the policy functions&lt;/li&gt;
&lt;li&gt;holding constant the policy functions, calculate the aggregate capital $(K_t)_{t=1}^{T}$&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function backward_update(g_low_ss::Function, g_high_ss::Function, K_path_guess::Array{Float64,1}, z_path::Array{Float64,1}, p::Params)
    &amp;quot;&amp;quot;&amp;quot;
    Update policy functions backward, holding {K_t,z_t} constant
    &amp;quot;&amp;quot;&amp;quot;
    nT = length(z_path)
    g_low_path = Array{Function}(undef,nT) #initialize two lists of functions
    g_high_path =  Array{Function}(undef,nT)
    g_low_path[nT] = g_low_ss
    g_high_path[nT] = g_high_ss
    a_path = zeros(p.nI, p.grid_size, nT) #to store policy functions on savings grid
    R_path = zeros(nT) #to store the interest rate on path
    W_path = zeros(nT) #to store the wage on path

    #Start from the steady-state and iterate backward
    #holding constant the path for {K_t,z_t}
    #---------------------------------------------------
    for t=nT👎2 #iterate backward
        # Next period
        R_path[t], W_path[t] = R(K_path_guess[t], z_path[t], p), W(K_path_guess[t], z_path[t], p)
        # Current period
        R_path[t-1], W_path[t-1] = R(K_path_guess[t-1], z_path[t-1], p), W(K_path_guess[t-1], z_path[t-1], p)
        # Current period&#39;s policy, given next period
        a_path[:,:,t-1], c_new, g_low_path[t-1], g_high_path[t-1] = euler_back(g_low_path[t], g_high_path[t], R_path[t-1], W_path[t-1], R_path[t], W_path[t], p)
    end

    return a_path, g_low_path, g_high_path
end

function forward_update(K_star::Float64, a_path::Array{Float64,3}, d_ss::Array{Float64,1}, p::Params)
    &amp;quot;&amp;quot;&amp;quot;
    Update forward the distribution of agents + aggregate capital
    dd_path_forward, K_path_forward
    &amp;quot;&amp;quot;&amp;quot;
    nT = length(z_path)
    K_path_forward = zeros(nT)
    K_path_forward[1] = K_star
    dd_path_forward = zeros(size(d_ss,1), nT)
    dd_path_forward[:,1] = d_ss
    #2. Iterate forward {K_t,z_t}, using the policy
    #functions from step 1
    #-----------------------------------------------
    for t=2:nT
        tt = make_trans_mat(a_path[:,:,t-1], p) #generate transition matrix
        dd_path_forward[:,t] = tt*dd_path_forward[:,t-1]
        K_path_forward[t] = aggregate_K(dd_path_forward[:,t], p)
    end

    return dd_path_forward, K_path_forward
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;finding-the-transition-path&#34;&gt;Finding the transition path&lt;/h4&gt;

&lt;p&gt;One problem with the backward-forward shooting method is that updating the path for $(K_t)_{t=1}^{T}$ &amp;ldquo;too quickly&amp;rdquo; may result in the overall procedure to diverge. An easy fix is to take a convex combination of the previous guess and the newly calculated path, with $\lambda$ small:&lt;/p&gt;

&lt;p&gt;$(K^{NEW}_t)_{t=1}^{T} = \lambda (K_t)_{t=1}^{T} + (1-\lambda)(K^{OLD}_t)_{t=1}^{T}$&lt;/p&gt;

&lt;p&gt;The next function implements this idea, with the extra feature that $\lambda$ increases when the distance between two iterations is getting small (too speed up convergence) and decreases when the distance is getting bigger (to prevent divergence). See &lt;a href=&#34;https://notes.quantecon.org/submission/5b3faf1fb9eab00015b89f9a&#34; target=&#34;_blank&#34;&gt;this excellent notebook&lt;/a&gt; for the same idea applied to an OLG model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function solve_mit!(K_path, g_low_ss::Function, g_high_ss::Function, d_ss::Array{Float64,1},
                    K_ss::Float64, z_path::Array{Float64,1}, p::Params; convex_combination::Float64 = 0.2,
                    shrink_factor::Float64 = 0.5, expand_factor::Float64 = 1.05,
                    max_iter::Int64 = 1000, tol::Float64=1e-6, verbose::Bool=true, display_iter::Int64 = 20)
    &amp;quot;&amp;quot;&amp;quot;
    Finds the path for aggregate capital K_path
    &amp;quot;&amp;quot;&amp;quot;
    diff = Inf #initialization
    diff_old = Inf #initialization
    convergence_flag = 0 #initialization
    damp = convex_combination #initial dampening parameter

    for i_mit=1:max_iter

        # Step 1. Solve backward the policy functions {g_t(a,e_low), g_t(a,e_high)}, keeping {K_t,z_t} constant:
        a_path, g_low_path, g_high_path = backward_update(g_low_ss, g_high_ss, K_path[i_mit], z_path, p);

        #2. Solve forward {K_t,z_t}, keeping policy functions {g_t(a,e_low), g_t(a,e_high)} constant:
        dd_path_forward, K_path_forward = forward_update(K_ss, a_path, d_star, p);

        # Distance between guess for {K_t} and implied values:
        diff = maximum(abs.(K_path_forward - K_path[i_mit]))

        # Display every display_iter iterations
        if verbose==true
            if mod(i_mit,display_iter) == 0
                println(&amp;quot;Iteration $(i_mit). diff = $(diff)&amp;quot;)
            end
        end

        if diff &amp;lt; tol
            if verbose==true
                println(&amp;quot;Convergence reached after $(i_mit) iterations.&amp;quot;)
            end
            convergence_flag = 1
            break
        else
            # Update the guess for the path {K_t}
            # Decrease the dampening factor
            if diff &amp;gt; diff_old
                damp = max(min(damp * shrink_factor, 1.0-eps()), eps())
            # Increase the dampening factor
            else
                damp = max(min(damp * expand_factor, 1.0-eps()), eps())
            end
            if mod(i_mit, 10) == 0
                if verbose==true
                    println(&amp;quot;damp = $(damp); diff = $(diff)&amp;quot;)
                end
            end
            # Store the updated path for {K_t}
            push!(K_path, damp.*K_path_forward .+ (1.0 - damp).*K_path[i_mit])
            diff_old = diff

        end
    end

    return K_path, convergence_flag
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;find-the-path-for-k-t-for-a-1-std-dev-positive-productivity-shock&#34;&gt;Find the path for {K_t} for a 1 std. dev positive productivity shock&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;max_t = 300 #Let us assume that the economy is back to the steady state after max_t periods
z_ss = 1.0  #Value of aggregate productivity at the non-stochastic steady-state
z_shock = 2.0 #Value of the inital shock

# Let&#39;s generate a path for the aggregate shock
z_path = ones(max_t)
z_path[1] = z_ss*z_shock #initial shock

# Evolution of aggregate productivity in level:
for t_index=2:max_t
    z_path[t_index] = z_path[t_index-1]^p.rho
end

# Heroic guess for the initial path of {K_t}: K_t = K* for all t
K_path = []
push!(K_path, repeat([K_star], max_t))

# Find the path for {K_t}:
@time K_path, convergence_flag = solve_mit!(K_path, g_low_star, g_high_star, d_star, K_star, z_path, p, convex_combination=0.25);

# Find the path for other aggregates:
R_path = zeros(length(z_path)) #to store the interest rate on path
W_path = zeros(length(z_path)) #to store the wage on path
for t=length(z_path)👎1 #iterate backward
    # Next period
    R_path[t], W_path[t] = R(K_path[end][t], z_path[t], p), W(K_path[end][t], z_path[t], p)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;visualize-convergence-of-the-transition-path&#34;&gt;Visualize convergence of the transition path&lt;/h4&gt;

&lt;p&gt;The first guess for $(K_t)_{t=1}^{T}$ is that it is equal to the non-stochastic steady-state value $K*$. Very quickly, the path for $(K_t)_{t=1}^{T}$ converges to the perfect foresight transition path:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p0 = plot(1:max_t, K_path[1], label= &amp;quot;Iteration 0&amp;quot;, title=&amp;quot;Convergence of K(t)&amp;quot;)
plot!(p0, 2:max_t, K_path[2][2:end], label = &amp;quot;Iteration 1&amp;quot;)
show_every = 5 #display {K_t} for each multiple of show_every
for k in 2:length(K_path)
    if mod(k,show_every) == 0
        plot!(p0, 2:max_t, K_path[k][2:end], xlabel=&amp;quot;t&amp;quot;, label = &amp;quot;Iteration $(k)&amp;quot;, title=&amp;quot;Convergence of K(t)&amp;quot;, legend=:best)
    end
end

p0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_30_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the path for capital {K_t}^(i) for different iterations of the backward-forward algorithm&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;removing-the-borrowing-constraint&#34;&gt;Removing the borrowing constraint&lt;/h4&gt;

&lt;p&gt;The next graph compares the current model to a similar model without a borrowing constraint. With no borrowing constraint, the aggregate level of capital reacts less to an aggregate shock in productivity:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p1 = plot(1:max_t, K_path[end], label= &amp;quot;K_t Aiyagari&amp;quot;, title=&amp;quot;IRF Aiyagari versus RBC&amp;quot;)
plot!(p1, xx[RBCp.iK,2:end] .+ K_star, label = &amp;quot;K_t RBC&amp;quot;, color = &amp;quot;black&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_34_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the impulse response of K_t of for the Aiyagari model and a RBC model.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p1 = plot(1:max_t, z_path./z_path[end] .-1, label = &amp;quot;z(t)&amp;quot;, xlabel= &amp;quot;t&amp;quot;)
p2 = plot(1:max_t, R_path./R_path[end] .-1, label= &amp;quot;R(t)&amp;quot;, xlabel= &amp;quot;t&amp;quot;)
p3 = plot(1:max_t, W_path./W_path[end].-1 , label= &amp;quot;W(t)&amp;quot;, xlabel= &amp;quot;t&amp;quot;)
p4 = plot(1:max_t, K_path[end]./K_path[end][end] .-1, label= &amp;quot;K(t)&amp;quot;, xlabel= &amp;quot;t&amp;quot; )

p5 = plot(p1, p2, p3, p4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_36_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the percentage deviation from steady-state values of (i) aggregate productivity (ii) the interest rate (iii) wages (iv) capital.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;linearity-checks&#34;&gt;Linearity checks&lt;/h4&gt;

&lt;p&gt;To simulate the stochastic economy, the BKM algorithm makes the assumption that an MIT shock is linear with
respect to the aggregate shock. That is, doubling the initial shock will simply double the value of aggregates
along the transition path without changing the shape of the path. The next block
calculates several transition paths for different initial aggregate shocks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;max_t = 300 #Let us assume that the economy is back to the steady state after max_t periods
z_ss = 1.0  #Value of aggregate productivity at the non-stochastic steady-state
# Different initial shocks
array_sigma = collect(range(-0.75, stop=0.75, step=0.25))
# Let&#39;s exclude sigma = 0
array_sigma = array_sigma[array_sigma .!= 0.]
# To store the different scaled IRF:
x_mit_scaled_sigma = zeros(max_t, length(array_sigma))
# To store path of aggregate productivity:
z_path_sigma = zeros(max_t, length(array_sigma))
# To store the path for the %deviation of aggregate productivity from its steady-state value
z_path_sigma_dev = zeros(max_t, length(array_sigma))

for (index_sigma, sigma) in enumerate(array_sigma)

    # Let&#39;s generate a path for the aggregate shock
    z_path = ones(max_t)
    z_path[1] = z_ss + z_ss*sigma

    # Evolution of aggregate productivity in level:
    for t_index=2:max_t
        z_path[t_index] = z_path[t_index-1]^p.rho
    end

    # Heroic guess for the initial path of {K_t}: K_t = K* for all t
    K_path = []
    push!(K_path, repeat([K_star], max_t))

    # Find the path for {K_t}:
    @time K_path, convergence_flag = solve_mit!(K_path, g_low_star, g_high_star, d_star, K_star, z_path, p, convex_combination=0.2, verbose=false);

    # Check for convergence
    if convergence_flag!=1
        error(&amp;quot;No convergence for z(1) = $(z_path[1]).&amp;quot;)
    end

    # store the path for z:
    z_path_sigma[:, index_sigma] = z_path

    # store for the %deviation of aggregate productivity from its steady-state value
    z_path_sigma_dev[:, index_sigma] = z_path./z_ss .- 1.0

    # Scaled IRF: how a percentage deviation in z_t from its steady-state results in a % deviation of k_t
    x_mit_scaled_sigma[:, index_sigma] = (K_path[end]./K_star .- 1.0)./z_path_sigma_dev[1, index_sigma]

end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p0 = plot()
p1 = plot()
p2 = plot()
p3 = plot()

for (index_sigma, sigma) in enumerate(array_sigma)
    if index_sigma == 1
        p0 = plot(100 .*z_path_sigma_dev[:, index_sigma], label=&amp;quot;z(t) with z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;)
        p1 = plot(z_path_sigma[:, index_sigma], label=&amp;quot;z(t) z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;)
        p2 = plot(x_mit_scaled_sigma[:, index_sigma], label=&amp;quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;)
        p3 = plot(sign(z_path_sigma[1, index_sigma] - z_ss)*x_mit_scaled_sigma[:, index_sigma], label=&amp;quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;)
    else
        plot!(p0, 100 .*z_path_sigma_dev[:, index_sigma], label=&amp;quot;z(t) with z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;, title = &amp;quot;% deviation aggregate productivity&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
        plot!(p1, z_path_sigma[:, index_sigma], label=&amp;quot;z(t) with z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;, title = &amp;quot;Aggregate productivity&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
        plot!(p2, x_mit_scaled_sigma[:, index_sigma], label=&amp;quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;, title = &amp;quot;Impact of shock&#39;s size on scaled IRF: x&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
        plot!(p3, sign(z_path_sigma[1, index_sigma] - z_ss)*x_mit_scaled_sigma[:, index_sigma], label=&amp;quot;z(1) = $(round(100 .*z_path_sigma_dev[1, index_sigma], digits=2))%&amp;quot;, title = &amp;quot;Impact of shock&#39;s sign on scaled IRF: sign(x - x_ss)*x&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(p0,p1, fg_legend = :transparent, legend=:best, layout=(2,1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_42_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: This graph shows the percentage deviation of aggregate productivity from its steady-state value (top panel) and aggregate productivity in level (bottom panel) for different initial shocks.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(p2,p3, fg_legend = :transparent, legend=:best, layout=(2,1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_44_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: The top panel shows the scaled impulse response function of capital for different aggregate shocks. The bottom panel shows the scaled impulse response function of capital for different aggregate shocks, multiplied by the sign of the aggregate shock.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;ii-d-out-of-steady-state-dynamics&#34;&gt;II.D. Out-of-steady-state dynamics&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;max_t = 2000
shocks_t = rand(Normal(0,0.005), max_t) # Series of aggregate shocks
# Let&#39;s generate a path for the aggregate shock
z_path = ones(max_t)
z_path[1] = z_ss

# Evolution of aggregate productivity in level:
for t_index=2:max_t
    z_path[t_index] = z_path[t_index-1]^p.rho + shocks_t[t_index]
end

# Calculation of GenBKM path:
XT_GenBKM = zeros(max_t);# Initialization
@time GenBKM_path!(XT_GenBKM, max_t, x_mit_scaled_sigma, z_path./z_ss .- 1.0, array_sigma)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.237683 seconds (1.06 M allocations: 50.188 MiB)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p1 = plot(100 .*(z_path./z_ss .- 1.0), label=&amp;quot;z_t&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
p2 = plot(100 .*XT_GenBKM, label = &amp;quot;K_t&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
plot(p1,p2, fg_legend = :transparent, legend=:best, layout=(2,1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;AiyagariBKM_48_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: The top panel shows the percentage deviation of aggregate productivity from its steady-state value. The bottom panel shows the percentage deviation of capital from its steady-state value.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This post presents the model of Aiyagari (1994) and a general three-step procedure to simulate out-of-steady-state dynamics for models of this class for &amp;ldquo;small&amp;rdquo; shocks. Solving incomplete market models for large shocks seems to be &lt;a href=&#34;https://www.springer.com/gp/book/9783319564357&#34; target=&#34;_blank&#34;&gt;much more complicated&lt;/a&gt; and is still an active area of research.&lt;/p&gt;

&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;An excellent course on heterogeneous agent models with code in Matlab, Python and Julia: &lt;a href=&#34;https://alisdairmckay.com/Notes/HetAgents/index.html&#34; target=&#34;_blank&#34;&gt;https://alisdairmckay.com/Notes/HetAgents/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More on the EGM method: &lt;a href=&#34;https://julia.quantecon.org/dynamic_programming/egm_policy_iter.html&#34; target=&#34;_blank&#34;&gt;https://julia.quantecon.org/dynamic_programming/egm_policy_iter.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More on the Aiyagari model: &lt;a href=&#34;https://python.quantecon.org/aiyagari.html&#34; target=&#34;_blank&#34;&gt;https://python.quantecon.org/aiyagari.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Aiyagari model in continuous time: &lt;a href=&#34;https://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/aiyagari_continuous_time.ipynb&#34; target=&#34;_blank&#34;&gt;https://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/aiyagari_continuous_time.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Similar model solved very efficiently with a Python package: &lt;a href=&#34;https://github.com/shade-econ/sequence-jacobian/blob/master/krusell_smith.ipynb&#34; target=&#34;_blank&#34;&gt;https://github.com/shade-econ/sequence-jacobian/blob/master/krusell_smith.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Aiyagari, S. Rao. &amp;ldquo;Uninsured idiosyncratic risk and aggregate saving.&amp;rdquo; The Quarterly Journal of Economics 109.3 (1994): 659-684.&lt;/li&gt;
&lt;li&gt;Bewley, Truman. &amp;ldquo;A difficulty with the optimum quantity of money.&amp;rdquo; Econometrica: Journal of the Econometric Society (1983): 1485-1504.&lt;/li&gt;
&lt;li&gt;Boppart, Timo, Per Krusell, and Kurt Mitman. “Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative.” Journal of Economic Dynamics and Control 89 (2018): 68-92.&lt;/li&gt;
&lt;li&gt;Christopher D Carroll. The method of endogenous gridpoints for solving dynamic stochastic optimization problems. Economics Letters, 91(3):312–320, 2006.&lt;/li&gt;
&lt;li&gt;Coeurdacier, Nicolas, Helene Rey, and Pablo Winant. &amp;ldquo;The risky steady state.&amp;rdquo; American Economic Review 101.3 (2011): 398-401.&lt;/li&gt;
&lt;li&gt;Huggett, Mark. &amp;ldquo;The risk-free rate in heterogeneous-agent incomplete-insurance economies.&amp;rdquo; Journal of economic Dynamics and Control 17.5-6 (1993): 953-969.&lt;/li&gt;
&lt;li&gt;İmrohoroğlu, Ayşe. &amp;ldquo;The welfare cost of inflation under imperfect insurance.&amp;rdquo; Journal of Economic Dynamics and Control 16.1 (1992): 79-91.&lt;/li&gt;
&lt;li&gt;Ljungqvist, Lars, and Thomas J. Sargent. Recursive macroeconomic theory. MIT press, 2018.&lt;/li&gt;
&lt;li&gt;Reiter, Michael. “Comments on” Exploiting MIT Shocks in Heterogeneous-Agent Economies: The Impulse Response as a Numerical Derivative” by T. Boppart, P. Krusell and K. Mitman.” Journal of Economic Dynamics and Control 89 (2018): 93-99.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The GenBKM Algorithm</title>
      <link>https://julienpascal.github.io/post/genbkm/</link>
      <pubDate>Tue, 21 Apr 2020 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/genbkm/</guid>
      <description>

&lt;p&gt;In a &lt;a href=&#34;https://julienpascal.github.io/post/bkm/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; I presented the &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0165188918300022&#34; target=&#34;_blank&#34;&gt;BKM algorithm&lt;/a&gt; , which can used to approximate solutions of macroeconomic models with &lt;strong&gt;aggregate uncertainty&lt;/strong&gt; and &lt;strong&gt;heterogeneous agents&lt;/strong&gt;. This class of models has been been of great interest for Economists for quite a long time. For instance, &lt;a href=&#34;https://www.jstor.org/stable/2118417?seq=1#metadata_info_tab_contents&#34; target=&#34;_blank&#34;&gt;Aiyagari (1994)&lt;/a&gt; already hinted that taking into consideration heterogeneity along the business cycle is both theoretically important and challenging:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This class of models may also be useful in &lt;strong&gt;resolving various asset return puzzles&lt;/strong&gt;. Mehra and
Prescott [1985, p. 145] suggested that these puzzles cannot be &amp;ldquo;accounted for by models that abstract from transactions costs. This is a very hard problem computationally since the distribution of assets
across households can no longer be taken to be constant. Instead, &lt;strong&gt;the cross-section distribution is part of the state vector that evolves stochastically over time in response to aggregate shocks&lt;/strong&gt;. This is an issue that remains to be explored&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recent developments have relied on the &lt;strong&gt;sequence representation&lt;/strong&gt; of the mutli-stage decision process, instead of the traditional &lt;strong&gt;recursive form&lt;/strong&gt; using Bellman&amp;rsquo;s principle (see for instance &lt;a href=&#34;https://web.stanford.edu/~aauclert/sequence_space_jacobian.pdf&#34; target=&#34;_blank&#34;&gt;Auclert at al (2019)&lt;/a&gt; or &lt;a href=&#34;http://xavier-ragot.fr/pdf/progress/Optimal_policies.pdf&#34; target=&#34;_blank&#34;&gt;Le Grand and Ragot (2019)&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In this short blog post, I would like to present a small modification of the BKM algorithm that delivers large improvements in accuracy: the GenBKM algorithm by &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v89y2018icp93-99.html&#34; target=&#34;_blank&#34;&gt;Reiter (2018)&lt;/a&gt;. The code for this post can be found &lt;a href=&#34;https://github.com/JulienPascal/GenBKM&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;i-a-the-bkm-algorithm&#34;&gt;I.A. The BKM algorithm&lt;/h3&gt;

&lt;p&gt;It is common to use Bellman&amp;rsquo;s principle of optimality to characterize solutions of a mutli-stage decision process. The principle of optimality leads to a solution in a &lt;strong&gt;recursive form&lt;/strong&gt; $d_{t} = d(S_{t})$, where $S_t$ is a vector of state variables and d(.) a policy function describing the optimal action of a decision-maker when faced with any given state. In model with heterogeneous agents (HA) and aggregate uncertainty, $S_t$ is generally infinite-dimensional. Hence, there is a huge premium in avoiding the recursive form.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;sequence form&lt;/strong&gt; of the problem is as follows: at each step in time, a decision-maker observes a new realization of a random process $z_t$ and taking into account the full history of past shocks, the agent make a new choice to maximize her expected discounted sum of returns: $d_t = d(z_t, z_{t-1}, z_{t-2}, &amp;hellip;)$.&lt;/p&gt;

&lt;p&gt;The BKM algorithm makes the &lt;strong&gt;assumption&lt;/strong&gt; of &lt;strong&gt;linearity&lt;/strong&gt; of d(.) with respect to the &lt;strong&gt;aggregate state&lt;/strong&gt; $z_t$:&lt;/p&gt;

&lt;p&gt;$$ d_t = z_t d(1, 0, 0, &amp;hellip;) + z_{t-1}d(0, 1, 0, &amp;hellip;) + z_{t-2}d(0, 0, 1, &amp;hellip;) + &amp;hellip; $$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$$ d_{1} = d(1,0,0,&amp;hellip;)$$
$$ d_{2} = d(0,1,0,&amp;hellip;)$$
$$ d_{3} = d(0,0,1,&amp;hellip;)$$&lt;/p&gt;

&lt;p&gt;or more compactly:&lt;/p&gt;

&lt;p&gt;$$ d_t = \sum_{k=0}^{+\infty} z_{t-k} d_{k} $$&lt;/p&gt;

&lt;p&gt;The series of $d_{k}$ describes the trajectory of the economy after an &amp;ldquo;MIT&amp;rdquo; shock: the economy is hit by an aggregate shock in period 1 and then goes back directly to its steady-state value. The evolution of equilibrium variables are simply a moving average of past shocks.&lt;/p&gt;

&lt;p&gt;The BKM algorithm works well because computing an MIT shock in macroeconomic models with aggregate uncertainty and heterogeneous agents is generally feasible and fast.&lt;/p&gt;

&lt;h4 id=&#34;tl-dr&#34;&gt;TL;DR&lt;/h4&gt;

&lt;p&gt;To simulate out-of-steady-state dynamics, the BKM algorithm superposes &lt;strong&gt;scaled impulse response functions&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;i-b-bkm-example&#34;&gt;I. B. BKM example&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s illustrate how the BKM algorithm works with the following non-linear model:&lt;/p&gt;

&lt;p&gt;$$ x_{t} = a x_{t-1} + b x_{t-1}^2 + z_{t} $$&lt;/p&gt;

&lt;p&gt;The next plot illustrates that BKM algorithm does a good job at approximating the dynamic of the true
nonlinear model. But can we do better? The next section shows that the answer is yes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Dependencies
using Distributions
using Plots
using DataFrames
using Random
pgfplotsx()

# Parameters
max_iter=1000 #number of iterations for the simulation
a = 0.5
b = 0.05
sigma_shock=1.0 #variance of shocks
mu_shock=0. #mean of shocks
Random.seed!(1234)
d = Normal(mu_shock, sigma_shock)

# transition function
function iter_x(x_min1::Float64, a::Float64, b::Float64)
    &amp;quot;&amp;quot;&amp;quot;
    Function to find the next iteration of x_{t} = a x_{t-1} + b x_{t-1}^2
    x_min1::Float64: x_{t-1}
    a::Float64
    b::Float64
    &amp;quot;&amp;quot;&amp;quot;
    return a*x_min1 + b*x_min1^2
end

# Simulation of an MIT Shock
# We assume that after max_iter_mit periods, the economy is back at the steady-state
max_iter_mit = 25
x_mit=zeros(max_iter_mit)
# Initial shock
z_t=zeros(max_iter_mit)
z_t[1] = sigma_shock #a 1 std. deviation
x_mit[1] = 0 #steady-state

for i=2:max_iter_mit
    x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1]
end

# Scaled-version of the impulse response:
x_mit_scaled = x_mit./z_t[1];

# Scaled-version of the impulse response:
p0 = plot(x_mit_scaled, label=&amp;quot;x scaled&amp;quot;, xlabel=&amp;quot;t&amp;quot;)
title!(&amp;quot;MIT shock&amp;quot;)

# Function to calculate the path of xt using the BKM algorithm
function BKM_path!(XT::Array{Float64,1}, x_scaled::Array{Float64,1}, shocks::Array{Float64,1})
    &amp;quot;&amp;quot;&amp;quot;
    XT::Array{Float64,1}: array to store the evolution of the variable xt
    x_scaled::Array{Float64,1}: a scaled MIT shock
    shocks::Array{Float64,1}: sequence of shocks
    &amp;quot;&amp;quot;&amp;quot;
    # get the length of x_scaled
    len_x_scaled = length(x_scaled)
    max_iter = length(XT)

    # Loop over time periods periods
    for t=2:max_iter
        # Superposition of MIT shocks:
        for k=1:t
            # After some time, we assume that the effect of past shocks vanishes:
            if k&amp;lt;=len_x_scaled
                XT[t]+=x_scaled[k]*shocks[t-k+1]
            end
        end
    end

end

XT = zeros(max_iter) # Initialization
shocks_t = rand(d, max_iter).*0.5 # Series of shocks
@time BKM_path!(XT, x_mit_scaled, shocks_t) # Solving using BKM:
x_true = zeros(max_iter) # True value of the series
for i=2:max_iter
    x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1]
end

# Let&#39;s store statistics on error:
diff_BKM = x_true - XT
max_abs_err_BKM = maximum(abs.(diff_BKM))
min_abs_err_BKM = minimum(abs.(diff_BKM))
mean_abs_err_BKM = mean(abs.(diff_BKM))
median_abs_err_BKM = median(abs.(diff_BKM))

p1 = plot(XT[2:end], label=&amp;quot;x_t BKM&amp;quot;)
plot!(x_true[2:end], label=&amp;quot;x_t true&amp;quot;)
title!(&amp;quot;BKM with b=$(b)&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;GenBKM_7_2.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: In orange the exact value as a function of time; in blue the approximation obtained by using
the BKM algorithm&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;ii-a-the-genbkm-algorithm&#34;&gt;II. A. The GenBKM algorithm&lt;/h3&gt;

&lt;p&gt;The BKM algorithm is based on the assumption that both the &lt;strong&gt;size&lt;/strong&gt; and &lt;strong&gt;sign&lt;/strong&gt; of the initial shock does not
change the shape of the scaled impulse response function. But is really the case? The next graph shows that both the size and the sign matter for the shape of the scaled impulse response function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Different initial shocks
array_sigma = collect(range(-2, stop=2, step=0.5))
# Let&#39;s exclude sigma = 0
array_sigma = array_sigma[array_sigma .!= 0.]
# To store the different scaled IRF:
x_mit_scaled_sigma = zeros(max_iter_mit, length(array_sigma))

for (index_sigma, sigma) in enumerate(array_sigma)
    x_mit=zeros(max_iter_mit)
    # Initial shock
    z_t=zeros(max_iter_mit)
    z_t[1] = sigma #a 1 std. deviation
    x_mit[1] = 0 #steady-state

    for i=2:max_iter_mit
        x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1]
    end

    # Scaled-version of the impulse response:
    x_mit_scaled = x_mit./z_t[1];
    # store the scaled response
    x_mit_scaled_sigma[:, index_sigma] = x_mit_scaled

end


p2 = plot(x_mit_scaled_sigma[:, 1], label=&amp;quot;sigma = $(array_sigma[1])&amp;quot;)
p3 = plot(sign(array_sigma[1])*x_mit_scaled_sigma[:, 1], label=&amp;quot;sigma = $(array_sigma[1])&amp;quot;)

for (index_sigma, sigma) in enumerate(array_sigma)
    plot!(p2, x_mit_scaled_sigma[:, index_sigma], label=&amp;quot;sigma = $(sigma)&amp;quot;, title = &amp;quot;Impact of shock&#39;s size on scaled IRF: x&amp;quot;)
    plot!(p3, sign(sigma)*x_mit_scaled_sigma[:, index_sigma], label=&amp;quot;sigma = $(sigma)&amp;quot;, title = &amp;quot;Impact of shock&#39;s sign on scaled IRF: sign(x)*x&amp;quot;)
end

p4 = plot(p2, p3, layout = (2, 1))
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;GenBKM_11_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: The top panel shows the scaled impulse response function for different
values of shocks. The bottom panel shows the scaled impulse response function
multiplied by the sign of the shock. If the size and the sign of the shock did not matter, we would
see only one line. It is not the case here.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The GenBKM algorithm is the similar to the BKM algorithm, except that the impacts of the size and the sign of the
initial shock on the response of the economy are taken into consideration. It proceeds as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Divide the support of the shock into $n$ intervals $ I_i = (a_i, b_i)$&lt;/li&gt;
&lt;li&gt;Compute $n$ MIT shocks using shocks in $z_i \in I_i$, denoted by $d_k^{i}$&lt;/li&gt;
&lt;li&gt;The state of the economy at time $t$ is given by the moving average of past shocks, taking into consideration
past shock values:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$ d_t = \sum_{k=0}^{+\infty} z_{t-k} d_{k}^{f(t-k)} $$&lt;/p&gt;

&lt;p&gt;where the function $f(t)$ returns the index of the interval in which the shock $z_t$ falls.&lt;/p&gt;

&lt;h4 id=&#34;tl-dr-1&#34;&gt;TL;DR&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;GenBKM = BKM with different scaled IRF, instead of one.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;ii-b-genbkm-example&#34;&gt;II. B. GenBKM example&lt;/h3&gt;

&lt;p&gt;The GenBKM algorithm is implemented in the block of code that follows. The next plot shows that the approximation error is much smaller using the GenBKM algorithm. The next table shows that the mean absolute error drops by more than 300% if the GenBKM is used instead of BKM.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function GenBKM_path!(XT::Array{Float64,1}, max_iter::Int64, x_scaled::Array{Float64,2}, shocks::Array{Float64,1}, array_sigma::Array{Float64,1})

    # get the length of x_scaled
    len_x_scaled = size(x_scaled,1)

    # We don&#39;t want x_scaled to contain any NaN value
    if sum(isnan.( x_scaled) .== true) != 0
        error(&amp;quot;x_scaled contains at least one NaN value.&amp;quot;)
    end

    # We don&#39;t want shocks to contain any NaN value
    if sum(isnan.(shocks) .== true) != 0
        error(&amp;quot;shocks contains at least one NaN value.&amp;quot;)
    end

    # Loop over time periods periods
    for t=2:max_iter
        # Superposition of MIT shocks:
        for k=1:t
            # After some time, we assume that the effect of past shocks vanishes:
            if k&amp;lt;=len_x_scaled
                # Find were the initial shock stood on the sigma grid:
                index_sigma = searchsortednearest(array_sigma, shocks[t-k+1])
                XT[t]+=x_scaled[k, index_sigma]*shocks[t-k+1]
            end
        end
    end

end

# Function to find the index corresponding to the closest value on a grid:
# Source: https://discourse.julialang.org/t/findnearest-function/4143/4
function searchsortednearest(a::Array{Float64,1},x::Float64)
    &amp;quot;&amp;quot;&amp;quot;
    a::Array{Float64,1}: grid
    x::Float64: value to be found
    returns the index of the closest value to x on grid a
    &amp;quot;&amp;quot;&amp;quot;
    idx = searchsortedfirst(a,x)
    if (idx==1); return idx; end
    if (idx&amp;gt;length(a)); return length(a); end
    if (a[idx]==x); return idx; end
    if (abs(a[idx]-x) &amp;lt; abs(a[idx-1]-x))
      return idx
    else
      return idx-1
    end
end

# Calculation of GenBKM path:
XT_GenBKM = zeros(max_iter);
@time GenBKM_path!(XT_GenBKM, max_iter, x_mit_scaled_sigma, shocks_t, array_sigma)

# Let&#39;s store statistics on error:
diff_GenBKM = x_true - XT_GenBKM
max_abs_err_GenBKM = maximum(abs.(diff_GenBKM))
min_abs_err_GenBKM = minimum(abs.(diff_GenBKM))
mean_abs_err_GenBKM = mean(abs.(diff_GenBKM))
median_abs_err_GenBKM = median(abs.(diff_GenBKM))

df = DataFrame(Statistics = [&amp;quot;Max absolute error&amp;quot;, &amp;quot;Min absolute error&amp;quot;, &amp;quot;Mean absolute error&amp;quot;, &amp;quot;Median absolute error&amp;quot;],
               BKM = [max_abs_err_BKM, min_abs_err_BKM, mean_abs_err_BKM, median_abs_err_BKM],
               GenBKM = [max_abs_err_GenBKM, min_abs_err_GenBKM, mean_abs_err_GenBKM, median_abs_err_GenBKM])

# Plot errors
p6 = plot(diff_GenBKM[2:end], label=&amp;quot;GenBKM&amp;quot;, xlabel = &amp;quot;t&amp;quot;, ylabel = &amp;quot;Error&amp;quot;)
plot!(diff_BKM[2:end], label=&amp;quot;BKM&amp;quot;)
title!(&amp;quot;Error BKM and GenBKM&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;GenBKM_14_1.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes: In orange the approximation error when using BKM; in blue the approximation error when using GenBKM&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;table class=&#34;data-frame&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;Statistics&lt;/th&gt;&lt;th&gt;BKM&lt;/th&gt;&lt;th&gt;GenBKM&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt; &lt;/th&gt;&lt;th&gt; &lt;/th&gt;&lt;th&gt; &lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;p&gt; &lt;/p&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;td&gt;Max absolute error&lt;/td&gt;&lt;td&gt;0.263167&lt;/td&gt;&lt;td&gt;0.132318&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;td&gt;Min absolute error&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;&lt;td&gt;Mean absolute error&lt;/td&gt;&lt;td&gt;0.0381089&lt;/td&gt;&lt;td&gt;0.0136915&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;&lt;td&gt;Median absolute error&lt;/td&gt;&lt;td&gt;0.0237194&lt;/td&gt;&lt;td&gt;0.0100031&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Heterogeneity along the business cycle matters. This blog post presented two simple algorithms that are both fast and accurate to solve for macroeconomics models in which heterogeneity is key. GenBKM is a refinement of BKM, which tends to be more accurate. However, there is no free lunch. The increased accuracy of GenBKM is obtained by using several MIT shocks instead of one.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Aiyagari, S. Rao. &amp;ldquo;Uninsured idiosyncratic risk and aggregate saving.&amp;rdquo; The Quarterly Journal of Economics 109.3 (1994): 659-684.&lt;/li&gt;
&lt;li&gt;Auclert, Adrien, et al. Using the Sequence-Space Jacobian to Solve and Estimate Heterogeneous-Agent Models. No. w26123. National Bureau of Economic Research, 2019.&lt;/li&gt;
&lt;li&gt;Boppart, Timo, Per Krusell, and Kurt Mitman. &amp;ldquo;Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative.&amp;rdquo; Journal of Economic Dynamics and Control 89 (2018): 68-92.&lt;/li&gt;
&lt;li&gt;Le Grand, François, and Ragot, Xavier. &amp;ldquo;Managing Inequality over the Business Cycles: Optimal Policies with Heterogeneous Agents and Aggregate Shocks&amp;rdquo;. No. 1090. Society for Economic Dynamics, 2019.&lt;/li&gt;
&lt;li&gt;Reiter, Michael. &amp;ldquo;Comments on&amp;rdquo; Exploiting MIT Shocks in Heterogeneous-Agent Economies: The Impulse Response as a Numerical Derivative&amp;rdquo; by T. Boppart, P. Krusell and K. Mitman.&amp;rdquo; Journal of Economic Dynamics and Control 89 (2018): 93-99.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;versioninfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Julia Version 1.3.0
Commit 46ce4d7933 (2019-11-26 06:09 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-6.0.1 (ORCJIT, skylake)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The linear–quadratic regulator Part II</title>
      <link>https://julienpascal.github.io/post/lqr_partii/</link>
      <pubDate>Sun, 05 Apr 2020 16:00:00 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/lqr_partii/</guid>
      <description>

&lt;p&gt;This notebook builds upon what has been described in &lt;a href=&#34;https://julienpascal.github.io/post/lqr/&#34; target=&#34;_blank&#34;&gt;Part I&lt;/a&gt;. In &lt;a href=&#34;https://julienpascal.github.io/post/lqr/&#34; target=&#34;_blank&#34;&gt;Part I&lt;/a&gt;, we introduced the linear–quadratic regulator (LQR) framework in Python. We solved the linearized control problem.&lt;/p&gt;

&lt;p&gt;In this notebook, we will see that we can do better. The basic idea is to follow the the evolution of &amp;ldquo;observables&amp;rdquo; — functions of the state space — instead of the evolution of the state itself using the &lt;strong&gt;Koopman operator&lt;/strong&gt;. In the space of observables, the differential equation is linear. Thus, we can solve for the optimal control in the this transformed space, without having to linearize the system around its steady-state.&lt;/p&gt;

&lt;h4 id=&#34;in-this-notebook-you-will-learn&#34;&gt;In this notebook, you will learn:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;the basics on the Koopman operator&lt;/li&gt;
&lt;li&gt;how to solve for the optimal control in the Koopman subspace&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;i-the-koopman-operator-101&#34;&gt;I. The Koopman operator 101&lt;/h1&gt;

&lt;p&gt;Let us remember that in Part I, we analyzed the evolution of the following dynamical system:&lt;/p&gt;

&lt;p&gt;$$ \begin{cases} \frac{d}{dt} x = \mu x \\ \frac{d}{dt} y = \lambda (y - x^2)  \end{cases} $$&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s make the observation that we can defined a new vector $z$ defined by:&lt;/p&gt;

&lt;p&gt;$$ z \equiv \begin{pmatrix} z_1 \\ z_2 \\ z_3 \end{pmatrix} = \begin{pmatrix} x \\ y \\ x^2 \end{pmatrix}$$&lt;/p&gt;

&lt;p&gt;The original nonlinear dynamical system is linear when considering the evolution of $z$:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \begin{pmatrix} z_1 \\ z_2 \\ z_3 \end{pmatrix} = \begin{pmatrix} \mu &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \lambda &amp;amp; -\lambda \\ 0 &amp;amp; 0 &amp;amp; 2 \mu \end{pmatrix}  \begin{pmatrix} z_1 \\ z_2 \\ z_3 \end{pmatrix}$$&lt;/p&gt;

&lt;p&gt;What we just did — writing the evolution the dynamical system using some observables — is the central idea of the Koopman operator. Here we are lucky because our new variable $z$ is of finite dimension. In the general case, $z$ is infinite-dimensional:&lt;/p&gt;

&lt;p&gt;$$
\frac{d}{dt} \begin{pmatrix} z_1 \\ z_2 \\ &amp;hellip; \end{pmatrix} = \begin{pmatrix} a_{11} &amp;amp; a_{12} &amp;amp; &amp;hellip; \\ a_{21} &amp;amp; a_{22} &amp;amp; &amp;hellip; \\ &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; \end{pmatrix} \begin{pmatrix} z_1 \\ z_2 \\ &amp;hellip; \end{pmatrix} $$&lt;/p&gt;

&lt;p&gt;and we have no guarantee that the truncation of z by a finite dimensional counterpart will produce a good approximation of the system. For more detail on this problem, I invite you to read &lt;a href=&#34;https://arxiv.org/abs/1510.03007&#34; target=&#34;_blank&#34;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Because now the dynamical system is linear, we can directly use the results from LQR framework and solve for the optimal control that minimizes the cost $J$:&lt;/p&gt;

&lt;p&gt;$$ J = \int_{0}^{\infty} x&amp;rsquo;(t) Q x(t) + u&amp;rsquo;(t) R u(t) dt $$&lt;/p&gt;

&lt;p&gt;We already know that the optimal control from the controller is a linear function of the current state of the system:&lt;/p&gt;

&lt;p&gt;$$ u = - C_z z$$&lt;/p&gt;

&lt;p&gt;When the system is controlled optimally, the equation governing the evolution of the system writes:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} z = A_z z - B C_z z $$&lt;/p&gt;

&lt;p&gt;At a later stage, because we want to compare the Koopman controller to the linearized controller, we do not want to penalize the variable $z_3$ for being away from its steady state. The rationale is the variable $z_3$ is &amp;ldquo;fictive&amp;rdquo;.
We still have in mind that we want to control the &amp;ldquo;real&amp;rdquo; variables $x$ and $y$. As a result, the matrix $Q$ we consider
is:&lt;/p&gt;

&lt;p&gt;$$ \begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 \end{pmatrix} $$&lt;/p&gt;

&lt;h2 id=&#34;ii-simulating-forward-the-dynamical-system&#34;&gt;II. Simulating forward the dynamical system&lt;/h2&gt;

&lt;p&gt;For what follows, you will need the following packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import matplotlib.pyplot as plt
plt.style.use(&#39;ggplot&#39;)
%matplotlib inline
import numpy as np
from control.matlab import *  # MATLAB-like functions
#to solve ODE
from scipy import integrate
#show the version of Python I am using:
!python3 --version
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Python 3.5.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s study optimal control for the following differential equation:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \begin{pmatrix} z_1 \\ z_2 \\ z_3 \end{pmatrix} = \begin{pmatrix} \mu &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \lambda &amp;amp; -\lambda \\ 0 &amp;amp; 0 &amp;amp; 2 \mu \end{pmatrix}  \begin{pmatrix} z_1 \\ z_2 \\ z_3 \end{pmatrix}  $$&lt;/p&gt;

&lt;p&gt;The following block of code define the parameter values, the matrix A for the linear part of the differential equation and the matrix B specifying that the controller can only act on x. We also specify the time span during which we want to simulate forward the trajectories:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Parameters and matrices A and B:
mu = -0.05
llambda = -1.0
# Matrices for the orginial system
A = np.array([[mu, 0], [0, llambda]])
B = np.array([[0], [1]])
R = np.array([1])
Q = np.eye(2)
# Matrices for the transformed system:
A_z = np.array([[mu, 0, 0], [0, llambda, -llambda], [0, 0, 2*mu]])  
B_z = np.array([[0], [1], [0]])
R_z = np.array([1])
Q_z = np.eye(3)
# Time span
t0, t1 = 0, 100 # start and end
t = np.arange(t0, t1, 0.01)

# Function that defines the dynamic system:
def vdp0(t, y):
    # linear part + nonlinear part:
    x = A.dot(y) + np.array([0, -llambda*(y[0]**2)])
    return x

# Function that defines the dynamic system in the Koopman subspace:
def vdp0z(t, y):
    x = A_z.dot(y)
    return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then define 4 different starting values and simulate forward the system using the &lt;code&gt;scipy.integrate&lt;/code&gt; toolkit:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set of starting values:
y0A = np.array([1.5, -1, (1.5)**2])
y0B = np.array([-1.5, -1, (-1.5)**2])
y0C = np.array([5, 5, 25])
y0D = np.array([-5, 5, 25])

# To store the different trajectories
list_y_z = []

# Method for the ODE:
# This is an explicit runge-kutta method of order (4)5 due to Dormand &amp;amp; Prince
integrator = &amp;quot;dopri5&amp;quot;

# Loop over the different starting values and calculate trajectories:
for y0 in [y0A, y0B, y0C,y0D]:
    # initialize an array to store the solution
    y = np.zeros((len(t), len(y0)))   # array for solution
    r0 = integrate.ode(vdp0z).set_integrator(integrator)
    r0.set_initial_value(y0, t0)   # initial values
    for i in range(1, t.size):
       y[i, :] = r0.integrate(t[i]) # get one more value, add it to the array
       if not r0.successful():
           raise RuntimeError(&amp;quot;Could not integrate&amp;quot;)
    # append the list of solution
    list_y_z.append(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next graph shows the trajectory of x and y using the augmented linear system corresponds to the trajectory
we found using the nonlinear dynamical system, as expected:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot the different paths:
fig, ax = plt.subplots(figsize=(10, 5))

for index,y0 in enumerate([y0A, y0B, y0C, y0D]):
    ax.plot(list_y_z[index][1:-1, 0], list_y_z[index][1:-1, 1], label = str(y0))
    plt.xlabel(&amp;quot;x&amp;quot;)
    plt.ylabel(&amp;quot;y&amp;quot;)
plt.title(&amp;quot;Trajectories for different starting values&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartII_21_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The next graph shows that the dynamical system moves along a nice parabola in the third dimension:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot the different paths:
fig, ax = plt.subplots(figsize=(10, 5))

for index,y0 in enumerate([y0A, y0B, y0C, y0D]):
    ax.plot(list_y_z[index][1:-1, 0], list_y_z[index][1:-1, 2], label = str(y0))
    plt.xlabel(&amp;quot;x&amp;quot;)
    plt.ylabel(&amp;quot;z&amp;quot;)
plt.title(&amp;quot;Trajectories for different starting values&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartII_23_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;iii-solving-for-the-optimal-control&#34;&gt;III. Solving for the optimal control&lt;/h2&gt;

&lt;p&gt;Let us remember that our aim is find the matrix $C_z$ defining the optimal control:&lt;/p&gt;

&lt;p&gt;$$ u = - C_z z $$&lt;/p&gt;

&lt;p&gt;Interestingly, while the optimal $u$ is linear when considering $z$, it is quadratic when considering the original vector of state $x$:&lt;/p&gt;

&lt;p&gt;$$ u = - (C_{z,1} C_{z,2}) \begin{pmatrix} x \\ y  \end{pmatrix} -  C_{z,3} x^2  $$&lt;/p&gt;

&lt;p&gt;We will see in a minute that having a non-linear control allows us to outperform the linear control obtained
in &lt;a href=&#34;https://julienpascal.github.io/post/lqr/&#34; target=&#34;_blank&#34;&gt;Part I&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Solve for C:
(C, X, E) = lqr(A, B, Q, R)
print(&amp;quot;Feedback matrix C : {}&amp;quot;.format(C))

# Solve for C_z:
(C_z, X_z, E_z) = lqr(A_z, B_z, Q_z, R_z)
print(&amp;quot;Feedback matrix C_z : {}&amp;quot;.format(C_z))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Feedback matrix C : [[0.         0.41421356]]
Feedback matrix C_z : [[0.         0.41421356 0.27355029]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now proceed as previously to simulate forward the differential equation. The only difference is that
now we have to take into account the optimal control applied each period on the system. The optimal control is
taken into consideration in the function &lt;code&gt;vdp1(t, y)&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vdp1(t, y):
    # Ay - B*X*y + Cy
    x = A.dot(y)  - np.matmul(B,C).dot(y)
    return x + np.array([0, -llambda*(y[0]**2)])

def vdp1z(t, y):
    return A_z.dot(y)  - np.matmul(B_z,C_z).dot(y)

y0 = [-5, 5]                   # initial value
y0_z = [-5, 5, 25]                   # initial value

y = np.zeros((len(t), len(y0))) # array for solution
y_z = np.zeros((len(t), len(y0_z))) # array for solution
y[0, :] = y0
y_z[0, :] = y0_z
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Controlled trajectory using the linearized system
r = integrate.ode(vdp1).set_integrator(integrator)
r.set_initial_value(y0, t0)   # initial values

for i in range(1, t.size):
   y[i, :] = r.integrate(t[i]) # get one more value, add it to the array
   if not r.successful():
       raise RuntimeError(&amp;quot;Could not integrate&amp;quot;)

# Controlled trajectory using the linearized system
r = integrate.ode(vdp1z).set_integrator(integrator)
r.set_initial_value(y0_z, t0)   # initial values

for i in range(1, t.size):
   y_z[i, :] = r.integrate(t[i]) # get one more value, add it to the array
   if not r.successful():
       raise RuntimeError(&amp;quot;Could not integrate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now compare the controlled and Koopman controlled trajectories. Interestingly, the Koopman controlled
trajectory moves along a trajectory involving lower values for $y$:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;index = 3 #choose the last trajectory
fig, ax = plt.subplots(figsize=(10, 5))
plt.plot(list_y_z[index][1:-1, 0], list_y_z[index][1:-1, 1], label = &amp;quot;Uncontrolled&amp;quot;)
plt.plot(y[:, 0], y[:, 1], label = &amp;quot;Controlled&amp;quot;)
plt.plot(y_z[:, 0], y_z[:, 1], label = &amp;quot;Koopman controlled&amp;quot;)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartII_32_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The next logical question is whether or not the Koopman controller is better than the controller based on the
linearization of the dynamical system around its steady state. The next plot shows that indeed the Koopman controller (in purple) outperforms the latter (in blue).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Controlled
JLQR = np.cumsum( np.square(y[:, 0]) + np.square(y[:, 1]) + np.square(np.matmul(C, y.T)) ).T
# Koopman controlled
JLQRz = np.cumsum( np.square(y_z[:, 0]) + np.square(y_z[:, 1]) + np.square(np.matmul(C_z, y_z.T)) ).T
# Uncontrolled
JLQR0 = np.cumsum( np.square(list_y_z[index][:, 0]) + np.square(list_y_z[index][:, 1]) )

fig, ax = plt.subplots(figsize=(10, 5))
plt.plot(t, JLQR0, label = &amp;quot;Uncontrolled&amp;quot;)
plt.plot(t, JLQR, label = &amp;quot;Controlled&amp;quot;)
plt.plot(t, JLQRz, label = &amp;quot;Koopman controlled&amp;quot;)
plt.xlabel(&amp;quot;t&amp;quot;)
plt.ylabel(&amp;quot;JLQR&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartII_34_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This notebook illustrates the idea behind the Koopman operator in a very simple setting. We saw that by solving for the optimal control in the space of observables, in which the system is exactly linear, we find a controller that outperforms the one obtained by linearizing the system around its steady-state.&lt;/p&gt;

&lt;p&gt;The Koopman LQR control drew my attention because many techniques, especially in Economics, are based on the linearization of dynamic systems around its steady-state. While these linearization techniques are accurate when the economy is close to its steady-state (in a &amp;ldquo;business as usual&amp;rdquo; situation), it is hard to know how these approximations perform when the economy is an unusual state. Given the current economic context, relying on linearization might be inaccurate.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The linear system studied in this notebook is based on the paper: Brunton, Steven L., et al. &amp;ldquo;Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control.&amp;rdquo; PloS one 11.2 (2016).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Koopman, Bernard O. &amp;ldquo;Hamiltonian systems and transformation in Hilbert space.&amp;rdquo; Proceedings of the national academy of sciences of the united states of america 17.5 (1931): 315.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The linear–quadratic regulator Part I</title>
      <link>https://julienpascal.github.io/post/lqr/</link>
      <pubDate>Wed, 01 Apr 2020 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/lqr/</guid>
      <description>

&lt;p&gt;The two main goals of this blog post is to introduce what the &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator&#34; target=&#34;_blank&#34;&gt;linear–quadratic regulator (LQR)&lt;/a&gt; framework is and to show how to solve LQR problems using Python. The LQR is concerned with operating a dynamic system (a rocket, a car, the economy, etc.) at minimum cost.&lt;/p&gt;

&lt;h4 id=&#34;in-this-blog-post-you-will-learn&#34;&gt;In this blog post you will learn&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;what the LQR framework is&lt;/li&gt;
&lt;li&gt;how to simulate forward an ordinary differential equation using &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html&#34; target=&#34;_blank&#34;&gt;scipy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;how to solve for the optimal control using the &lt;a href=&#34;https://python-control.readthedocs.io/en/0.8.3/&#34; target=&#34;_blank&#34;&gt;Python Control Systems Library&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Jupyter notebook with the code used to generate this blog post can be found &lt;a href=&#34;https://github.com/JulienPascal/KoopmanObservableSubspaces&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;i-the-lqr-framework-in-a-nutshell&#34;&gt;I. The LQR framework in a nutshell&lt;/h2&gt;

&lt;p&gt;Many natural phenomena naturally lead to differential equations. A differential equation is an equation in which the rate of the change of a variable ($\frac{d}{dt} x$) is a function its state $x$. The unknown is a function satisfying both the differential equation and an initial value. For instance, a simple model of the spread of the Covid-19 can be written as a system of differential equations (see for instance the &lt;a href=&#34;https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology&#34; target=&#34;_blank&#34;&gt;SIR model&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \boldsymbol{x} = f(\boldsymbol{x},t) $$&lt;/p&gt;

&lt;p&gt;The LQR theory studies a special case of the above problem. It focuses on problems where the function $f(\boldsymbol{x},t)$ is linear:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \boldsymbol{x} = A  \boldsymbol{x} $$&lt;/p&gt;

&lt;p&gt;The equation above is a &amp;ldquo;passive&amp;rdquo; one. We simply observe the trajectory of $\boldsymbol{x}$ and there is nothing we can do about it. The LQR framework is based on the idea that an observer may want to change the trajectory of the system by exerting a control on $\boldsymbol{x}$. In the case of the spread of the Covid-19, the government may want to limit the number of new cases. When considering the economy, a central bank may want to control the interest rate to reach its inflation target. In the case of the SpaceX, the engineers may want to stabilize the rocket so that it does not explode when trying to land back on Earth.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;spaceX.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the LQR framework, the controller wants to stabilize the system to reach one of its steady-state values, defined by:&lt;/p&gt;

&lt;p&gt;$$ \boldsymbol{x_{ss}} = A \boldsymbol{x_{ss}} $$&lt;/p&gt;

&lt;p&gt;We need to take into consideration the impact that the controller has on the system. Let us add the control, denoted by $u$, to the uncontrolled system from above:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \boldsymbol{x} = A  \boldsymbol{x} + B  \boldsymbol{u} $$&lt;/p&gt;

&lt;p&gt;where $B$ is a matrix capturing the idea that the controller could only control some elements of $\boldsymbol{x}$.&lt;/p&gt;

&lt;p&gt;However, there is no free lunch. In order to stabilize the system, the controller needs to pay a cost. Going back to our rocket example, some fuel is burnt in order to stabilize the trajectory of the rocket. The LQR is based on a cost function that is quadratic. This quadratic assumption simplifies the algebra substantially and captures the intuitive idea that doubling the effort actually costs four times more, not two times more.&lt;/p&gt;

&lt;p&gt;Let us assume that the steady state of the system is $\boldsymbol{0}$. This is without loss of generality, because we can rewrite the system as a deviation from its steady-state value $\boldsymbol{\tilde{x}} \equiv \boldsymbol{x} - \boldsymbol{x_{ss}}$, in which case the steady-state is reached for  $\boldsymbol{\tilde{x}} = \boldsymbol{0}$.&lt;/p&gt;

&lt;p&gt;To capture the cost of stabilizing the system, let us use the letter $J$. $J$ captures two types of cost. Firstly, the controller dislikes when the system is not at its steady-state. In the equation below, this type of cost is captured by the matrix $Q$. Secondly, the controller dislikes spending energy to control the system. This second type of cost is captured by the matrix $R$:&lt;/p&gt;

&lt;p&gt;$$ J = \int_{0}^{\infty} \boldsymbol{x&amp;rsquo;}(t) Q \boldsymbol{x}(t) + \boldsymbol{u&amp;rsquo;}(t) R \boldsymbol{u}(t) dt $$&lt;/p&gt;

&lt;p&gt;A beautiful result from the LQR theory is that the optimal control from the controller is simply a linear function of the current state of the system:&lt;/p&gt;

&lt;p&gt;$$ \boldsymbol{u} = - C \boldsymbol{x}$$&lt;/p&gt;

&lt;p&gt;When the system is controlled optimally, the equation governing the evolution of the system writes:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \boldsymbol{x} = A \boldsymbol{x} - B  C \boldsymbol{x} $$&lt;/p&gt;

&lt;h2 id=&#34;ii-simulating-forward-an-ordinary-differential-equation-in-python&#34;&gt;II. Simulating forward an ordinary differential equation in Python&lt;/h2&gt;

&lt;p&gt;Having summarized what the LQR framework is, we can now give an illustration of how it works using a simple example using Python. For what follows, you will need the following packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import matplotlib.pyplot as plt
plt.style.use(&#39;ggplot&#39;)
%matplotlib inline
import numpy as np
from control.matlab import *  # MATLAB-like functions
#to solve ODE
from scipy import integrate
#show the version of Python I am using:
!python3 --version
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Python 3.5.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s study optimal control for the following differential equation:&lt;/p&gt;

&lt;p&gt;$$\begin{cases} \frac{d}{dt} x =\mu x \\ \frac{d}{dt} y = \lambda (y - x^2) \end{cases}$$&lt;/p&gt;

&lt;p&gt;Two observations on this dynamical system. Firstly, the system is not linear, but we will see how to deal with that in a minute. Secondly, by eyeballing the equation, it is easy to see that when $\mu &amp;lt; 1$ and $\lambda &amp;lt; 1$, the unique fixed point is&lt;/p&gt;

&lt;p&gt;$$ \begin{pmatrix} x_{SS} \\ y_{SS} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} $$&lt;/p&gt;

&lt;p&gt;We can convince ourselves by simulating forward the trajectory of the system using different starting values.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s notice that the system can be written as a linear one, plus a part that is nonlinear:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} \mu &amp;amp; 0 \\ 0 &amp;amp; \lambda \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} +  \begin{pmatrix} O \\ -  \lambda x^2 \end{pmatrix} $$&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \begin{pmatrix} x \\ y \end{pmatrix} = A \begin{pmatrix} x \\ y \end{pmatrix} +  \begin{pmatrix} O \\ -  \lambda x^2 \end{pmatrix}$$&lt;/p&gt;

&lt;p&gt;The following block of code defines the parameter values, the matrix A for the linear part of the differential equation and the matrix B specifying that the controller can only act on x. We also specify the time span during which we want to simulate forward the trajectories:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Parameters and matrices A and B:
mu = -0.05
llambda = -1.0
A = np.array([[mu, 0], [0, llambda]])
B = np.array([[0], [1]])
# Time span
t0, t1 = 0, 100 # start and end
t = np.arange(t0, t1, 0.01)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A clean way to simulate forward the trajectory is to define a function that return the evolution of the system:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Function that defines the dynamic system:
def vdp0(t, y):
    # linear part + nonlinear part:
    x = A.dot(y) + np.array([0, -llambda*(y[0]**2)])
    return x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then define 4 different starting values and simulate forward the system using the &lt;code&gt;scipy.integrate&lt;/code&gt; toolkit:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set of starting values:
y0A = np.array([1.5, -1])
y0B = np.array([-1.5, -1])
y0C = np.array([5, 5])
y0D = np.array([-5, 5])

# To store the different trajectories
list_y = []

# Method for the ODE:
# This is an explicit runge-kutta method of order (4)5 due to Dormand &amp;amp; Prince
integrator = &amp;quot;dopri5&amp;quot;

# Loop over the different starting values and calculate trajectories:
for y0 in [y0A, y0B, y0C,y0D]:
    # initialize an array to store the solution
    y = np.zeros((len(t), len(y0)))   # array for solution
    r0 = integrate.ode(vdp0).set_integrator(integrator)
    r0.set_initial_value(y0, t0)   # initial values
    for i in range(1, t.size):
       y[i, :] = r0.integrate(t[i]) # get one more value, add it to the array
       if not r0.successful():
           raise RuntimeError(&amp;quot;Could not integrate&amp;quot;)
    # append the list of solution
    list_y.append(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then plot the trajectories we just calculated on a same graph using &lt;code&gt;matplotlib&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot the different paths:
fig, ax = plt.subplots(figsize=(10, 5))

for index,y0 in enumerate([y0A, y0B, y0C, y0D]):
    ax.plot(list_y[index][1:-1, 0], list_y[index][1:-1, 1], label = str(y0))
    plt.xlabel(&amp;quot;x&amp;quot;)
    plt.ylabel(&amp;quot;y&amp;quot;)
plt.title(&amp;quot;Trajectories for different starting values&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartI_20_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the graph, we see that $y$ moves very quickly to the parabola defined by $y = x^2$. Then, the system slowly converges
towards $(0,0)$, moving along the same parabola.&lt;/p&gt;

&lt;p&gt;Before moving to the optimal control of the system, let us calculate the total cost $J$ of letting the system
converging naturally to its steady state value:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Store the cost associated with each starting value:
list_cost = []
for y in list_y:
    JLQR = np.cumsum( np.square(y[:, 0]) + np.square(y[:, 1]) )
    # append the list of solution
    list_cost.append(JLQR)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next plot shows the cumulative cost as time passes. The further away the starting value is from the steady-state, the higher the cost. We also see that because the cost function treats negative and positive deviations from the steady state the same way (because deviations are squared), the cost for the starting values (1.5, -1) and (-1.5, -1) are the same. The same observation holds for (5, 5) and (-5, 5).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot the cost associated with each starting value:
fig, ax = plt.subplots(figsize=(10, 5))
for JLQR, y0 in zip(list_cost,[y0A, y0B, y0C, y0D]):
    plt.plot(t, JLQR, label = str(y0))
    plt.xlabel(&amp;quot;t&amp;quot;)
    plt.ylabel(&amp;quot;JLQR&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartI_25_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;iii-solving-for-the-optimal-control&#34;&gt;III. Solving for the optimal control&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s first define the matrices Q and R, before solving for the optimal control matrix $C$ using the &lt;code&gt;lqr&lt;/code&gt; function from the &lt;a href=&#34;https://python-control.readthedocs.io/en/0.8.3/&#34; target=&#34;_blank&#34;&gt;Python Control Systems Library&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;R = np.array([1])
Q = np.eye(2)
# Solve for C:
(C, X, E) = lqr(A, B, Q, R)
print(&amp;quot;Feedback matrix C : {}&amp;quot;.format(C))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Feedback matrix C : [[0.         0.41421356]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now proceed as previously to simulate forward the differential equation. The only difference is that
now we have to take into account the optimal control applied each period on the system. The optimal control is
taken into consideration in the function &lt;code&gt;vdp1(t, y)&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vdp1(t, y):
    # Ay - B*X*y + Cy
    x = A.dot(y)  - np.matmul(B,C).dot(y)
    return x + np.array([0, -llambda*(y[0]**2)])

y0 = [-5, 5]                   # initial value
y = np.zeros((len(t), len(y0))) # array for solution
y[0, :] = y0

r = integrate.ode(vdp1).set_integrator(integrator)
r.set_initial_value(y0, t0)   # initial values

for i in range(1, t.size):
   y[i, :] = r.integrate(t[i]) # get one more value, add it to the array
   if not r.successful():
       raise RuntimeError(&amp;quot;Could not integrate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now compare the controlled trajectory (in red) to the uncontrolled trajectory (in blue):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(10, 5))
plt.plot(y[:, 0], y[:, 1], label = &amp;quot;controlled&amp;quot;)
plt.plot(list_y[index][1:-1, 0], list_y[index][1:-1, 1], label = &amp;quot;uncontrolled&amp;quot;)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;y&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartI_32_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With the controlled trajectory, the deviation of $y$ from its steady state value is less extreme. The system converges to $(0,0)$ on a different parabola. As expected, controlling the system with the optimal controller is less costly than letting the system evolve uncontrolled:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;JLQR = np.cumsum( np.square(y[:, 0]) + np.square(y[:, 1]) + np.square(np.matmul(C, y.T)) ).T

fig, ax = plt.subplots(figsize=(10, 5))
plt.plot(t, JLQR, label = &amp;quot;controlled&amp;quot;)
plt.plot(t, list_cost[3], label = &amp;quot;uncontrolled&amp;quot;)
plt.xlabel(&amp;quot;t&amp;quot;)
plt.ylabel(&amp;quot;JLQR&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Koopman_Control_Python_PartI_34_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;iv-what-about-lambda-x-2-linearizing-the-nonlinear-system&#34;&gt;IV. What about $-\lambda x^2$? Linearizing the nonlinear system&lt;/h2&gt;

&lt;p&gt;$$ \frac{d}{dt} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} \mu &amp;amp; 0 \\ 0 &amp;amp; \lambda \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} +  \begin{pmatrix} O \\ -  \lambda x^2 \end{pmatrix} $$&lt;/p&gt;

&lt;p&gt;A careful reader would have noticed that we used a linear controller on a non-linear system. Is it legitimate? Intuitively, we ignored the term $-\lambda x^2$, which is small when $x &amp;lt; 1$ and/or when $\lambda$ is small. We can show more &amp;ldquo;rigorously&amp;rdquo; that what we did makes sense.&lt;/p&gt;

&lt;p&gt;Let us remember that the dynamical system is:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt} \begin{pmatrix} x \\ y \end{pmatrix} = f \Big(  \begin{pmatrix} x \\ y \end{pmatrix}, t \Big) $$&lt;/p&gt;

&lt;p&gt;Or more precisely:&lt;/p&gt;

&lt;p&gt;$$ \begin{cases} \frac{d}{dt} x = \mu x \\ \frac{d}{dt} y = \lambda (y - x^2)  \end{cases} $$&lt;/p&gt;

&lt;p&gt;A first order Taylor expansion around the steady-state gives us:&lt;/p&gt;

&lt;p&gt;$$ \boldsymbol{x} - \boldsymbol{x_{ss}} \approx A (\boldsymbol{x} - \boldsymbol{x_{ss}}) $$&lt;/p&gt;

&lt;p&gt;Where $A$ is the Jacobian matrix (matrix of first derivatives) evaluated at the steady-state value, which is (0,0) in our simple example. The first derivatives are:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dx}(\frac{d}{dt}x) = \mu  \\ \frac{d}{dy}(\frac{d}{dt}x) = 0 \\ \frac{d}{dx}(\frac{d}{dt}y) = \lambda \\ \frac{d}{dy}(\frac{d}{dt}y)= -2 \lambda x $$&lt;/p&gt;

&lt;p&gt;Evaluated at the steady-state, the matrix A is equal to:&lt;/p&gt;

&lt;p&gt;$$ A = \begin{pmatrix} \mu &amp;amp; 0 \\ -2 \lambda \times 0 &amp;amp; \lambda \end{pmatrix} = \begin{pmatrix} \mu &amp;amp; 0 \\ 0 &amp;amp; \lambda \end{pmatrix} $$&lt;/p&gt;

&lt;p&gt;and because the steady-state is $(0,0)$, we have $\tilde{x} = x$. The take-away is that in the neighborhood of the steady-state $(0,0)$, we can solve the LQR ignoring the $-\lambda x^2$ term.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This notebook introduced what the LQR framework and showed how to solve for the optimal control in Python. We saw that despite the fact that the example we studied is not linear, we can linearize the dynamical system around its stead-state. In the Part II of this series on the LQR framework, we will see that we can do even better by solving the dynamical system in a new space, in which the system is exactly linear.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The linear system studied in this notebook is based on the paper: Brunton, Steven L., et al. &amp;ldquo;Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control.&amp;rdquo; PloS one 11.2 (2016).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Primer on Computer Vision</title>
      <link>https://julienpascal.github.io/post/cnn/</link>
      <pubDate>Thu, 05 Dec 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/cnn/</guid>
      <description>

&lt;p&gt;For a specific project on the housing market (&lt;a href=&#34;https://julienpascal.github.io/project/rentalmarket/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), I had to analyze thousands of photos. To do that, I used a &lt;strong&gt;convolutional neural network&lt;/strong&gt; (CNN), which is a fancy name for a complicated function that can be &amp;ldquo;trained&amp;rdquo; to recognize patterns in images. In this blog post, I would like to introduce the &lt;strong&gt;&amp;ldquo;Hello World&amp;rdquo;&lt;/strong&gt; of computer vision and CNN: the classification of hand-written digits from the &lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34; target=&#34;_blank&#34;&gt;MNIST dataset&lt;/a&gt;. There are thousands of tutorials on the same topic using Python freely available on the Internet. Instead, let&amp;rsquo;s use Julia and the package &lt;strong&gt;Flux.jl&lt;/strong&gt;. Why? Because &lt;strong&gt;Julia is fast&lt;/strong&gt;, and if you have millions of images to analyze, the speed up could be substantial compared to Python. The Jupyter notebook used to generate this post can be found &lt;a href=&#34;https://github.com/JulienPascal/CNNPrimer&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;The MNIST dataset contains images of hand-written digits (0 to 9) in grayscale and that are nicely centered. Each pixel is represented by a number in between 0 (black) and 255 (white). Each image is 28 by 28 pixels. One way to represent an image is to see it as a 1d-column vector of 28*28 = 784 pixels. However, this representation ignores the &amp;ldquo;structure&amp;rdquo; of an image: pixels that are close to each others are informative on the digit we are trying to identify. A CNN is a good tool to keep the spatial structure of an image, while avoiding issues linked to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_dimensionality&#34; target=&#34;_blank&#34;&gt;curse of dimensionality&lt;/a&gt;: images are noisy and high-dimensional input data.&lt;/p&gt;

&lt;h2 id=&#34;a-crash-course-on-cnn&#34;&gt;A crash course on CNN&lt;/h2&gt;

&lt;p&gt;Two of the key ingredients of a CNN are a &lt;strong&gt;convolutional layer&lt;/strong&gt; (hence the name) and a &lt;strong&gt;maxpool layer&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;convolutional-layer&#34;&gt;Convolutional layer&lt;/h3&gt;

&lt;p&gt;A convolutional layer applies a &lt;em&gt;stencil&lt;/em&gt; to each point. The output of a convolutional layer is an &amp;ldquo;image&amp;rdquo; of lower dimension, that is informative on some features of the input image (shapes, edges, etc.). The figure below shows how a convolutional layer works:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;convolution.gif&#34; alt=&#34;alt text&#34; /&gt;
source:&lt;a href=&#34;https://mitmath.github.io/18337/lecture14/pdes_and_convolutions&#34; target=&#34;_blank&#34;&gt;https://mitmath.github.io/18337/lecture14/pdes_and_convolutions&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;maxpool-layer&#34;&gt;Maxpool layer&lt;/h3&gt;

&lt;p&gt;A maxpool layer is a &lt;em&gt;stencil&lt;/em&gt; that selects the maximum value within a square. Below is an illustration of a maxpool layer applied to a $ 4 \times 4$ image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;maxpool.gif&#34; alt=&#34;alt text&#34; /&gt;
source:&lt;a href=&#34;https://mauriciocodesso.com/post/convolution-neural-network/&#34; target=&#34;_blank&#34;&gt;https://mauriciocodesso.com/post/convolution-neural-network/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;stride-and-padding&#34;&gt;Stride and Padding&lt;/h3&gt;

&lt;p&gt;When building a CNN, one must specify two hyper parameters: &lt;strong&gt;stride and padding&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When the stride is equal to 1, we move the filters one pixel at a time. When stride is equal to 2, we move the filters two pixel at a time, etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Padding refers to &amp;ldquo;adding zeroes&amp;rdquo; at the border of an image. Padding can be used to control the size of the output volume and helps in keeping information at the border of images&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is an example of a $3 \times 3$ filter applied to a $5 \times 5$ input padded with a $1 \times 1$ border of zeros using $2 \times 2$ strides:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;padding_strides.gif&#34; alt=&#34;alt text&#34; /&gt;
source: &lt;a href=&#34;http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html&#34; target=&#34;_blank&#34;&gt;http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The typical infrastructure of a CNN is first to apply a convolutional layer to the input image, then to use a maxpool layer, before using a fully-connected layer. Several &amp;ldquo;convolutional layer - maxpool layer&amp;rdquo; units can be stacked together before using a fully-connected (FC) layer. Note that an activation layer (often &lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&#34; target=&#34;_blank&#34;&gt;ReLU&lt;/a&gt;) is generally inserted between the the convolutional and the maxpool layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;architecture.png&#34; alt=&#34;alt text&#34; /&gt;
source: &lt;a href=&#34;https://towardsdatascience.com/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69&#34; target=&#34;_blank&#34;&gt;https://towardsdatascience.com/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;using-flux-jl&#34;&gt;Using Flux.jl&lt;/h2&gt;

&lt;p&gt;Flux.jl is a leading machine learning package in the Julia ecosystem. In what follows, we load both the train and the test samples of the MNIST dataset. The train sample is a set of images used to fine-tune the parameters of the CNN, while the test sample contains images used to check that we did not overfit the train sample. A smoking gun for overfitting is when the accuracy in the train sample is much better than the accuracy using images from the test sample.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Flux, Flux.Data.MNIST, Statistics
using Flux: onehotbatch, onecold, crossentropy, throttle
using Base.Iterators: repeated, partition
using Printf, BSON
using ImageView

# Load labels and images from Flux.Data.MNIST
# Train set: images used to estimate the CNN
train_labels = MNIST.labels(:train)
train_imgs = MNIST.images(:train);

# Test set: images used to see how well the CNN perform &amp;quot;out-of-the-sample&amp;quot;
test_imgs = MNIST.images(:test)
test_labels = MNIST.labels(:test)

print(&amp;quot;Images in the train set: $(size(train_imgs))&amp;quot;)
print(&amp;quot;Images in the test set: $(size(test_imgs))&amp;quot;)

# Visualization of one digit
NROWS, NCOLS = 28, 28
a = reshape(train_imgs[1], NROWS, NCOLS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Images in the train set: (60000,)Images in the test set: (10000,)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;CNN_13_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;cnn-architecture&#34;&gt;CNN architecture&lt;/h2&gt;

&lt;p&gt;Our CNN has the usual CONV-&amp;gt;ReLU-&amp;gt;MaxPool components, before using a FC layer. We use a $1 \times 1$ padding and a stride of $1$ (the default value). The size of input is gradually reduced by using $2 \times 2$ maxpool layers. The default activation in Flux.jl is the function is $ x-&amp;gt;x $. Here, we use the Rectified Linear Unit function (ReLU) instead:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;model = Chain(
    # First convolution, operating upon a 28x28 image
    Conv((3, 3), 1=&amp;gt;16, pad=(1,1), relu),
    MaxPool((2,2)), #maxpooling

    # Second convolution, operating upon a 14x14 image
    Conv((3, 3), 16=&amp;gt;32, pad=(1,1), relu),
    MaxPool((2,2)), #maxpooling

    # Third convolution, operating upon a 7x7 image
    Conv((3, 3), 32=&amp;gt;32,pad=(1,1), relu),
    MaxPool((2,2)),

    # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)
    # which is where we get the 288 in the `Dense` layer below:
    x -&amp;gt; reshape(x, :, size(x, 4)),
    Dense(288, 10),

    # Softmax to get probabilities
    softmax,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ReLU activation function is a piece-wise linear function. In the &lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&#34; target=&#34;_blank&#34;&gt;“ImageNet Classification with Deep Convolutional Neural Networks&amp;rdquo;&lt;/a&gt; paper by Krizhevsky and coauthors, the authors write:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The ReLU activation function also helps in reducing the practical issues caused by &lt;a href=&#34;https://en.wikipedia.org/wiki/Vanishing_gradient_problem&#34; target=&#34;_blank&#34;&gt;the vanishing gradient problem&lt;/a&gt;. That is, the failure of the minizimation algorithm used to find the parameters of our CNN. Below is a plot of the ReLU activation function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;xgrid = collect(range(-1, 1, length=100))
plot(xgrid, NNlib.relu.(xgrid), label = &amp;quot;relu(x)&amp;quot;, title=&amp;quot;ReLU activation function&amp;quot;, xlabel=&amp;quot;x&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;CNN_18_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;

&lt;h3 id=&#34;batching&#34;&gt;Batching&lt;/h3&gt;

&lt;p&gt;The batch size is a parameter that tells us how many images the network will &amp;ldquo;see&amp;rdquo; at once when &amp;ldquo;training&amp;rdquo;.
In technical terms, when performing gradient descent, we don&amp;rsquo;t use all the information at once (because of memory limitations and because it is not necessarily efficient). The following function generates &amp;ldquo;batches&amp;rdquo; of images:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Bundle images together with labels and group into minibatchess
function make_minibatch(X, Y, idxs)
    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))
    for i in 1:length(idxs)
        X_batch[:, :, :, i] = Float32.(X[idxs[i]])
    end
    Y_batch = onehotbatch(Y[idxs], 0:9)
    return (X_batch, Y_batch)
end
# The CNN only &amp;quot;sees&amp;quot; 128 images at each training cycle:
batch_size = 128
mb_idxs = partition(1:length(train_imgs), batch_size)
# train set in the form of batches
train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs];
# train set in one-go: used to calculate accuracy with the train set
train_set_full = make_minibatch(train_imgs, train_labels, 1:length(train_imgs));
# test set: to check we do not overfit the train data:
test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;loss-function-and-minimization&#34;&gt;Loss function and minimization&lt;/h3&gt;

&lt;p&gt;For the CNN to &amp;ldquo;learn&amp;rdquo; anything at all, it must have a notion of &amp;ldquo;wrong&amp;rdquo; or &amp;ldquo;right&amp;rdquo;. The loss function does exactly that, by quantifying how well the model performs at recognizing digits. When the output is a probability, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_entropy&#34; target=&#34;_blank&#34;&gt;cross entropy&lt;/a&gt; loss function is appropriate. The final step is to select an algorithm to minimize the loss function. Here, let&amp;rsquo;s select the &lt;a href=&#34;https://arxiv.org/abs/1412.6980&#34; target=&#34;_blank&#34;&gt;ADAM&lt;/a&gt; algorithm, which I understand as some sort of &lt;a href=&#34;https://julienpascal.github.io/post/ols_ml/&#34; target=&#34;_blank&#34;&gt;Stochastic Gradient Descent&lt;/a&gt; with momentum and adaptive learning rate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# `loss()` calculates the crossentropy loss between our prediction `y_hat`
# We augment the data a bit, adding gaussian random noise to our image to make it more robust.
function loss(x, y)
    # Add some noise to the image
    # we reduce the risk of overfitting the train sample by doing so:
    x_aug = x .+ 0.1f0*gpu(randn(eltype(x), size(x)))

    y_hat = model(x_aug)
    return crossentropy(y_hat, y)
end
accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))

# ADAM optimizer
opt = ADAM(0.001);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This block &amp;ldquo;train&amp;rdquo; (fine-tune the CNN parameter values) the model until a pre-determined accuracy level is reached:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;best_acc = 0.0
last_improvement = 0
accuracy_target = 0.97 #Set an accuracy target. When reached, we stop training.
max_epochs = 100 #Maximum
for epoch_idx in 1:100
    global best_acc, last_improvement
    # Train for a single epoch
    Flux.train!(loss, params(model), train_set, opt)

    # Calculate accuracy:
    acc = accuracy(train_set_full...)
    @info(@sprintf(&amp;quot;[%d]: Train accuracy: %.4f&amp;quot;, epoch_idx, acc))

    # Calculate accuracy:
    acc = accuracy(test_set...)
    @info(@sprintf(&amp;quot;[%d]: Test accuracy: %.4f&amp;quot;, epoch_idx, acc))

    # If our accuracy is good enough, quit out.
    if acc &amp;gt;= accuracy_target
        @info(&amp;quot; -&amp;gt; Early-exiting: We reached our target accuracy of $(accuracy_target*100)%&amp;quot;)
        break
    end

    if epoch_idx - last_improvement &amp;gt;= 10
        @warn(&amp;quot; -&amp;gt; We&#39;re calling this converged.&amp;quot;)
        break
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Info: [1]: Train accuracy: 0.9579
└ @ Main In[14]:12
┌ Info: [1]: Test accuracy: 0.9605
└ @ Main In[14]:16
┌ Info: [2]: Train accuracy: 0.9749
└ @ Main In[14]:12
┌ Info: [2]: Test accuracy: 0.9756
└ @ Main In[14]:16
┌ Info:  -&amp;gt; Early-exiting: We reached our target accuracy of 97.0%
└ @ Main In[14]:20
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;predictions&#34;&gt;Predictions&lt;/h2&gt;

&lt;p&gt;Once the model is trained, predicted values are easily obtained as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Get predictions and convert data to Array:
pred = Tracker.data(model(test_set[1]));
# Show the first 5 predictions
# One column is an image
# Each row corresponds to the probability of a digit
pred[:,1:5]
# Function to get the row index of the max value:
f1(x) = getindex.(argmax(x, dims=1), 1)
# Final predicted value is the one with the maximum probability:
pred = f1(pred) .- 1; #minus 1 because the first element is 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see how the model performs on the test set. Can the CNN recognize digits using images that were not used when training the model? As you can see below, our model does an amazing job at recognizing hand-written digits:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;println(&amp;quot;Predicted value = $(pred[1])&amp;quot;)
a = reshape(test_imgs[1], NROWS, NCOLS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Predicted value = 7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;CNN_30_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;println(&amp;quot;Predicted value = $(pred[2])&amp;quot;)
a = reshape(test_imgs[2], NROWS, NCOLS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Predicted value = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;CNN_31_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;println(&amp;quot;Predicted value = $(pred[3])&amp;quot;)
a = reshape(test_imgs[3], NROWS, NCOLS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Predicted value = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;CNN_32_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;accuracy-checks&#34;&gt;Accuracy checks&lt;/h3&gt;

&lt;p&gt;We now have a model that seems to do quite a good job in recognizing digits. But can we improve it? If yes, how?
To improve our model, we first need to identify when and why it fails.&lt;/p&gt;

&lt;h4 id=&#34;confusion-matrix&#34;&gt;Confusion matrix&lt;/h4&gt;

&lt;p&gt;To do that, a useful reporting
tool is a &lt;strong&gt;confusion matrix&lt;/strong&gt;. Each row of a confusion matrix shows instances of the true value, while each column displays instances of the predicted value. Ideally, we would like our model to perfectly predict the outcome. With a perfect model, all instances would be located on the diagonal elements of the confusion matrix.&lt;/p&gt;

&lt;p&gt;The last time I checked, &lt;code&gt;Flux.jl&lt;/code&gt; did not have an in-built function to calculate confusion matrices. Fortunately, an implementation is available in the package &lt;code&gt;MLBase&lt;/code&gt;. The next block of code calculates the confusion matrix and displays it. Most of instances are on located on the diagonal, which is not a surprise given that the accuracy rate for our model is more than $97.0\%$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using MLBase
# Adding 1 to outcome because the index 0 in arrays does not exist in Julia
Cm = confusmat(10, test_labels .+ 1, vec(pred) .+ 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;10×10 Array{Int64,2}:
 968     1     1    0    0    4    3    1    2    0
   0  1128     3    0    0    0    1    0    3    0
   3     5  1003    6    1    1    0    6    7    0
   0     0     1  992    0   10    0    2    4    1
   0     1     2    0  972    0    0    1    1    5
   1     0     1    4    0  883    1    1    1    0
   1     4     0    0    1   13  936    0    3    0
   1     7    10    5    0    1    0  986    3   15
   2     0     4    6    4    8    2    4  942    2
   4     4     0    7    7   10    0    8    2  967
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Normalize output:
Cm  = Cm ./ sum(Cm, dims=2)
# Labels
xs = [string(i) for i = 0:9]
heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;CNN_41_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To visualize where our model makes mistakes, one can use the optional argument &lt;code&gt;clim&lt;/code&gt;, to put an upper bound on
the underlying colormap. For instance, the next plot shows that our model has troubles differencing 7 and 2 or 8 and 2.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Limits to colormap, so we can see where errors are located:
xs = [string(i) for i = 0:9]
heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma, clim=(0., 0.01))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;CNN_43_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;error-analysis&#34;&gt;Error Analysis&lt;/h4&gt;

&lt;p&gt;The next block of code displays digits for which our CNN failed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# indices for errors:
using ImageView, Gtk.ShortNames
mistakes = test_labels .!= vec(pred)
max_images = 5
grid, frames, canvases = canvasgrid((1,max_images)); # 1 row
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;k=0#counter for mistakes
for (j, i) in enumerate(mistakes)
    if i == true
        k+=1 # a false value has been found
        println(&amp;quot;Predicted value = $(pred[j])&amp;quot;)
        println(&amp;quot;True value = $(test_labels[j])&amp;quot;)
        imshow(canvases[1,k], test_imgs[j])
        idx = ImageView.annotate!(guidict, AnnotationText(0, 0, &amp;quot;$(pred[j])&amp;quot;, color=RGB(0,0,0), fontsize=3))
    end
    if k &amp;gt;= max_images
        break
    end
end
win = Window(grid);
Gtk.showall(win);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Predicted value = 5
True value = 9
Predicted value = 5
True value = 6
Predicted value = 4
True value = 8
Predicted value = 3
True value = 2
Predicted value = 7
True value = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;false_values.png&#34; alt=&#34;alt&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While it seems obvious that the two digits starting from the left are a 9 and a 6, the remaining 3 elements are not trivial. The 8 in the middle could be easily confused with something else and the two remaining digits are weirdly shaped.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;When dealing with images, a convolutional neural network generally does an amazing job at recognizing patterns. This blog post was a non-technical introduction to the topic. While Python is the tool of predilection in machine learning (Keras, TensorFlow, etc.), my guess is that Julia will get increasingly popular because Julia is both easy to use and fast.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This blog post is heavily based on this Flux.jl tutorial: &lt;a href=&#34;https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl&#34; target=&#34;_blank&#34;&gt;https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the links between CNN and PDEs: &lt;a href=&#34;https://mitmath.github.io/18337/lecture14/pdes_and_convolutions&#34; target=&#34;_blank&#34;&gt;https://mitmath.github.io/18337/lecture14/pdes_and_convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A full course on CNN. Most of the content is available online: &lt;a href=&#34;http://cs231n.github.io/convolutional-networks/&#34; target=&#34;_blank&#34;&gt;http://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression from Scratch</title>
      <link>https://julienpascal.github.io/post/logistic/</link>
      <pubDate>Fri, 22 Nov 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/logistic/</guid>
      <description>

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34;&gt;logistic model&lt;/a&gt; (also called logit model) is a natural candidate when one is interested in a &lt;strong&gt;binary outcome&lt;/strong&gt;. For instance, a researcher might be interested in knowing what makes a politician successful or not. For the purpose of this blog post, &amp;ldquo;success&amp;rdquo;
means the probability of winning an election. In that case, it would be sub-optimal
to use a linear regression model to see what factors are associated with successful
politicians, as the outcome variable is binary (a politician either wins or loses an election).
The linear model is built around the idea that the outcome variable is continuous.&lt;/p&gt;

&lt;p&gt;What if the statistician tries to identify what factors are influencing the &lt;strong&gt;probability&lt;/strong&gt; of
winning? This strategy naturally lends itself to using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34;&gt;logistic model&lt;/a&gt; (or a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probit_model&#34; target=&#34;_blank&#34;&gt;probit&lt;/a&gt;).
In this blog post, I derive the &lt;strong&gt;logistic model from scratch&lt;/strong&gt; and show how one
can estimate its parameters using &lt;strong&gt;gradient descent&lt;/strong&gt; or &lt;strong&gt;Newton-Raphson&lt;/strong&gt; algorithms. I also use data on &lt;strong&gt;NBA players&lt;/strong&gt; to see what factors are influencing the &lt;strong&gt;success of a shot&lt;/strong&gt;. The GitHub repository for this post can be
found &lt;a href=&#34;https://github.com/JulienPascal/LogisticRegression&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-logistic-model&#34;&gt;The logistic model&lt;/h2&gt;

&lt;p&gt;The outcome variable $y_i$ is either $1$ (&amp;ldquo;winning&amp;rdquo;) or $0$ (&amp;ldquo;losing&amp;rdquo;). The logistic
model makes the assumption that the probability of winning is given by the logistic
function :&lt;/p&gt;

&lt;p&gt;$$ f(y_i | x_{i}, \theta_{i}) =  \sigma(x_{i} &amp;lsquo;\theta)$$&lt;/p&gt;

&lt;p&gt;with $\sigma(v) = \frac{exp(v)}{1+exp(v)}$&lt;/p&gt;

&lt;p&gt;The probability of losing is 1 minus the probability of wining:&lt;/p&gt;

&lt;p&gt;$$ f(y_i | x_{i}, \theta) =  1 - \sigma(x_{i} &amp;lsquo;\theta)$$&lt;/p&gt;

&lt;h2 id=&#34;a-latent-variable-formulation&#34;&gt;A latent variable formulation&lt;/h2&gt;

&lt;p&gt;A powerful way of interpreting the logistic model is to see it as the outcome of a latent variable model.
An unobservable latent variable $z_{i}$ depends linearly on $x_{i}$ plus a noise term $\varepsilon_{i}$:&lt;/p&gt;

&lt;p&gt;$$ z_{i} = x_{i} &amp;lsquo;\theta + \varepsilon_{i} $$&lt;/p&gt;

&lt;p&gt;We only observe $y_i$, which is equal to 1 when $z_{i}$ is strictly positive, and 0 otherwise. If the error term is distributed according to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_distribution&#34; target=&#34;_blank&#34;&gt;logistic distribution&lt;/a&gt;, we end up with the logistic model described above. If the error term is normally distributed, the model is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probit_model&#34; target=&#34;_blank&#34;&gt;probit model&lt;/a&gt;. To see that, simply express the probability of the latent variable to be bigger than 0:&lt;/p&gt;

&lt;p&gt;$$ f(y_i | x_{i}, \theta_{i}) = P( x_{i} &amp;lsquo;\theta + \varepsilon_{i} &amp;gt; 0) $$
$$  = 1 - P( x_{i} &amp;lsquo;\theta + \varepsilon_{i} \leq 0) $$
$$  = 1 - P(\varepsilon_{i} \leq - x_{i} &amp;lsquo;\theta ) $$
$$  = 1 - P(\varepsilon_{i} \leq - x_{i} &amp;lsquo;\theta ) $$
$$  = \frac{exp(x_{i} &amp;lsquo;\theta )}{1+exp(x_{i} &amp;lsquo;\theta )} $$&lt;/p&gt;

&lt;p&gt;where the last line comes from using the expression for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_distribution&#34; target=&#34;_blank&#34;&gt;cdf of the logistic distribution&lt;/a&gt; with zero mean and scale parameter equal to 1.&lt;/p&gt;

&lt;h2 id=&#34;interpretation-of-coefficients&#34;&gt;Interpretation of coefficients&lt;/h2&gt;

&lt;p&gt;How can we read the coefficients from a logistic model? The marginal effect of a change in $x_{ij}$ (the $jth$ component of $x_i$) on the probability that $y_i = 1$ is given by:&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial f(y_i | x_{i}, \theta)}{\partial x_{ij}} = \sigma(x_{i} &amp;lsquo;\theta)(1-\sigma(x_{i} &amp;lsquo;\theta))\theta_j$$&lt;/p&gt;

&lt;p&gt;A first observation is that the marginal effect depends on $x_i$, unlike in the linear regression model. A second observation is that the first two terms are always positive, so we do have that the interpretation that if $\theta_j$ is positive, an increase in the $jth$ component of $x_i$ leads to a bigger probability of obtaining a success (holding everything else constant).&lt;/p&gt;

&lt;p&gt;Another way to read the results from a logistic model is to realize that it implies that the log of odd ratio is linear:&lt;/p&gt;

&lt;p&gt;$$ log\Big(\frac{f(y_i | x_{i}, \theta)}{1 - f(y_i | x_{i}, \theta)}\Big) = x_{i} &amp;lsquo;\theta$$&lt;/p&gt;

&lt;p&gt;Going back to what makes a politician successful in an election, if the coefficient $\theta_j$ is equal to 0.1, it means that a one unit increase in $x_{ij}$ rises the &lt;strong&gt;relative&lt;/strong&gt; probability of winning an election by approximately $10\%$.&lt;/p&gt;

&lt;h2 id=&#34;log-likelihood-function&#34;&gt;Log-likelihood function&lt;/h2&gt;

&lt;p&gt;To predict who is going to win the next elections, one must estimate the value of $\theta$ using the information contained in the sample $(y_i, x_i)_{i=1}^{N}$. One &amp;ldquo;natural&amp;rdquo; criterion is to find the value for $\theta$ that &lt;strong&gt;maximizes the probability of observing the
sample&lt;/strong&gt;. This procedure is called &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34; target=&#34;_blank&#34;&gt;Maximum likelihood estimation&lt;/a&gt;. Let us assume that sample is &lt;a href=&#34;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&#34; target=&#34;_blank&#34;&gt;i.i.d&lt;/a&gt;. If the i.i.d assumption holds, the probability of observing the sample $(y_i, x_i)_{i=1}^{N}$ is the product of the probability of observing each observation. Instead of maximizing the likelihood, it is more convenient to maximize the log-likelihood, which transforms the product of probabilities into a sum:&lt;/p&gt;

&lt;p&gt;$$ L((y_i, x_i)_{i=1}^{N};\theta) = log( \prod_{i=1}^{N}f(y_i | x_{i}, \theta)) = \sum_{i=1}^{N} log(f(y_i | x_{i}, \theta_{i}))$$&lt;/p&gt;

&lt;p&gt;The probability of observing $y_i$  can compactly be written as&lt;/p&gt;

&lt;p&gt;$$ f(y_i | x_{i}, \theta_{i}) = \sigma(x_{i} &amp;lsquo;\theta)^{y_i}(1 - \sigma(x_{i} &amp;lsquo;\theta))^{1 - y_i} $$&lt;/p&gt;

&lt;p&gt;Hence, the log-likelihood function writes:&lt;/p&gt;

&lt;p&gt;$$L((y_i, x_i)_{i=1}^{N};\theta) = \sum_{i=1}^{N} y_i log(\sigma(x_{i} &amp;lsquo;\theta)) + (1 - y_i)log(1 - \sigma(x_{i} &amp;lsquo;\theta))$$&lt;/p&gt;

&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;

&lt;p&gt;Taking the derivative of $f(y_i | x_{i}, \theta)$ with respect to the parameter $\theta$ gives:&lt;/p&gt;

&lt;p&gt;$$ f_{\theta}(y_i | x_{i}, \theta) = [y_i - \sigma(x_{i} &amp;lsquo;\theta)] x_{i} $$&lt;/p&gt;

&lt;p&gt;and the derivative of the log-likelihood function with respect to $\theta$ is:&lt;/p&gt;

&lt;p&gt;$$ L_{\theta}((y_i, x_i)_{i=1}^{N};\theta) = \sum_{i=1}^{N}[y_i - \sigma(x_{i} &amp;lsquo;\theta)] x_{i} $$&lt;/p&gt;

&lt;h2 id=&#34;gradient-descent&#34;&gt;Gradient descent&lt;/h2&gt;

&lt;p&gt;To make the link with this &lt;a href=&#34;https://julienpascal.github.io/post/ols_ml/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;, we can use gradient descent to find the MLE estimate:&lt;/p&gt;

&lt;p&gt;$$ \theta_{i+1} = \theta_{i} - \gamma \Big(- L_{\theta}((y_i, x_i)_{i=1}^{N};\theta_{i}) \Big)$$&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34; target=&#34;_blank&#34;&gt;gradient descent&lt;/a&gt; algorithm is an iterative procedure to find a minimizer of a function. At each step, the algorithm takes a step of length $\gamma$ towards the direction of steepest descent. Note that I reformulated the problem of finding the maximum of a function $f$ (the log-likelihood) as the problem of finding the minimum of $-f$.&lt;/p&gt;

&lt;h2 id=&#34;newton-raphson-method&#34;&gt;Newton–Raphson method&lt;/h2&gt;

&lt;p&gt;Roughly speaking, the Newton-Raphson method is a &amp;ldquo;smart&amp;rdquo; gradient descent which uses the information contained in the Hessian of the log-likelihood $HL((y_i, x_i)_{i=1}^{N};\theta_{i})$ (on top of the gradient) to make a right move toward the minimizer. This iterative algorithm proceeds as follows:&lt;/p&gt;

&lt;p&gt;$$ \theta_{i+1} = \theta_{i} - (HL((y_i, x_i)_{i=1}^{N};\theta_{i}) )^{-1}  \Big(- L_{\theta}((y_i, x_i)_{i=1}^{N};\theta_{i}) \Big)$$&lt;/p&gt;

&lt;p&gt;The next plot shows how the Newton-Raphson method works for a one dimensional root-finding problem:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif&#34; alt=&#34;Alt Text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_method&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Newton%27s_method&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Should we use gradient descent or Newton-Raphson? I let the following extract
from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization&#34; target=&#34;_blank&#34;&gt;Wikipedia article&lt;/a&gt; on Newton-Raphson speak for itself:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Where applicable&lt;/strong&gt;, Newton&amp;rsquo;s method &lt;strong&gt;converges much faster&lt;/strong&gt; towards a local maximum or minimum than gradient descent. In fact, every local minimum has a neighborhood N such that, if we start with x0 ∈ N, Newton&amp;rsquo;s method with step size γ = 1 &lt;strong&gt;converges quadratically&lt;/strong&gt; (if the Hessian is invertible and a Lipschitz continuous function of x in that neighborhood).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For the logistic model, the Newton-Raphson algorithm is easily applicable because there exists a closed-form formula for the Hessian:&lt;/p&gt;

&lt;p&gt;$$ HL((y_i, x_i)_{i=1}^{N};\theta_{i}) = \sum_{i=1}^{N} - \sigma(x_{i} &amp;lsquo;\theta)[1 - \sigma(x_{i} &amp;lsquo;\theta)] x_{i} x_{i}&amp;lsquo;$$&lt;/p&gt;

&lt;h2 id=&#34;implementation-in-julia&#34;&gt;Implementation in Julia&lt;/h2&gt;

&lt;h2 id=&#34;i-working-with-simulated-data&#34;&gt;I. Working with simulated data&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s first work with simulated data. Can we actually recover the true parameter values using a manual implementation of the logistic model?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s load a few dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions
using Plots
pyplot()
using DataFrames
using GLM
using Optim
using CSV
using GLM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s create the logistic function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Logistic function for a scalar input:
function sigma(x::Float64)
    exp(x)/(1.0 + exp(x))
end

# Logistic function for a vector input:
function sigma(x::Array{Float64,1})
    exp.(x) ./ (1.0 .+ exp.(x))
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s create a function that calculates the likelihood:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1})
    sum = 0.0
    #Loop over individuals in the sample
    for i=1:size(X,1)
        sum += y[i]*log(sigma(transpose(X[i,:])*theta)) + (1.0 - y[i])*log(1.0 - sigma(transpose(X[i,:])*theta))
    end
    return sum
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s create a function that returns the derivative of the log-likelihood of the sample, which we need for the gradient descent algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Function to calculate the gradient of the log-likelihood of the sample:
function derivative_log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1})
    sum = zeros(size(X,2))
    #Loop over individuals in the sample
    for i=1:size(X,1)
        sum .+= (y[i] - sigma(transpose(X[i,:])*theta))*X[i,:]
    end
    return sum
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s create a function that returns the Hessian of the log-likelihood of the sample, which we need for the Newthon-Raphson algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Function to calculate the hessian of the log-likelihood of the sample:
function hessian_log_likelihood(y::Array{Float64,1}, X::Array{Float64,2}, theta::Array{Float64,1})
    hessian = zeros(size(X,2), size(X,2))
    #Loop over individuals in the sample
    for i=1:size(X,1)
        hessian .+= - sigma(transpose(X[i,:])*theta)*(1.0 - sigma(transpose(X[i,:])*theta))*(X[i,:]*transpose(X[i,:]))
    end
    return hessian
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s simulate a sample of individuals:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Generation of a sample:
#----------------------
N_individuals = 10000 #how many individuals in the sample?
dim_X = 3 #How many dimensions for x
d = Normal(0.0, 1.0)
d_logistic = Logistic(0.0, 1.0)
# Generate true parameter values:
theta0 = [0.0; 1.0; 2.0];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Generate X:
X = rand(d, N_individuals, dim_X)
# The first column is full one ones (to have a constant)
X[:,1] = ones(N_individuals);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Convert y to a binary outcome using the latent variabe representation:
proba_success = X*theta0 .+ rand(d_logistic, N_individuals)
y = ifelse.(proba_success .&amp;gt; 0.0, 1.0, 0.0);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p1 = histogram(proba_success, bins=20, normalize=true, title=&amp;quot;Pdf probability of success&amp;quot;, legend=false)
p2 = histogram(y, title=&amp;quot;Nb of successes vs failures&amp;quot;, legend=false)
plot(p1,p2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Logistic_regression_29_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;maximization-with-optim&#34;&gt;Maximization with Optim&lt;/h3&gt;

&lt;p&gt;As a first pass, we can maximize the log-likelihood using the package Optim. I use the the &lt;a href=&#34;https://en.wikipedia.org/wiki/Limited-memory_BFGS&#34; target=&#34;_blank&#34;&gt;LBFGS&lt;/a&gt;
algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;theta_guess = ones(dim_X)
@time res = optimize(theta -&amp;gt; - log_likelihood(y, X, theta), theta_guess, LBFGS())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.409340 seconds (3.79 M allocations: 396.250 MiB, 15.73% gc time)&lt;/p&gt;

&lt;p&gt;We successfully recover the true parameter values (see &lt;code&gt;theta0&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;Estimate for theta using Optim is $(res.minimizer)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Estimate for theta using Optim is [-0.0312666, 0.996344, 1.96038]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;minimization-with-gradient-descent&#34;&gt;Minimization with gradient descent&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s implement the gradient descent algorithm within a function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function gradient_descent_probit(y, X , theta_initial::Array{Float64,1}; max_iter::Int64 = 1000,
                                learning_rate::Float64 = 0.000001, tol::Float64=0.01)
    #initial value for theta:
    theta_old = theta_initial
    theta_new = similar(theta_old)
    #convergence reached?
    success_flag = 0
    #Let&#39;s store the convergence history
    history= fill!(zeros(max_iter), NaN)
    for i=1:max_iter
        theta_new = theta_old + learning_rate*derivative_log_likelihood(y, X, theta_old)
        diff = maximum(abs, theta_new .- theta_old)
        history[i] = diff
        if diff &amp;lt; tol
            success_flag = 1
            break
        end
        theta_old = theta_new
    end

    return theta_new, success_flag, history[isnan.(history) .== false]

end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;theta_guess = zeros(dim_X)
@time theta, flag, history = gradient_descent_probit(y, X, theta_guess, max_iter=100000, learning_rate=0.0001, tol=0.00001);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.252281 seconds (4.89 M allocations: 523.159 MiB, 29.08% gc time)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following graph shows the error as a function of the number of iterations. After a few iterations of the gradient descent algorithm, the error is quite small.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(history, label= &amp;quot;error&amp;quot;, title = &amp;quot;Convergence of the gradient descent algorithm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Logistic_regression_40_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;Estimate for theta using gradient descent is $(theta)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Estimate for theta using gradient descent is [0.0581777, 1.00105, 1.97901]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;minimization-with-newton-raphson&#34;&gt;Minimization with Newton-Raphson&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s implement the Newton-Raphson algorithm within a function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function nr_probit(y, X , theta_initial::Array{Float64,1};
                    max_iter::Int64 = 1000, tol::Float64=0.01)
    #initial value for theta:
    theta_old = theta_initial
    theta_new = similar(theta_old)
    #convergence reached?
    success_flag = 0
    #Let&#39;s store the convergence history
    history= fill!(zeros(max_iter), NaN)
    for i=1:max_iter
        theta_new = theta_old - inv(hessian_log_likelihood(y, X, theta_old))*derivative_log_likelihood(y, X, theta_old)
        diff = maximum(abs, theta_new .- theta_old)
        history[i] = diff
        if diff &amp;lt; tol
            success_flag = 1
            break
        end
        theta_old = theta_new
    end

    return theta_new, success_flag, history[isnan.(history) .== false]

end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following graph shows that we find the minimizer in only 5 steps! The Newton-Raphson algorithm clearly outperforms gradient descent. Of course, everything works well because the
problem is well-behaved and a nice formula for the Hessian is available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;theta_guess = ones(dim_X)
@time theta, flag, history = nr_probit(y, X, theta_guess, max_iter=1000, tol=0.00001);
plot(history, label= &amp;quot;error&amp;quot;, title = &amp;quot;Convergence of the gradient descent algorithm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.037832 seconds (500.07 k allocations: 53.431 MiB, 35.44% gc time)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Logistic_regression_46_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;Estimate for theta using Newton-Raphson is $(theta)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Estimate for theta using Newton-Raphson is [0.0581789, 1.00114, 1.97919]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-glm&#34;&gt;Using GLM&lt;/h3&gt;

&lt;p&gt;We can also use the package &lt;a href=&#34;https://github.com/JuliaStats/GLM.jl&#34; target=&#34;_blank&#34;&gt;GLM&lt;/a&gt; to estimate the logistic model. We first need to put the data into a dataframe. In the &lt;code&gt;glm()&lt;/code&gt; function, we should use the &lt;code&gt;LogitLink()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;df = DataFrame(X1=X[:,1],X2=X[:,2], X3=X[:,3], y=y);
first(df,6)
&lt;/code&gt;&lt;/pre&gt;

&lt;table class=&#34;data-frame&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;X1&lt;/th&gt;&lt;th&gt;X2&lt;/th&gt;&lt;th&gt;X3&lt;/th&gt;&lt;th&gt;y&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;p&gt;6 rows × 4 columns&lt;/p&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;td&gt;0.215315&lt;/td&gt;&lt;td&gt;1.29817&lt;/td&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;td&gt;-0.0714749&lt;/td&gt;&lt;td&gt;0.586886&lt;/td&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;td&gt;0.0463648&lt;/td&gt;&lt;td&gt;-0.145116&lt;/td&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;td&gt;1.95096&lt;/td&gt;&lt;td&gt;0.26349&lt;/td&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;5&lt;/th&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;td&gt;-0.162081&lt;/td&gt;&lt;td&gt;1.34871&lt;/td&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;6&lt;/th&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;td&gt;0.00214326&lt;/td&gt;&lt;td&gt;-0.271835&lt;/td&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;fittedmodel = glm(@formula(y ~ X2 + X3), df, Binomial(), LogitLink(), verbose=true);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;Estimate for theta using GLM is $(coef(fittedmodel))&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Estimate for theta using GLM is [-0.0312666, 0.996344, 1.96038]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ii-what-makes-a-successful-nba-player&#34;&gt;II. What makes a successful NBA player?&lt;/h2&gt;

&lt;p&gt;For an example involving real data, I use the data set on &lt;strong&gt;NBA shots&lt;/strong&gt; taken during the 2014-2015 season.
It contains information on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;who took the shot&lt;/li&gt;
&lt;li&gt;where on the floor was the shot taken from&lt;/li&gt;
&lt;li&gt;who was the nearest defender,&lt;/li&gt;
&lt;li&gt;how far away was the nearest defender&lt;/li&gt;
&lt;li&gt;time on the shot clock&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The data is available on &lt;strong&gt;Kaggle&lt;/strong&gt; &lt;a href=&#34;https://www.kaggle.com/dansbecker/nba-shot-logs&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;df_nba = CSV.read(&amp;quot;/home/julien/Documents/REPOSITORIES/LogisticRegression/data/shot_logs.csv&amp;quot;);
names(df_nba)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The dataset is quite extensive. Let&amp;rsquo;s select whether or not the shot was successful, &lt;strong&gt;the shot clock, the shot distance, and the proximity with the closest defender&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;df_nba = df_nba[[:SHOT_RESULT, :SHOT_CLOCK, :SHOT_DIST, :CLOSE_DEF_DIST]]
# Drop rows with missings:
df_nba = dropmissing(df_nba);
# Drop rows with NaN:
df_nba = df_nba[completecases(df_nba), :]
# Convert SHOT_RESULT to a binary variable (1 for success, 0 for missed)
df_nba[:, :SHOT_RESULT] = ifelse.(df_nba[:, :SHOT_RESULT] .== &amp;quot;made&amp;quot;, 1.0, 0.0);
# Show the first few rows of df_nba:
first(df_nba, 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;table class=&#34;data-frame&#34;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;SHOT_RESULT&lt;/th&gt;&lt;th&gt;SHOT_CLOCK&lt;/th&gt;&lt;th&gt;SHOT_DIST&lt;/th&gt;&lt;th&gt;CLOSE_DEF_DIST&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;th&gt;Float64&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;p&gt;4 rows × 4 columns&lt;/p&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;td&gt;1.0&lt;/td&gt;&lt;td&gt;10.8&lt;/td&gt;&lt;td&gt;7.7&lt;/td&gt;&lt;td&gt;1.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;3.4&lt;/td&gt;&lt;td&gt;28.2&lt;/td&gt;&lt;td&gt;6.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;10.3&lt;/td&gt;&lt;td&gt;17.2&lt;/td&gt;&lt;td&gt;3.4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;10.9&lt;/td&gt;&lt;td&gt;3.7&lt;/td&gt;&lt;td&gt;1.1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;Let&amp;rsquo;s first use GLM:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;fittedmodel = glm(@formula(SHOT_RESULT ~ SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST), df_nba, Binomial(), LogitLink(), verbose=true);
fittedmodel
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;SHOT_RESULT ~ 1 + SHOT_CLOCK + SHOT_DIST + CLOSE_DEF_DIST

Coefficients:
────────────────────────────────────────────────────────────────────────────────────
                  Estimate   Std. Error    z value  Pr(&amp;gt;|z|)   Lower 95%   Upper 95%
────────────────────────────────────────────────────────────────────────────────────
(Intercept)     -0.0575127  0.0181349     -3.17139    0.0015  -0.0930564  -0.021969
SHOT_CLOCK       0.0185198  0.00104899    17.6549     &amp;lt;1e-69   0.0164639   0.0205758
SHOT_DIST       -0.059745   0.000858282  -69.61       &amp;lt;1e-99  -0.0614272  -0.0580628
CLOSE_DEF_DIST   0.108392   0.00279232    38.8179     &amp;lt;1e-99   0.102919    0.113865
────────────────────────────────────────────────────────────────────────────────────
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How can we interpret those results?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;time pressure makes NBA players more successful: the higher the shot clock, the more likely to score&lt;/li&gt;
&lt;li&gt;shots from further away are more likely to be missed&lt;/li&gt;
&lt;li&gt;the further away the closest defender is, the more likely the shot will be a success&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Can we find similar results &amp;ldquo;manually&amp;rdquo;? The answer is &lt;strong&gt;yes&lt;/strong&gt;. To see that, let&amp;rsquo;s first create the binary variable &lt;code&gt;y&lt;/code&gt; and put the explanatory variables into &lt;code&gt;X&lt;/code&gt; and then use Newton-Raphson:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;y = convert(Array, df_nba[:SHOT_RESULT]);
X = convert(Matrix, df_nba[[:SHOT_CLOCK, :SHOT_DIST, :CLOSE_DEF_DIST]])
X = hcat(ones(size(X,1)), X);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;theta_guess = zeros(size(X,2))
@time theta, flag, history = nr_probit(y, X, theta_guess, max_iter=1000, tol=0.0001);
plot(history, label= &amp;quot;error&amp;quot;, title = &amp;quot;Convergence of the gradient descent algorithm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.301748 seconds (4.90 M allocations: 568.272 MiB, 29.93% gc time)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Logistic_regression_63_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our implementation of the logistic model gives us parameter values that are almost
identical to the ones we get using the package &lt;code&gt;GLM&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;print(&amp;quot;Estimate for theta using Optim is $(res.minimizer)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Estimate for theta using Optim is [-0.057513, 0.0185199, -0.0597451, 0.108392]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The logistic model, often used in social sciences and in machine learning for classification purposes is a powerful tool. This blog post shows how the logistic model can be derived from first principles (latent variable interpretation) and how it can be implemented in just a few lines of codes. A few extensions to this blog post could be to calculate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34; target=&#34;_blank&#34;&gt;ROC curve&lt;/a&gt; and to calculate the standard errors.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rpubs.com/junworks/Understanding-Logistic-Regression-from-Scratch&#34; target=&#34;_blank&#34;&gt;https://rpubs.com/junworks/Understanding-Logistic-Regression-from-Scratch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Rental Housing Market and Directed Search</title>
      <link>https://julienpascal.github.io/project/rentalmarket/</link>
      <pubDate>Mon, 21 Oct 2019 18:11:07 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/project/rentalmarket/</guid>
      <description>&lt;p&gt;Using a novel dataset on the rental housing market in the Paris area, I show that the rental housing market is well described by a directed search model. I develop a hedonic pricing model taking into consideration apartment features and subjective attractiveness using photos and computer vision techniques from the machine learning literature.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;https://julienpascal.github.io/slides/rental-market/#/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/icons/beamer_small.png&#34;&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker for Dummies</title>
      <link>https://julienpascal.github.io/post/docker/</link>
      <pubDate>Fri, 18 Oct 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/docker/</guid>
      <description>

&lt;p&gt;In my quest for the perfect tool for reproducible science, I thought that the silver
bullet was to wrap your code in a neat library/package and make it available to the world.
Yet, I was wrong. &lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34;&gt;Docker&lt;/a&gt; is a much cooler and a much more effective way for sharing
your work with a broad audience. This post is a 101 introduction to Docker.
I describe what is Docker and show one simple application with a script in Julia.&lt;/p&gt;

&lt;h2 id=&#34;why-docker&#34;&gt;Why Docker?&lt;/h2&gt;

&lt;p&gt;Your script works locally, but not on your friend&amp;rsquo;s laptop because of dependency
issues. Docker solves the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dependency_hell&#34; target=&#34;_blank&#34;&gt;dependency hell&lt;/a&gt;
by giving you the opportunity to &amp;ldquo;ship&amp;rdquo; your application in a &amp;ldquo;container&amp;rdquo;. One can think of a &amp;ldquo;container&amp;rdquo; as some sort of lightweight virtual image. Some technical details can be found &lt;a href=&#34;https://docs.docker.com/engine/docker-overview/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://devopscube.com/what-is-docker/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. In a nutshell, if your application works on your local machine, Docker helps you to put your application inside an container. Once in a container, your application will run smoothly for the rest of the world.&lt;/p&gt;

&lt;h2 id=&#34;application-in-julia&#34;&gt;Application in Julia&lt;/h2&gt;

&lt;p&gt;The goal is to create a container with a simple script to calculate an approximation
of π. Here I am making a copy-paste from &lt;a href=&#34;https://julienpascal.github.io/post/buildyourcluster/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;, in which I calculated an approximation of π using Monte-Carlo. I create a folder
&lt;code&gt;julia-app&lt;/code&gt;, which contains 3 files (see the Github repository with the 3 files &lt;a href=&#34;https://github.com/JulienPascal/MyFirstDockerApp&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;julia-app
  ├ app.jl
  ├ deps.jl
  └ Dockerfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The file &lt;code&gt;app.jl&lt;/code&gt; contains the application we want to containerize. The file &lt;code&gt;deps.jl&lt;/code&gt;
contains the list of libraries/packages that are used within &lt;code&gt;app.jl&lt;/code&gt;. The file &lt;code&gt;Dockerfile&lt;/code&gt;
is a &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34; target=&#34;_blank&#34;&gt;text document&lt;/a&gt; that contains instructions to build the container.
Generally, a &lt;code&gt;Dockerfile&lt;/code&gt; contains 4 types of instructions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;FROM&lt;/code&gt;: specifies the &amp;ldquo;base image&amp;rdquo; we want to use within the container. In our case,
we want to run an application with Julia. Luckily, we can pull a base image with
Julia pre-installed on it using &lt;code&gt;FROM julia:&amp;lt;julia-version-you-want&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;COPY&lt;/code&gt;: adds files to your container&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RUN&lt;/code&gt;: executes command(s) in a new layer and creates a new image. &lt;code&gt;RUN&lt;/code&gt; is perfect
for installing packages&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CMD&lt;/code&gt;: specifies what command to run within the container&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;#choose a base image
FROM julia:1.0.3

# install julia dependencies
COPY deps.jl /usr/src/app/
RUN julia /usr/src/app/deps.jl

# copy files required for the app to run
COPY app.jl /usr/src/app/

# run the application
CMD [&amp;quot;julia&amp;quot;, &amp;quot;/usr/src/app/app.jl&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create the container, use the command &lt;code&gt;docker build&lt;/code&gt;. You can give a name
to your container using the &lt;code&gt;--tag , -t&lt;/code&gt; option:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker build julia-app -t &amp;lt;your-tag&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To run the container you have just created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --name julia-app &amp;lt;your-tag&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few seconds, you should see an approximation of π showing up in your
terminal. Voilà, you have have just created your first container. The next step
is to put your container on &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34;&gt;DockerHub&lt;/a&gt;. &lt;a href=&#34;https://docs.docker.com/docker-hub/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt;
is a tutorial on how to do it.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;From an academic perspective, Docker solves the dependency hell and helps in producing
reproducible research. I will try to use it more often for sharing my own research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OLS the Machine Learning Way</title>
      <link>https://julienpascal.github.io/post/ols_ml/</link>
      <pubDate>Sun, 29 Sep 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/ols_ml/</guid>
      <description>

&lt;p&gt;Coming from an Economics/Econometrics background, I have always been a bit puzzled by the way several Machine Learning (ML) textbooks I have read solve the ordinary least squares model (OLS). When considering a linear model of the form:&lt;/p&gt;

&lt;p&gt;$$ y = X \beta + e $$&lt;/p&gt;

&lt;p&gt;with $e$ a zero-mean noise term, the closed-form solution associated with minimizing the mean square error is:&lt;/p&gt;

&lt;p&gt;$$ \beta = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y $$&lt;/p&gt;

&lt;p&gt;Several ML textbooks explain that a recursive algorithm (see below) may be used to solve for $\beta$. Why not directly using the analytical formula to calculate an estimate of $\beta$ ? While feasible with &amp;ldquo;small&amp;rdquo; datasets (not too many explanatory variables and/or observations), direct inversion of $X&amp;rsquo;X$ is not recommended when working with thousands of explanatory variables and/or billions of observations. The alternative is to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34; target=&#34;_blank&#34;&gt;gradient descent&lt;/a&gt;, or better, &lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&#34; target=&#34;_blank&#34;&gt;stochastic gradient descent&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this short post, I solve OLS the &amp;ldquo;machine-learning way&amp;rdquo;. That is, using (stochastic) gradient descent. The idea for gradient descent (GD) is quite intuitive. The gradient of $f$ at a given point tells us the direction of &lt;strong&gt;greatest increase&lt;/strong&gt; for $f$ at this point. Hence, moving in the opposite direction (minus the gradient) is probably a good idea to find a local minimum. And indeed it is. The GD algorithm repetitively applies this procedure until a minimum (hopefully global) is found. Starting from an initial guess for $\beta$, one updates the guess using the following formula:&lt;/p&gt;

&lt;p&gt;$$ \beta_{n} = \beta_{n-1} - \alpha * grad_{n}(\beta_{n-1},X,y) $$&lt;/p&gt;

&lt;p&gt;where $\alpha$ is a small value (the &amp;ldquo;learning rate&amp;rdquo;) and $grad_{n}(\beta_{n-1},X,y)$ the gradient of the mean square error (another loss function can be used) evaluated at $\beta_{n-1}$ using the observations $X$ and $y$. Using the mean square error as a loss function generates a closed-form solution for the gradient:&lt;/p&gt;

&lt;p&gt;$$ grad_{n}(\beta_{n-1},X,y) = (X&amp;rsquo;X)\beta - X&amp;rsquo;y $$&lt;/p&gt;

&lt;p&gt;A refinement of GD, especially handy when dealing with a large dataset, is to use only a subset of the full sample when calculating the gradient:&lt;/p&gt;

&lt;p&gt;$$ \beta_{n} = \beta_{n-1} - \alpha * grad_{n}(\beta_{n-1},X_n,y_n) $$&lt;/p&gt;

&lt;p&gt;where $X_n$ and $y_n$ are a randomly selected sub-sample of $X$ and $y$. Stochastic Gradient Descent (SGD) reduces the computational burden associated with computing the gradient, while still having good convergence properties, as illustrated in the application below.&lt;/p&gt;

&lt;h2 id=&#34;implementation-in-julia&#34;&gt;Implementation in Julia&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s first load packages and define parameters&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using LinearAlgebra
using Distributions
using Plots
using Distributions
using Random
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;n_points=10000
dim_input=100 #dim of input, without the intercept
dim_output=1
# Normal noise
d = Normal()
# True parameters
beta = rand(d, dim_input + 1);
# Noise
e = rand(d, n_points);
# Input data:
X = rand(d, (n_points,dim_input));
# Add the intercept:
X = hcat(ones(n_points),X);
#Linear Model
y = X*beta .+ e;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function calculates an estimate of $\beta$ using the analytical formula for OLS&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#OLS way
function OLS_direct(X::Array, y::Vector)
    inv(transpose(X)*X)*transpose(X)*y
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OLS_direct (generic function with 1 method)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@time beta_hat = OLS_direct(X, y);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.611144 seconds (1.85 M allocations: 96.576 MiB, 13.64% gc time)&lt;/p&gt;

&lt;p&gt;Without any major surprise, using the analytical solution works perfectly well, as illustrated in the following plot&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(beta, beta_hat, seriestype=:scatter, label=&amp;quot;OLS (Direct)&amp;quot;)
plot!(beta, beta, seriestype=:line, label=&amp;quot;true&amp;quot;)
xlabel!(&amp;quot;true value&amp;quot;)
ylabel!(&amp;quot;estimated value (Direct)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;OLSMachineLearning_9_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h2&gt;

&lt;p&gt;Now it&amp;rsquo;s time to solve OLS the machine learning way. I first define a function that calculates the gradient of the loss
function, evaluated at the current guess using the full sample. Then, a second function applies the GD updating rule.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Calculate the gradient
function grad_OLS!(G, beta_hat, X, y)
    G[:] = transpose(X)*X*beta_hat - transpose(X)*y
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;grad_OLS! (generic function with 1 method)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Gradient descent way
function OLS_gd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false)
    #initial guess
    beta_hat = zeros(size(X,2))
    grad_n = zeros(size(X,2))
    for epoch=1:epochs
        grad_OLS!(grad_n, beta_hat, X, y)
        beta_hat -= r*grad_n
        if verbose==true
            if mod(epoch, round(Int, epochs/10))==1
                println(&amp;quot;MSE: $(mse(beta_hat, X, y))&amp;quot;)
            end
        end
    end
    return beta_hat
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OLS_gd (generic function with 1 method)&lt;/p&gt;

&lt;p&gt;As illustrated below, after 20 iterations we are quite close to the true value. After 100 iterations, values obtained by GD are indistinguishable from the true values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@time beta_hat_gd_20 = OLS_gd(X,y, epochs=20);
@time beta_hat_gd = OLS_gd(X,y, epochs=100);
plot(beta, beta_hat_gd_20, seriestype=:scatter, label=&amp;quot;GD (20 iter.)&amp;quot;)
plot!(beta, beta_hat_gd, seriestype=:scatter, label=&amp;quot;GD (100 iter.)&amp;quot;)
plot!(beta, beta, seriestype=:line, label=&amp;quot;true&amp;quot;)
xlabel!(&amp;quot;true value&amp;quot;)
ylabel!(&amp;quot;estimated value (GD)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.137474 seconds (81.55 k allocations: 5.814 MiB)
0.466605 seconds (714 allocations: 8.225 MiB)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;OLSMachineLearning_15_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/h2&gt;

&lt;p&gt;One issue associated with plain vanilla GD is that computing the gradient might be slow. Let&amp;rsquo;s now randomly select only a fraction of the full sample every time we iterate. Here, I take only 10 percent of the full sample.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Gradient descent way
function OLS_sgd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false, batchsizePer::Int64=10)
    #initial guess
    beta_hat = zeros(size(X,2))
    grad_n = zeros(size(X,2))
    #how many draws from the dataset?
    batchsize = round(Int, size(X,1)*(batchsizePer/100))
    Xn = zeros(batchsize, size(X,2))
    yn = zeros(batchsize)
    for epoch=1:epochs
        indices = shuffle(Vector(1:size(X,1)))
        Xn = X[indices[1:batchsize],:]
        yn = y[indices[1:batchsize]]
        grad_OLS!(grad_n, beta_hat, Xn, yn)
        #gradient descent:
        beta_hat -= r*grad_n
        if verbose==true
            if mod(epoch, round(Int, epochs/10))==1
                println(&amp;quot;MSE: $(mse(beta_hat, X, y))&amp;quot;)
            end
        end
    end
    return beta_hat
end

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OLS_sgd (generic function with 1 method)&lt;/p&gt;

&lt;p&gt;The following block of code shows that SGD achieves the same degree of accuracy, while being much faster than GD.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@time beta_hat_gd = OLS_gd(X,y, epochs=200);
@time beta_hat_sgd = OLS_sgd(X,y, epochs=200, batchsizePer=20);
plot(beta, beta_hat_sgd, seriestype=:scatter, label=&amp;quot;SGD&amp;quot;)
plot!(beta, beta_hat_gd, seriestype=:scatter, label=&amp;quot;GD&amp;quot;)
plot!(beta, beta, seriestype=:line, label=&amp;quot;true&amp;quot;)
xlabel!(&amp;quot;true value&amp;quot;)
ylabel!(&amp;quot;estimated value (SGD)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.894465 seconds (1.41 k allocations: 16.448 MiB, 0.42% gc time)
0.513217 seconds (338.19 k allocations: 382.550 MiB, 4.60% gc time)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;OLSMachineLearning_20_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The OLS analytical formula is the gold standard to derive theoretical properties and is perfectly fine when working with reasonably-sized data. In a big data context, (stochastic) gradient descent is the way to go. SGD can be applied to a wide-range of minimization problems. In a machine-learning context, SGD is used to estimate (&amp;ldquo;train&amp;rdquo;) much more complicated models than the simple linear model presented here. In the Appendix below, I show how one can use SGD when no analytical solution for the gradient is available.&lt;/p&gt;

&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&#34;gd-without-analytical-solution-for-the-gradient&#34;&gt;GD without analytical solution for the gradient&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s assume we don&amp;rsquo;t have a closed-form solution for the gradient. In this context, Julia&amp;rsquo;s automatic differentiation library &lt;code&gt;ForwardDiff&lt;/code&gt; is a good choice to calculate the gradient. Below, I define the loss function (MSE), I obtain the gradient of the loss function using &lt;code&gt;ForwardDiff&lt;/code&gt; and I apply the SGD algorithm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using ForwardDiff
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function mse(beta::Vector, X::Array, y::Vector)
   result = zero(eltype(y))
   for i in 1:length(y)
       #sum squared errors
       result += (y[i] - dot(X[i,:],beta))^2
   end
   return result
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mse (generic function with 1 method)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function grad!(G, beta_hat, X, y)
    G[:] = ForwardDiff.gradient(x -&amp;gt; mse(x, X, y), beta_hat)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;grad! (generic function with 1 method)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Gradient descent way
function OLS_gd(X::Array, y::Vector; epochs::Int64=1000, r::Float64=1e-5, verbose::Bool=false)
    #initial guess
    beta_hat = zeros(size(X,2))
    grad_n = zeros(size(X,2))
    for epoch=1:epochs
        grad!(grad_n, beta_hat, X, y)
        beta_hat -= r*grad_n
        if verbose==true
            if mod(epoch, round(Int, epochs/10))==1
                println(&amp;quot;MSE: $(mse(beta_hat, X, y))&amp;quot;)
            end
        end
    end
    return beta_hat
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;OLS_gd (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@time beta_hat_gd = OLS_gd(X,y, epochs=100);
plot(beta, beta_hat_gd, seriestype=:scatter, label=&amp;quot;GD (100 iter.)&amp;quot;)
plot!(beta, beta, seriestype=:line, label=&amp;quot;true&amp;quot;)
xlabel!(&amp;quot;true value&amp;quot;)
ylabel!(&amp;quot;estimated value (GD)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.180767 seconds (9.71 M allocations: 7.547 GiB, 12.66% gc time)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;OLSMachineLearning_29_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The BKM Algorithm</title>
      <link>https://julienpascal.github.io/post/bkm/</link>
      <pubDate>Mon, 16 Sep 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/bkm/</guid>
      <description>

&lt;p&gt;Finding solutions to economic models with heterogeneity and aggregate uncertainty is a notoriously difficult task.
It is an active area of research in Mathematics (see &lt;a href=&#34;https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2013.0397&#34; target=&#34;_blank&#34;&gt;mean field games with aggregate uncertainty&lt;/a&gt;). Because such models naturally arise when considering economic situations, Economists have developed a battery of techniques to (numerically) solve them. In this post, I would like to describe a recent algorithm by &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v89y2018icp68-92.html&#34; target=&#34;_blank&#34;&gt;Boppart, Krusell and Mitman (BKM)&lt;/a&gt; that is both fast and accurate. I will first describe the problem that Economists face when working with heterogeneous model with aggregate uncertainty, heuristically discuss the BKM algorithm (based on the presentation of &lt;a href=&#34;https://ideas.repec.org/a/eee/dyncon/v89y2018icp93-99.html&#34; target=&#34;_blank&#34;&gt;Reiter (2018)&lt;/a&gt;) and show an application in Julia.&lt;/p&gt;

&lt;h3 id=&#34;bkm-in-a-nutshell&#34;&gt;BKM in a nutshell&lt;/h3&gt;

&lt;p&gt;It is common to use Bellman&amp;rsquo;s principle of optimality to characterize solutions of a multi-stage decision processes.
The principle of optimality leads to a solution in a &lt;strong&gt;recursive form&lt;/strong&gt; $d_{t} = d(S_{t})$, where $S_t$ is a vector of state variables and $d(.)$ a policy function describing the optimal action of a decision-maker when faced with any given state.&lt;/p&gt;

&lt;p&gt;An alternative representation of the problem is to consider a solution in the &lt;strong&gt;sequence form&lt;/strong&gt;. At each step in time, a decision-maker observes a new realization of a random process $z_t$ and taking into account the full history of past shocks, the agent makes a new choice to maximize her expected discounted sum of returns: $d_t = d(z_t, z_{t-1}, z_{t-2}, &amp;hellip;)$.&lt;/p&gt;

&lt;p&gt;While most of the time the recursive form is a much more parsimonious approach, it fails when $S_t$ is infinite-dimensional. In models with heterogeneous agents (HA) and aggregate uncertainty, this is generally the case because the distribution of agents over certain variables will end up being in $S_t$. While this a problem with is the recursive approach (how can we discretize $S_t$ to put it on a computer?), the sequence form is immune to this problem. The BKM algorithm uses this insight, adding the &lt;strong&gt;assumption&lt;/strong&gt; of &lt;strong&gt;linearity&lt;/strong&gt; of $d(.)$ with respect to the &lt;strong&gt;aggregate state&lt;/strong&gt; $z_t$:&lt;/p&gt;

&lt;p&gt;$$ d_t = z_t d(1, 0, 0, &amp;hellip;) + z_{t-1}d(0, 1, 0, &amp;hellip;) + z_{t-2}d(0, 0, 1, &amp;hellip;) + &amp;hellip; $$
$$ d_t = \sum_{k=0}^{+\infty} z_{t-k} d_{k} $$&lt;/p&gt;

&lt;p&gt;where $$ d_{1} = d(1,0,0,&amp;hellip;)$$
$$ d_{2} = d(0,1,0,&amp;hellip;)$$
$$ d_{3} = d(0,0,1,&amp;hellip;)$$&lt;/p&gt;

&lt;p&gt;The series of $d_{k}$ describes the trajectory of the economy after an &amp;ldquo;MIT&amp;rdquo; shock (when the economy is hit by an unexpected single-period aggregate shock). If the linearity assumption holds, the evolution of equilibrium variables are simply a moving average of past shocks. We made progress because solving for the trajectory after an MIT shock is perfectly feasible in a RA model. As long as one can calculate the steady-state with no aggregate uncertainty and solve the &lt;strong&gt;deterministic perfect foresight path&lt;/strong&gt;, BKM can be used.&lt;/p&gt;

&lt;h3 id=&#34;numerical-implementation&#34;&gt;Numerical implementation&lt;/h3&gt;

&lt;p&gt;Here is the implementation of the method using a toy example. I intentionally circumvent the problem of finding the perfect foresight transition path, which is (potentially) the complicated part of BKM. This is the example given in Reiter (2018): the exact model is the non-linear model $x_{t} = a x_{t-1} + b x_{t-1}^2 + z_{t}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions
using Plots
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Parameters
max_iter=100 #number of iterations for the simulation
a = 0.5
b = 0.1
sigma_shock=1.0
mu_shock=0.
d = Normal(mu_shock, sigma_shock)

# transition function
function iter_x(x_min1::Float64, a::Float64, b::Float64)
    a*x_min1 + b*x_min1^2
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;iter_x (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;impulse-response-function&#34;&gt;Impulse response function&lt;/h4&gt;

&lt;p&gt;Let us assume that the economy is at the non-stochastic steady-state and a shock occurs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# We assume that after 100 periods, the economy is back to the steady-state
max_iter_mit = 100
x_mit=zeros(max_iter_mit)
# Initial shock
z_t[1] = sigma_shock #a 1 std. deviation
x_mit[1] = 0 #steady-state

for i=2:max_iter_mit
    x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1]
end

# Scaled-version of the impulse response:
x_mit_scaled = x_mit./z_t[1];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I define two functions to calculate the moving average:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function calculate_Xt(x_scaled::Array{Float64,1}, shocks::Array{Float64,1}, t::Int64, kk::Int64)

    output = 0.

    for k=1:kk
        output+=x_scaled[k]*shocks[t-k+1]
    end

    return output
end

function BPM_path!(XT::Array{Float64,1}, max_iter::Int64, x_scaled::Array{Float64,1}, shocks::Array{Float64,1})

    for t=2:max_iter
        XT[t] = calculate_Xt(x_scaled, shocks, t, t)
    end

end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;BPM_path! (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Initialization
XT = zeros(max_iter);
# Series of shocks
shocks_t = rand(d, max_iter).*0.5;
# Solving using BKM:
@time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t)
# True value:
x_true = zeros(max_iter)
for i=2:max_iter
    x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1]
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.025906 seconds (29.30 k allocations: 1.489 MiB, 26.80% gc time)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(XT[2:end], label=&amp;quot;x_t BKM&amp;quot;)
plot!(x_true[2:end], label=&amp;quot;x_t true&amp;quot;)
title!(&amp;quot;b=$(b)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;BKM_Algorithm_16_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The previous plot shows how the BKM algorithm approximates the true model and it does quite a good job.
Of course, the more the model is linear with respect to $z_t$ (captured by the value of $b_2$), the better the approximation. To illustrate this idea, I use BKM on a perfectly linear model ($b=0$) and on a model with stronger non-linearities ($b=0.2$). As expected, the approximation is perfect when the model is linear and the approximation deteriorates when strong non-linearities are present.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(p1,p2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;BKM_Algorithm_18_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The BKM algorithm is a new addition to the toolbox of methods to solve HA models. The sequence representation of the problem seems to be a fruitful area of research, as it has also been used in Le Grand and Ragot (&lt;a href=&#34;http://xavier-ragot.fr/pdf/progress/LeGrand_Ragot_final.pdf&#34; target=&#34;_blank&#34;&gt;2017&lt;/a&gt;) and (&lt;a href=&#34;http://xavier-ragot.fr/pdf/progress/Optimal_policies.pdf&#34; target=&#34;_blank&#34;&gt;2019&lt;/a&gt;) to develop fast and reliable method to solve HA models. The recent contribution of &lt;a href=&#34;https://github.com/shade-econ/sequence-jacobian/#sequence-space-jacobian&#34; target=&#34;_blank&#34;&gt;Auclert at al (2019)&lt;/a&gt; also uses a similar approach.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Achdou, Yves, et al. &amp;ldquo;Partial differential equation models in macroeconomics.&amp;rdquo; Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 372.2028 (2014): 20130397.&lt;/li&gt;
&lt;li&gt;Auclert, Adrien, et al. Using the Sequence-Space Jacobian to Solve and Estimate Heterogeneous-Agent Models. No. w26123. National Bureau of Economic Research, 2019.&lt;/li&gt;
&lt;li&gt;Boppart, Timo, Per Krusell, and Kurt Mitman. &amp;ldquo;Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative.&amp;rdquo; Journal of Economic Dynamics and Control 89 (2018): 68-92.&lt;/li&gt;
&lt;li&gt;Le Grand, François, and Xavier Ragot. &amp;ldquo;Optimal fiscal policy with heterogeneous agents and aggregate shocks.&amp;rdquo; Document de travail (2017).&lt;/li&gt;
&lt;li&gt;Le Grand, François, and Ragot, Xavier. &amp;ldquo;Managing Inequality over the Business Cycles: Optimal Policies with Heterogeneous Agents and Aggregate Shocks&amp;rdquo;. No. 1090. Society for Economic Dynamics, 2019.&lt;/li&gt;
&lt;li&gt;Reiter, Michael. &amp;ldquo;Comments on&amp;rdquo; Exploiting MIT Shocks in Heterogeneous-Agent Economies: The Impulse Response as a Numerical Derivative&amp;rdquo; by T. Boppart, P. Krusell and K. Mitman.&amp;rdquo; Journal of Economic Dynamics and Control 89 (2018): 93-99.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;versioninfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Julia Version 1.0.3
Commit 099e826241 (2018-12-18 01:34 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-6.0.0 (ORCJIT, skylake)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;### To create plot 2

#### Linear Model

# Parameters
max_iter=100 #number of iterations for the simulation
a = 0.5
b = 0.0
sigma_shock=1.0
mu_shock=0.
d = Normal(mu_shock, sigma_shock)

# IRF
x_mit=zeros(max_iter_mit)
# Initial shock
z_t[1] = sigma_shock #a 1 std. deviation
x_mit[1] = 0 #steady-state

for i=2:max_iter_mit
    x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1]
end

# Scaled-version of the impulse response:
x_mit_scaled = x_mit./z_t[1];


# Initialization
XT = zeros(max_iter);
# Series of shocks
shocks_t = rand(d, max_iter).*0.5;
# Solving using BKM:
@time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t)
# True value:
x_true = zeros(max_iter)
for i=2:max_iter
    x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1]
end

p1 = plot(XT[2:end], label=&amp;quot;x_t BKM&amp;quot;)
plot!(x_true[2:end], label=&amp;quot;x_t true&amp;quot;)
title!(&amp;quot;b=$(b)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.000009 seconds (4 allocations: 160 bytes)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;BKM_Algorithm_24_1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;### Plot 2

#### Linear Model

# Parameters
max_iter=100 #number of iterations for the simulation
a = 0.5
b = 0.2
sigma_shock=1.0
mu_shock=0.
d = Normal(mu_shock, sigma_shock)

# IRF
x_mit=zeros(max_iter_mit)
# Initial shock
z_t[1] = sigma_shock #a 1 std. deviation
x_mit[1] = 0 #steady-state

for i=2:max_iter_mit
    x_mit[i] = iter_x(x_mit[i-1], a, b) + z_t[i-1]
end

# Scaled-version of the impulse response:
x_mit_scaled = x_mit./z_t[1];


# Initialization
XT = zeros(max_iter);
# Series of shocks
shocks_t = rand(d, max_iter).*0.5;
# Solving using BKM:
@time BPM_path!(XT, max_iter, x_mit_scaled, shocks_t)
# True value:
x_true = zeros(max_iter)
for i=2:max_iter
    x_true[i] = iter_x(x_true[i-1], a, b) + shocks_t[i-1]
end

p2 = plot(XT[2:end], label=&amp;quot;x_t BKM&amp;quot;)
plot!(x_true[2:end], label=&amp;quot;x_t true&amp;quot;)
title!(&amp;quot;b=$(b)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.000007 seconds (4 allocations: 160 bytes)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;BKM_Algorithm_25_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Time Iteration (Part II)</title>
      <link>https://julienpascal.github.io/post/lineartimeiteration2/</link>
      <pubDate>Mon, 26 Aug 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/lineartimeiteration2/</guid>
      <description>

&lt;p&gt;In a &lt;a href=&#34;https://julienpascal.github.io/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I introduced the logic for the Linear Time Iteration (LTI) method (Pontus Rendahl (2017)).
Now it&amp;rsquo;s time to apply the technique to a &amp;ldquo;real&amp;rdquo; (yet simple) economic model:
a stochastic growth model with endogenous labor supply.
The implementation is in Julia and is based a Matlab code by Pontus Rendahl available &lt;a href=&#34;https://sites.google.com/site/pontusrendahl/Research&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. We will use a three-step approach:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[1] solve the non-stochastic steady-state of the model&lt;/li&gt;
&lt;li&gt;[2] differentiate the system around the non-stochastic steady-state to obtain a linear difference equation of the
form $A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0$&lt;/li&gt;
&lt;li&gt;[3] apply the LTI method to find the law of motion  $x_{t} = F x_{t-1} + Q u_{t}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;

&lt;p&gt;We consider an economy populated by a representative household, deciding how much to work, save and consume at any given time. Output $y$ depends on the capital level $k$ (inherited from period $t-1$), on the number of hours worked $l$, and on the productivity level $z$:&lt;/p&gt;

&lt;p&gt;$$ y_t = z_t k_{t-1}^{\alpha} l_{t}^{1 - \alpha}$$&lt;/p&gt;

&lt;p&gt;Business cycles are driven by variations in productivity, that follows an AR(1) process, with $e_t$ a zero-mean stochastic variable:&lt;/p&gt;

&lt;p&gt;$$z_t = \rho z_{t-1} + e_{t} $$&lt;/p&gt;

&lt;p&gt;Capital at the end of period $t$ is equal to investment plus the non-depreciated capital stock inherited from last period:&lt;/p&gt;

&lt;p&gt;$$ k_{t} = I_{t} + (1 - \delta) k_{t-1} $$&lt;/p&gt;

&lt;p&gt;The representative household enjoys consumption and dislikes providing labor:&lt;/p&gt;

&lt;p&gt;$$ U(c,l) = \frac{C^{1-\sigma}}{1-\sigma} -  \frac{l^{1-\eta}}{1-\eta} $$&lt;/p&gt;

&lt;p&gt;Everything that is produced in the economy is either consumed or saved:&lt;/p&gt;

&lt;p&gt;$$ c_{t} + k_{t} = z_t k_{t-1}^{\alpha} l_{t}^{1 - \alpha} + (1 - \delta)k_{t-1}$$&lt;/p&gt;

&lt;p&gt;The optimal decision of the household is characterized by two equations:&lt;/p&gt;

&lt;p&gt;$$ c_{t}^{-\sigma} = \beta E_{t}(c_{t+1}^{-\sigma}(1 - \delta + \alpha z_{t+1} k_{t}^{\alpha -1} l_{t+1}^{1 - \alpha} ) )$$&lt;/p&gt;

&lt;p&gt;$$ l_{t}^{-\eta} = c_{t}^{-\gamma}(1 - \alpha) z_{t} k_{t-1}^\alpha l_{t}^{-\alpha} $$&lt;/p&gt;

&lt;p&gt;The first one states the gain of raising consumption today by one unit has to be equal to the expected gain from saving
one extra unit today and consuming it tomorrow (inter-temporal FOC). The second equation states that the marginal cost of working one extra hour today has to be equal
to the marginal gain of that extra hour worked (intra-temporal FOC).&lt;/p&gt;

&lt;h2 id=&#34;solving-the-steady-state&#34;&gt;Solving the steady-state&lt;/h2&gt;

&lt;p&gt;Calculating the steady-state of the model is a root finding problem. Let&amp;rsquo;s use the package &lt;code&gt;NLsolve&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;versioninfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Julia Version 1.0.3
Commit 099e826241 (2018-12-18 01:34 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-6.0.0 (ORCJIT, skylake)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Declare parameters
const alpha = 1/3;        # Capital share of output
const beta = 1.03^(-1/4); # Discount factor.
const gamma = 2;          # Coefficient of risk aversion
const eta = 2;            # Frisch elasticity of labor supply
const delta = 0.025;      # Depreciation rate of capital
const rho = 0.9;          # Persistence of TFP process.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using NLsolve

# Let&#39;s define a function for each equation of the model at the steady-state
function Ee(x::Array{Float64,1})
    -x[1]^(-gamma) + beta*(1.0 + alpha*(x[2]^(alpha - 1.0))*(x[3]^(1.0 - alpha)) - delta)*(x[1]^(-gamma))
end

function Rc(x::Array{Float64,1})
    -x[1] - x[2] + (x[2]^(alpha))*(x[3]^(1.0 - alpha)) + (1.0 - delta)*x[2]
end

function Ls(x::Array{Float64,1})
    (-x[1]^(-gamma))*(1.0 - alpha)*(x[2]^(alpha))*(x[3]^(-alpha)) + x[3]^(eta)
end

# The steady-state of the model is described by a system of three equations
f! = function (dx,x)
  dx[1] = Ee(x)
  dx[2] = Rc(x)
  dx[3] = Ls(x)
end


res = nlsolve(f!,[1.0; 20; 0.7])
xss = res.zero;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;css = xss[1];
kss = xss[2];
lss = xss[3];
# steady-state output and investment:
yss = kss^(alpha)*lss^(1-alpha);
Iss = kss-(1-delta)*kss;
XSS = zeros(6)
XSS[1]=yss
XSS[2]=Iss
XSS[3:5] = xss
XSS[6]=1.0;

print(&amp;quot;Steady-state value [css, kss, lss, yss, Iss, zss] = &amp;quot;, XSS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Steady-state value [css, kss, lss, yss, Iss, zss] = [2.51213, 0.645783, 1.86634, 25.8313, 0.78341, 1.0]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;solving-the-stochastic-model&#34;&gt;Solving the stochastic model&lt;/h2&gt;

&lt;p&gt;To find a solution to the stochastic model,
let&amp;rsquo;s differentiate the system around the non-stochastic steady-state calculated above. Here, we will limit ourself to a first-order approximation since the goal is to obtain is a linear difference equation of the form $ A x_{t-1} + B x_{t} + C E_{t} [x_{t+1}] + u_{t} = 0 $, for which LTI is applicable. What is the rationale for the linear approximation? Fist of all, notice that the model can be put in the form of:&lt;/p&gt;

&lt;p&gt;$$E_t(f(Y_t, \sigma)) = 0 $$&lt;/p&gt;

&lt;p&gt;where $Y_t = [x_{t-1}, x_{t}, x_{t+1}]$ is  $3n × 1$ vector containing endogenous and exogenous variables and $\sigma$ is variable scaling the level of uncertainty in the model. For instance, if $v_{t}$ is a zero-mean normally distributed variable with variance $\sigma^2$:&lt;/p&gt;

&lt;p&gt;$$z_t = \rho z_{t-1} + \sigma v_{t} $$&lt;/p&gt;

&lt;p&gt;In the non-stochastic state, $\sigma = 0$. Let&amp;rsquo;s take a first-order Taylor expansion around the non-stochastic steady-state:&lt;/p&gt;

&lt;p&gt;$$f(Y_t, \sigma) \approx f(Y_{SS}, 0) + \frac{Df}{DY_t}|Y_{SS}(Y_t - Y_{SS}) + \frac{Df}{D\sigma}|\sigma_{SS}(\sigma - 0) = 0$$&lt;/p&gt;

&lt;p&gt;where $\frac{Df}{DY_t}|Y_{SS}$ is the derivative of the vector-valued function $f$ with respect to the vector $Y_t$ evaluated at $Y_{SS}$&lt;/p&gt;

&lt;p&gt;Using $f(Y_{SS}, 0) = 0$ and that the last term disappears when we take the expectation:&lt;/p&gt;

&lt;p&gt;$$E_t(f(Y_t, \sigma)) \approx E_t(\frac{Df}{DY_t}|Y_{SS}(Y_t - Y_{SS})) = 0 $$&lt;/p&gt;

&lt;p&gt;Defining the matrices $A$, $B$ and $C$ such that $\frac{Df}{DY_t}|Y_{SS} = [A; B; C]$ we obtain a system of the form:&lt;/p&gt;

&lt;p&gt;$A \tilde{x}_{t-1} + B \tilde{x}_{t} + C E_{t} [\tilde{x}_{t+1}] = 0$&lt;/p&gt;

&lt;p&gt;with $\tilde{x}_{t} = x_{t} - x_{SS} $&lt;/p&gt;

&lt;p&gt;In practical terms, obtaining a linear approximation around the non-stochastic
steady-state is easily done using the package &lt;code&gt;ForwardDiff&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using ForwardDiff

# Function defining the stochastic model
# Each line is an equation
# The input the vector x is [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp]
f! = (w, x) -&amp;gt; begin
    #naming the input variables:
    ym, y, yp, Im, I, Ip, cm, c, cp, km, k, kp, lm, l, lp, zm, z, zp = x
    w[1] = -y + z*km^(alpha)*l^(1.0 - alpha)
    w[2] = -I+k-(1.0-delta)*km
    w[3] = -c^(-gamma) + beta*(1+zp*alpha*k^(alpha-1)*lp^(1-alpha)-delta)*cp^(-gamma)
    w[4] = c + k - (z*km^(alpha)*l^(1.0-alpha)+(1.0-delta)*km)
    w[5] = c^(-gamma)*(1.0-alpha)*km^(alpha)*l^(-alpha)*z-l^(eta)
    w[6] = -z+zm*rho
    return nothing
end

f = x -&amp;gt; (w = fill(zero(promote_type(eltype(x), Float64)), 6); f!(w, x); return w)

# At the steady-state, the function f should be zero:
Xss = [yss yss yss Iss Iss Iss css css css kss kss kss lss lss lss 1 1 1];
#println(maximum(abs.(f(Xss))))

Jac = ForwardDiff.jacobian(f, Xss);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;collecting-matrices-a-b-and-c&#34;&gt;Collecting matrices A, B and C&lt;/h3&gt;

&lt;p&gt;Having successfully obtained $\frac{Df}{DY_t}|Y_{SS}$, we now need to collect the right elements to form the matrices A, B and C in order to apply the LTI algorithm. It is mainly a matter of bookkeeping:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# A is derivative of the function &amp;quot;system&amp;quot; f(Vars) w.r.t to Xm = [ym Im cm km lm zm]
# with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp]
A = zeros(6,6)
# Keeping track of indices:
A[:,1] = Jac[:,1]
A[:,2] = Jac[:,4]
A[:,3] = Jac[:,7]
A[:,4] = Jac[:,10]
A[:,5] = Jac[:,13]
A[:,6] = Jac[:,16];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# B is derivative of the function &amp;quot;system&amp;quot; f(Vars) w.r.t to X = [y I c k l z];
# with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp]
B = zeros(6,6)
# Keeping track of indices:
B[:,1] = Jac[:,2]
B[:,2] = Jac[:,5]
B[:,3] = Jac[:,8]
B[:,4] = Jac[:,11]
B[:,5] = Jac[:,14]
B[:,6] = Jac[:,17];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# C is derivative of the function &amp;quot;system&amp;quot; f(Vars) w.r.t to Xp = [yp Ip cp kp lp zp];
# with Vars = [ym y yp Im I Ip cm c cp km k kp lm l lp zm z zp]
C = zeros(6,6)
# Keeping track of indices:
C[:,1] = Jac[:,3]
C[:,2] = Jac[:,6]
C[:,3] = Jac[:,9]
C[:,4] = Jac[:,13]
C[:,5] = Jac[:,15]
C[:,6] = Jac[:,18];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Convert to log-linear system:
M = ones(6,1)*transpose(XSS)
A = A.*M; B = B.*M; C = C.*M;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;solving-the-model&#34;&gt;Solving the model&lt;/h3&gt;

&lt;p&gt;We are now in good place to find the law of motion of the economy using the LTI approach.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using LinearAlgebra

# Source: adapted from the matlab version made available by Pontus Rendahl on his website
# https://sites.google.com/site/pontusrendahl/Research
# This function solves the model Ax_{t-1}+Bx_{t}+CE_t[x_{t+1}]+u_t=0, and
# finds the solution x_t=Fx_{t-1}+Qu_t. The parameter mu should be set
# equal to a small number, e.g. mu=1e-6;

function t_iteration(A::Array{Float64,2},
                    B::Array{Float64,2},
                    C::Array{Float64,2},
                    mu::Float64;
                    tol::Float64=1e-12,
                    max_iter::Int64 = 1000,
                    F0::Array{Float64,2} = Array{Float64}(undef, 0, 0),
                    S0::Array{Float64,2} = Array{Float64}(undef, 0, 0),
                    verbose::Bool=false)

# success flag:
flag = 0

# Initialization
dim = size(A,2)
if isempty(F0) == true
    F = zeros(dim,dim)
else
    F = F0
end
if isempty(S0) == true
    S = zeros(dim,dim)
else
    S = S0
end

eye = zeros(dim,dim)
for i = 1:dim
    eye[i,i] = 1.0
end

I = eye*mu
Ch = C
Bh = (B+C*2*I)
Ah = (C*I^2+B*I+A)

#Check the reciprocal condition number
#if rcond(Ah)&amp;lt;1e-16
#    disp(&#39;Matrix Ah is singular&#39;)
#end

metric = 1;
nb_iter = 0

while metric&amp;gt;tol
    nb_iter+=1
    #\(x, y)
    #Left division operator:
    #multiplication of y by the inverse of x on the left.
    F = -(Bh+Ch*F)\Ah
    S = -(Bh+Ah*S)\Ch;

    metric1 = maximum(abs.(Ah+Bh*F+Ch*F*F))
    metric2 = maximum(abs.(Ah*S*S+Bh*S+Ch))
    metric = max(metric1, metric2)

    if nb_iter == max_iter
        if verbose == true
            print(&amp;quot;Maximum number of iterations reached. Convergence not reached.&amp;quot;)
            print(&amp;quot;metric = $metric&amp;quot;)
        end
        break
    end
end


eig_F = maximum(abs.(eigvals(F)));
eig_S = maximum(abs.(eigvals(S)));

if eig_F&amp;gt;1 || eig_S&amp;gt;1 || mu&amp;gt;1-eig_S
    if verbose == true
        println(&amp;quot;Conditions of Proposition 3 violated&amp;quot;)
    end
else
    flag = 1
end

F = F+I;
Q = -inv(B+C*F);

return F, Q, flag

end

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;t_iteration (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@time F, Q, flag = t_iteration(A, B, C, 0.01)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.018310 seconds (11.04 k allocations: 3.390 MiB, 56.25% gc time)

([0.0 0.0 … -1.38108e-18 1.04589; 0.0 0.0 … -8.16878e-18 3.50588; … ; 0.0 0.0 … -1.73472e-18 0.218832; 0.0 0.0 … 0.0 0.9], [0.398069 0.0 … 0.455811 1.1621; -0.0 1.54851 … 1.72393 3.89542; … ; -0.0 -0.0 … 0.683716 0.243147; -0.0 -0.0 … -0.0 1.0], 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;impulse-response-function&#34;&gt;Impulse Response Function&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s now simulate the response of the economy to a positive productivity shock.
The IRF plots show that this shock leads to a positive response in output, investment, consumption, capital and hours. These variables slowly converge to their steady-state values, as productivity goes back to its steady-state level.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Plots
pyplot()

nb_periods = 40
x = zeros(6, nb_periods)
u = zeros(6, nb_periods)
#initialization
u[:,1] = [0.0 0.0 0.0 0.0 0.0 1.0]

for t=2:nb_periods
    # Law of motion
    x[:,t] = F * x[:,t-1] + Q * u[:,t-1]
end

p1 = plot(x[1,2:end], label = &amp;quot;output gap xt&amp;quot;)
p2 = plot(x[2,2:end], label = &amp;quot;Investment&amp;quot;)
p3 = plot(x[3,2:end], label = &amp;quot;Consumption&amp;quot;)
p4 = plot(x[4,2:end], label = &amp;quot;Capital&amp;quot;)
p5 = plot(x[5,2:end], label = &amp;quot;Hours&amp;quot;)
p6 = plot(x[6,2:end], label = &amp;quot;Prod.&amp;quot;)
p = plot(p1,p2, p3, p4, p5, p6)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Linear%20Time%20Iteration%20Stochastic%20Growth%20Model_33_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;stochastic-simulation&#34;&gt;Stochastic Simulation&lt;/h2&gt;

&lt;p&gt;We can also generate a series of draws from $e_t$ to simulate an economy and calculate moments on the simulated
series:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Calculate a stochastic simulation
using Distributions
d = Normal()

nb_periods = 1000
x = zeros(6, nb_periods)
u = zeros(6, nb_periods)
#initialization
u[6,:] = rand(d, nb_periods) #series of shocks

for t=2:nb_periods
    # Law of motion
    x[:,t] = F * x[:,t-1] + Q * u[:,t-1]
end

p1 = plot(x[1,2:end], label = &amp;quot;output gap xt&amp;quot;)
p2 = plot(x[2,2:end], label = &amp;quot;Investment&amp;quot;)
p3 = plot(x[3,2:end], label = &amp;quot;Consumption&amp;quot;)
p4 = plot(x[4,2:end], label = &amp;quot;Capital&amp;quot;)
p5 = plot(x[5,2:end], label = &amp;quot;Hours&amp;quot;)
p6 = plot(x[6,2:end], label = &amp;quot;Prod.&amp;quot;)
p = plot(p1,p2, p3, p4, p5, p6)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Info: Precompiling Distributions [31c24e10-a181-5473-b8eb-7969acd0382f]
└ @ Base loading.jl:1192
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Linear%20Time%20Iteration%20Stochastic%20Growth%20Model_36_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#correlation matrix
cor(transpose(x[:,2:end]),transpose(x[:,2:end]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;6×6 Array{Float64,2}:
 1.0       0.954544   0.842972   0.712213   0.200527  0.979622
 0.954544  1.0        0.644305   0.470603   0.483428  0.99496
 0.842972  0.644305   1.0        0.978002  -0.357991  0.717745
 0.712213  0.470603   0.978002   1.0       -0.544888  0.556709
 0.200527  0.483428  -0.357991  -0.544888   1.0       0.393212
 0.979622  0.99496    0.717745   0.556709   0.393212  1.0     
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This post illustrated how one can solve the neoclassical growth model from scratch, using Linear Time
Iteration. While the model presented here is quite simple, the three-step approach discussed is quite general.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Rendahl, Pontus. Linear time iteration. No. 330. IHS Economics Series, 2017.
(&lt;a href=&#34;https://www.ihs.ac.at/publications/eco/es-330.pdf&#34; target=&#34;_blank&#34;&gt;https://www.ihs.ac.at/publications/eco/es-330.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;The original Matlab code is available &lt;a href=&#34;https://sites.google.com/site/pontusrendahl/Research&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Linear Time Iteration (Part I)</title>
      <link>https://julienpascal.github.io/post/lineartimeiteration/</link>
      <pubDate>Sun, 25 Aug 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/lineartimeiteration/</guid>
      <description>

&lt;p&gt;The details of solving rational expectation models are somewhat tricky. Yet, a recent paper by &lt;a href=&#34;https://sites.google.com/site/pontusrendahl/&#34; target=&#34;_blank&#34;&gt;Pontus Rendahl&lt;/a&gt; underlines that an easy (and fast) method exists. What&amp;rsquo;s more, this method seems to be adapted to regime switching models and to models with a large state variable. The last point is particularly relevant if one studies heterogeneous agent models and uses Reiter&amp;rsquo;s (2009) method to solve them. In this post, I describe the method (closely following the paper) and give simple examples in Julia.&lt;/p&gt;

&lt;h2 id=&#34;intuition&#34;&gt;Intuition&lt;/h2&gt;

&lt;p&gt;Below, I quote some fundamental passages from the paper, discussing the intuition of the method:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The logic underlying the procedure is simple enough to be described in words. Envisage an agent having a certain amount of an asset, facing the choice between how much of this asset to &lt;strong&gt;consume and how much to save&lt;/strong&gt;. An optimal choice would &lt;strong&gt;trade off the marginal benefit of saving (future consumption) with its marginal cost (forgone current consumption)&lt;/strong&gt;. The resulting optimal decision is implied by a linear(ized) second-order difference equation&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;[&amp;hellip;] the future marginal benefit of saving depends on &lt;strong&gt;the optimal saving choice in the future&lt;/strong&gt;. Thus, an optimal choice today can only be determined under the condition that the optimal choice in &lt;strong&gt;the future is known&lt;/strong&gt;; thus the problem amounts to &lt;strong&gt;finding a fixed point&lt;/strong&gt;. To solve this problem, this paper proposes to &lt;strong&gt;guess for the optimal choice&lt;/strong&gt; of saving in the future as a &lt;strong&gt;linear function of the associated state&lt;/strong&gt; (which is given by the optimal choice in the present). Given such a guess, the optimal choice in the present is then trivially given by solving a linear equation. However, the current optimal choice provides us with another suggestion regarding future optimal behavior, and the guess is updated accordingly.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To summarize (i) solving a rational expectation model is intrinsically a fixed-point problem (ii) the Linear Time Iteration approach assumes a particular form for the solution and iterates until convergence is reached. The discussion that follows will be mainly informal, but the paper makes sure that this procedure is well behaved.&lt;/p&gt;

&lt;h2 id=&#34;the-method&#34;&gt;The Method&lt;/h2&gt;

&lt;p&gt;We are interested in solving a model of the form:&lt;/p&gt;

&lt;p&gt;$$Ax_{t-1}+B x_{t}+CE_{t}[x_{t+1}]+u_{t}=0$$&lt;/p&gt;

&lt;p&gt;Where $x_t$ is an n × 1 vector containing endogenous and exogenous variables, $u_t$ is an n × 1 vector of mean-zero disturbances, and $A$, $B$ and $C$ are conformable matrices.&lt;/p&gt;

&lt;p&gt;Let us assume that the solution is:&lt;/p&gt;

&lt;p&gt;$$ x_{t} = F x_{t-1} + Q u_{t} $$&lt;/p&gt;

&lt;p&gt;where $F$ and $Q$ are unknown matrices.&lt;/p&gt;

&lt;p&gt;Substituting the linear law of motion into the first equation (and using the fact that $u_{t+1}$ is a mean-zero random noise term) yields:&lt;/p&gt;

&lt;p&gt;$$ A x_{t−1} + B x_{t} + CF x_{t} + u_{t} = 0. $$&lt;/p&gt;

&lt;p&gt;This equation can be written as:&lt;/p&gt;

&lt;p&gt;$$ x_{t} = -(B + CF)^{-1} A x_{t−1} + (-(B + CF)^{-1})u_t $$&lt;/p&gt;

&lt;p&gt;Comparing the solution we assumed in the first place, and the last equation, we see that:&lt;/p&gt;

&lt;p&gt;$$ Q = -(B + CF)^{-1} $$&lt;/p&gt;

&lt;p&gt;The previous manipulations show that if one knows $F$, finding $Q$ is trivial (because $B$ and $C$ are known).
In practical terms, we can focus on solving the deterministic part of the problem (ignoring the $u_t$), since
we can then back out the stochastic solution using our equation for $Q$.&lt;/p&gt;

&lt;p&gt;The deterministic problem is:&lt;/p&gt;

&lt;p&gt;$$ A x_{t-1} + B x_{t} + C x_{t+1}  = 0 $$&lt;/p&gt;

&lt;p&gt;And its associated solution is:&lt;/p&gt;

&lt;p&gt;$$ x_{t} = F x_{t-1} $$&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s guess a value for $F$, denoted by $F_{n}$.&lt;/p&gt;

&lt;p&gt;A simple substitution gives:&lt;/p&gt;

&lt;p&gt;$$ A x_{t-1} + B x_{t} + F_{n} C x_{t}  = 0 $$&lt;/p&gt;

&lt;p&gt;Which can be re-written as:&lt;/p&gt;

&lt;p&gt;$$ x_{t}  = - (B + F_{n} C)^{-1}A x_{t-1}  $$&lt;/p&gt;

&lt;p&gt;Comparing the solution we assumed in the first place and the last equation, the following &lt;strong&gt;updating rule&lt;/strong&gt; seems to make sense:&lt;/p&gt;

&lt;p&gt;$$ F_{n+1} = - (B + F_{n} C)^{-1} A $$&lt;/p&gt;

&lt;p&gt;One could apply the updating rule until the distance between $F_{n+1}$ and $F_{n}$ is small, but the paper uses
another stopping rule. Let&amp;rsquo;s start with an observation and then give a definition:&lt;/p&gt;

&lt;h4 id=&#34;fact&#34;&gt;Fact&lt;/h4&gt;

&lt;p&gt;If $F$ solves&lt;/p&gt;

&lt;p&gt;$$ A x_{t−1} + B x_{t} + CF x_{t} = 0 $$&lt;/p&gt;

&lt;p&gt;then $F$ solves the quadratic matrix equation:&lt;/p&gt;

&lt;p&gt;$$ A + B F + C F^2 = 0 $$&lt;/p&gt;

&lt;h4 id=&#34;definition&#34;&gt;Definition&lt;/h4&gt;

&lt;p&gt;A solution to the equation&lt;/p&gt;

&lt;p&gt;$$ A + B F + C F^2 = 0 $$&lt;/p&gt;

&lt;p&gt;is called a &lt;strong&gt;solvent&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We now have all the elements to describe the Linear Time Iteration algorithm.&lt;/p&gt;

&lt;h4 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Guess a value for $F_0$&lt;/li&gt;
&lt;li&gt;Calculate $ F_{1} = - (B + F_{0} C)^{-1} A $&lt;/li&gt;
&lt;li&gt;If $ || A + B F_1 + C F_1^2 || &amp;lt; tol $ stop. $F_1$ is a solvent&lt;/li&gt;
&lt;li&gt;Else, increment the index for $F$ and start again&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If all the eigenvalues of the resulting solvent are less than $1$ in absolute value, then we found a stable solution
to the quadratic matrix equation. However, it is not necessarily the unique stable solution. For discussion on uniqueness and stability, an interested reader may refer to proposition 2 of the paper.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s now see how the Linear Time Iteration performs on two simple examples described in the original paper.&lt;/p&gt;

&lt;h3 id=&#34;example-1-a-uni-dimensional-example&#34;&gt;Example 1: a uni-dimensional example&lt;/h3&gt;

&lt;p&gt;$$ 0.75 x_{t−1} − 2 x_{t} + x_{t+1} = 0 $$&lt;/p&gt;

&lt;p&gt;In this example, using Linear Time Iteration is clearly an overkill since we can calculate the solution by hand.
The two solvents are $1.5$ and $0.5$. As we will see, the method laid above converges to the smaller of the two values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;versioninfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Julia Version 1.0.3
Commit 099e826241 (2018-12-18 01:34 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-6.0.0 (ORCJIT, skylake)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Parameters
a = 0.75
b = -2.0
c = 1.0
# Tolerance
tol = 1e-6
# Maximum iterations
max_iter = 1000
# Initial guess for F
F_n = 0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;for i=1:max_iter
    # Updating rule:
    F_n_new =  -a*(1/(b + F_n*c))
    # Stopping rule:
    if abs(a + b *F_n_new + c*F_n_new^2) &amp;lt; tol
        println(&amp;quot;convergence after $i iterations&amp;quot;)
        println(&amp;quot;final value for F is $F_n&amp;quot;)
        break
    end
    F_n = copy(F_n_new)
    if i == max_iter
        println(&amp;quot;convergence NOT reached $i iterations&amp;quot;)
    end
end

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;convergence after 12 iterations
final value for F is 0.4999981183200362
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dealing-with-singular-solvents&#34;&gt;Dealing with singular solvents&lt;/h3&gt;

&lt;p&gt;Even in some reasonable cases, the simple Linear Time Iteration algorithm described above might fail. For instance, because the model contains accounting identities, in which case the solvent may be &amp;ldquo;singular&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;definition-1&#34;&gt;Definition&lt;/h4&gt;

&lt;p&gt;A solvent is &lt;strong&gt;singular&lt;/strong&gt; if it contains at least one eigenvalue equal to 1.&lt;/p&gt;

&lt;p&gt;Fortunately, a simple trick extends the Linear Time Iteration method to singular solvents.
One solves the modified quadratic matrix equation&lt;/p&gt;

&lt;p&gt;$$ \hat{A} S^2 + \hat{B} S + \hat{C} = 0 $$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$$ \hat{A} = C M^2 + B M + A $$
$$ \hat{B} = B + 2 C M$$
$$ \hat{C} = C$$
$$ M = \mu I $$&lt;/p&gt;

&lt;p&gt;with $\mu$ a small positive real number and $I$ a conformable identity matrix. If the Linear Time Iteration
algorithm applied to the modified system converges to $S$, the $F = S + M$ is solution to the original system.
Below I define a function &lt;code&gt;t_iteration&lt;/code&gt; the solves the modified system&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using LinearAlgebra

# Source: adapted from the matlab version made available by Pontus Rendahl on his website
# https://sites.google.com/site/pontusrendahl/Research
# This function solves the model Ax_{t-1}+Bx_{t}+CE_t[x_{t+1}]+u_t=0, and
# finds the solution x_t=Fx_{t-1}+Qu_t. The parameter mu should be set
# equal to a small number, e.g. mu=1e-6;

function t_iteration(A::Array{Float64,2},
                    B::Array{Float64,2},
                    C::Array{Float64,2},
                    mu::Float64;
                    tol::Float64=1e-12,
                    max_iter::Int64 = 1000,
                    F0::Array{Float64,2} = Array{Float64}(undef, 0, 0),
                    S0::Array{Float64,2} = Array{Float64}(undef, 0, 0),
                    verbose::Bool=false)

# success flag:
flag = 0

# Initialization
dim = size(A,2)
if isempty(F0) == true
    F = zeros(dim,dim)
else
    F = F0
end
if isempty(S0) == true
    S = zeros(dim,dim)
else
    S = S0
end

eye = zeros(dim,dim)
for i = 1:dim
    eye[i,i] = 1.0
end

I = eye*mu
Ch = C
Bh = (B+C*2*I)
Ah = (C*I^2+B*I+A)

#Check the reciprocal condition number
#if rcond(Ah)&amp;lt;1e-16
#    disp(&#39;Matrix Ah is singular&#39;)
#end

metric = 1;
nb_iter = 0

while metric&amp;gt;tol
    nb_iter+=1
    #\(x, y)
    #Left division operator:
    #multiplication of y by the inverse of x on the left.
    F = -(Bh+Ch*F)\Ah
    S = -(Bh+Ah*S)\Ch;

    metric1 = maximum(abs.(Ah+Bh*F+Ch*F*F))
    metric2 = maximum(abs.(Ah*S*S+Bh*S+Ch))
    metric = max(metric1, metric2)

    if nb_iter == max_iter
        if verbose == true
            print(&amp;quot;Maximum number of iterations reached. Convergence not reached.&amp;quot;)
            print(&amp;quot;metric = $metric&amp;quot;)
        end
        break
    end
end


eig_F = maximum(abs.(eigvals(F)));
eig_S = maximum(abs.(eigvals(S)));

if eig_F&amp;gt;1 || eig_S&amp;gt;1 || mu&amp;gt;1-eig_S
    if verbose == true
        println(&amp;quot;Conditions of Proposition 3 violated&amp;quot;)
    end
else
    flag = 1
end

F = F+I;
Q = -inv(B+C*F);

return F, Q, flag

end

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;t_iteration (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example-2-a-bi-dimensional-problem&#34;&gt;Example 2: a bi-dimensional problem&lt;/h3&gt;

&lt;p&gt;The problem is&lt;/p&gt;

&lt;p&gt;$$ 0.75 y_t - 0.5 y_{t+1} = 0 $$
$$ -2 x_t + x_{t-1} - y_{t} = 0 $$&lt;/p&gt;

&lt;p&gt;This problem has three solvents. Two of them lead to an unstable solution. The solvent associated to a stable solution is given by:&lt;/p&gt;

&lt;p&gt;$\begin{bmatrix} 0 &amp;amp; 0 \\ 0 &amp;amp; 0.5\end{bmatrix}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Defining the problem
A = [[0. 0.];
     [0. 1.]]

B = [[0.75 0.];
     [-1. -2.]]

C = [[-0.5 0.];
     [0. 0.]]

# Finding a solvent
F_n, Q_n, flag = t_iteration(A, B, C, 0.01, max_iter=1000)

println(&amp;quot;F is :&amp;quot;, F_n)

# Simulating the model forward
using Plots
pyplot()

nb_periods = 20
x = ones(2, nb_periods)
#initialization
x[:,1] = [1.0 1.0] #starting value

for t=2:nb_periods
    # Update rule
    x[:,t] = F_n * x[:,t-1]
end

plot(x[1,:], label = &amp;quot;xt&amp;quot;)
plot!(x[2,:], label = &amp;quot;yt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;F is :[0.0 0.0; 0.0 0.5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We successfully recovered the stable solution.
Starting an initial condition, we can simulate the behavior of the system
using the law of motion  $x_{t} = F x_{t-1} + Q u_{t}$ (see the next plot)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Linear%20Time%20Iteration_50_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Linear Time Iteration is an intuitive and easily applicable method to solve (linear) rational expectation models. This post aimed at describing the intuition for it and give simple examples. In a subsequent post, I will use this technique to solve the stochastic growth model.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Rendahl, Pontus. Linear time iteration. No. 330. IHS Economics Series, 2017.
(&lt;a href=&#34;https://www.ihs.ac.at/publications/eco/es-330.pdf&#34; target=&#34;_blank&#34;&gt;https://www.ihs.ac.at/publications/eco/es-330.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Reiter, Michael. &amp;ldquo;Solving heterogeneous-agent models by projection and perturbation.&amp;rdquo; Journal of Economic Dynamics and Control 33.3 (2009): 649-665.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
