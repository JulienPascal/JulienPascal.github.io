<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julien Pascal on Julien Pascal</title>
    <link>https://julienpascal.github.io/</link>
    <description>Recent content in Julien Pascal on Julien Pascal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Julien Pascal</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The Simulated Method of Moments</title>
      <link>https://julienpascal.github.io/post/smm/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://julienpascal.github.io/post/smm/</guid>
      <description>

&lt;p&gt;As Thomas Sargent said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;A rational expectations equilibrium model is a likelihood function&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But in many cases, the likelihood function is too complicated to be
written down in closed form. To estimate the structural parameters of the model, on can still use Monte-Carlo methods. In this post, I would like to describe the &lt;a href=&#34;https://en.wikipedia.org/wiki/Method_of_simulated_moments&#34; target=&#34;_blank&#34;&gt;simulated method of moments&lt;/a&gt; (SMM), which is widely used simulation-based estimation technique.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-setting&#34;&gt;A Simple Setting&lt;/h2&gt;

&lt;p&gt;I want to illustrate the SMM in one of the simplest setting you could think of:
the estimation of the mean of a normal density. Let&amp;rsquo;s say we have access to a (bi-dimensional) time series and we suspect it to be normally distributed with mean $\mathcal{N}([a,\,b]&amp;lsquo;,\,I_2)$. Let&amp;rsquo;s pretend that we have no idea of how to write down the associated likelihood function. The good news is that we have access to a &amp;ldquo;black box&amp;rdquo; that generates $i.i.d$ draws from to the law $\mathcal{N}([c,\,d]&amp;lsquo;,\,I_2)$, for any value $c$ and $d$ . It turns out that having access to this black box is enough for
us to do inference.&lt;/p&gt;

&lt;h2 id=&#34;smm-is-gmm&#34;&gt;SMM is GMM&lt;/h2&gt;

&lt;p&gt;the SMM can be viewed as an extension of the GMM estimator, in which we use both empirical and simulated data. Mathematically, we want to minimize the objective function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation_SMM.png&#34; alt=&#34;SMM objective function\label{objective_function}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where $m^*$ is vector of moments based on empirical observations, $m(\theta)$ a vector of the same moments
calculated using simulated data when the structural parameters are equal to $\theta$, and $W$ a &lt;a href=&#34;https://en.wikipedia.org/wiki/Weighing_matrix&#34; target=&#34;_blank&#34;&gt;weighting matrix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The SMM estimate will be such that the (weighted) distance between simulated and real-world moments are minimized. This estimator is quite intuitive: under the hypothesis that our model is correctly specified, our model should be able to reproduce the stylized facts of the data when the parameters are equal to the &amp;ldquo;true&amp;rdquo; ones.&lt;/p&gt;

&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;

&lt;p&gt;Under some regularity conditions (see &lt;a href=&#34;https://www.google.fr/search?client=ubuntu&amp;amp;channel=fs&amp;amp;q=+McFadden+1989&amp;amp;ie=utf-8&amp;amp;oe=utf-8&amp;amp;gfe_rd=cr&amp;amp;dcr=0&amp;amp;ei=a52BWsGULYeO8Qfq-p3YCw&#34; target=&#34;_blank&#34;&gt;McFadden 1989&lt;/a&gt;), the extra noise introduced by simulation is
not problematic and inference is possible. That is, we can build a confidence interval
for our SMM estimates using the standard GMM approach.&lt;/p&gt;

&lt;p&gt;An interesting fact is that we can also treat the histogram of realizations the markov chain , $S=\big( \theta^1, \theta^2, &amp;hellip;, \theta^N \big)$, as a &amp;ldquo;quasi-posterior&amp;rdquo; distribution (see &lt;a href=&#34;http://www.mit.edu/~vchern/ch_qbe.pdf&#34; target=&#34;_blank&#34;&gt;Chernozhukov 2003&lt;/a&gt;). The beauty of this approach is that for a given continously differentiable function $g:\Theta \rightarrow â„œ$, the $90\%$ confidence intervals are constructed by taking the $.05th$ and $0.95th$ quantiles of the sequence  $g(S)=\big( g(\theta^1), g(\theta^2), &amp;hellip;, g(\theta^N )\big)$.&lt;/p&gt;

&lt;h2 id=&#34;implementation-in-julia&#34;&gt;Implementation in Julia&lt;/h2&gt;

&lt;p&gt;The code below shows how one can recover the true parameters of the Normal density $\mathcal{N}([a,\,b]&amp;lsquo;,\,I_2)$. I use the package &lt;a href=&#34;https://github.com/floswald/MomentOpt.jl&#34; target=&#34;_blank&#34;&gt;MomentOpt&lt;/a&gt;, which uses some &lt;a href=&#34;(https://arxiv.org/abs/1108.3423)&#34; target=&#34;_blank&#34;&gt;refinements&lt;/a&gt; of the &lt;a href=&#34;(https://arxiv.org/abs/1108.3423)&#34; target=&#34;_blank&#34;&gt;MCMC&lt;/a&gt; methods to explore the state-space, using several Markov chains in parallel ($5$ chains in my example). Figure 1 shows the realizations for one Markov chain. As illustrated in Figure 2, we successfully recovered the true values for $a$ and $b$.  The quasi-posterior mean and median for $a$ and $b$ are extremely close to the true values ($1$ and $-1$).&lt;/p&gt;

&lt;h4 id=&#34;figure-1&#34;&gt;Figure 1&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/history_chain_1.svg&#34; alt=&#34;history one chain&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;History of one chain&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;figure-2&#34;&gt;Figure 2&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/histogram.svg&#34; alt=&#34;histogram SMM&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Histograms for the 5 chains&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#---------------------------------------------------------------------------------------------------------
# Julien Pascal
# last edit: 12/02/2018
#
# Julia script that shows how the simulated
# method of moments can be used in a simple
# setting: estimation of the mean of a Normal r.v.
#
# I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl
#
# Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl
#----------------------------------------------------------------------------------------------------------

using MomentOpt
using GLM
using DataStructures
using DataFrames
using Plots
#plotlyjs()
pyplot()

#------------------------------------------------
# Options
#-------------------------------------------------
# Boolean: do you want to save the plots to disk?
savePlots = true

# initialize the problem:
#------------------------
# initial values:
#----------------
pb    = OrderedDict(&amp;quot;p1&amp;quot; =&amp;gt; [0.2,-3,3] , &amp;quot;p2&amp;quot; =&amp;gt; [-0.2,-2,2] )
# moments to be matched:
#-----------------------
moms = DataFrame(name=[&amp;quot;mu1&amp;quot;,&amp;quot;mu2&amp;quot;],value=[-1.0,1.0], weight=ones(2))



&amp;quot;&amp;quot;&amp;quot;
    objfunc_normal(ev::Eval)

    GMM objective function to be minized.
    It returns a weigthed distance between empirical and simulated moments

    copy-paste of the function objfunc_norm(ev::Eval)
    I only made minor modifications to the original fuction

&amp;quot;&amp;quot;&amp;quot;

function objfunc_normal(ev::Eval)

    start(ev)


    # extract parameters from ev:
    #----------------------------
    mu  = collect(values(ev.params))

    # compute simulated moments
    #--------------------------
    # Monte-Carlo:
    #-------------
    ns = 10000 #number of i.i.d draws from N([a,b], sigma)
    #initialize a multivariate normal N([a,b], sigma)
    #using a = mu[1], b=mu[2]
    sigma = [1.0 ;1.0]
    randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma))
    simM            = mean(rand(randMultiNormal,ns),2) #mean of simulated data
    simMoments = Dict(:mu1 =&amp;gt; simM[1], :mu2 =&amp;gt; simM[2])#store simulated moments in a dictionary


    # Calculate the weighted distance between empirical moments
    # and simulated ones:
    #-----------------------------------------------------------
    v = Dict{Symbol,Float64}()
    for (k, mom) in dataMomentd(ev)
        # If weight for moment k exists:
        #-------------------------------
        if haskey(MomentOpt.dataMomentWd(ev), k)
            # divide by weight associated to moment k:
            #----------------------------------------
            v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2
        else
            v[k] = ((simMoments[k] .- mom) ) .^2
        end
    end

    # Set value of the objective function:
    #------------------------------------
    setValue(ev, mean(collect(values(v))))

    # also return the moments
    #-----------------------
    setMoment(ev, simMoments)

    # flag for success?:
    #-------------------
    ev.status = 1

    # finish and return
    finish(ev)

    return ev
end



# Initialize an empty MProb() object:
#------------------------------------
mprob = MProb()

# Add structural parameters to MProb():
# specify starting values and support
#--------------------------------------
addSampledParam!(mprob,pb)

# Add moments to be matched to MProb():
#--------------------------------------
addMoment!(mprob,moms)

# Attach an objective function to MProb():
#----------------------------------------
addEvalFunc!(mprob, objfunc_normal)


# estimation options:
#--------------------
# number of iterations for each chain
niter = 1000
# number of chains
nchains = 5

opts = Dict(&amp;quot;N&amp;quot;=&amp;gt;nchains,
        &amp;quot;maxiter&amp;quot;=&amp;gt;niter,
        &amp;quot;maxtemp&amp;quot;=&amp;gt; 5,
        # choose inital sd for each parameter p
        # such that Pr( x \in [init-b,init+b]) = 0.975
        # where b = (p[:ub]-p[:lb])*opts[&amp;quot;coverage&amp;quot;] i.e. the fraction of the search interval you want to search around the initial value
        &amp;quot;coverage&amp;quot;=&amp;gt;0.025,  # i.e. this gives you a 95% CI about the current parameter on chain number 1.
        &amp;quot;sigma_update_steps&amp;quot;=&amp;gt;10,
        &amp;quot;sigma_adjust_by&amp;quot;=&amp;gt;0.01,
        &amp;quot;smpl_iters&amp;quot;=&amp;gt;1000,
        &amp;quot;parallel&amp;quot;=&amp;gt;true,
        &amp;quot;maxdists&amp;quot;=&amp;gt;[0.05 for i in 1:nchains],
        &amp;quot;mixprob&amp;quot;=&amp;gt;0.3,
        &amp;quot;acc_tuner&amp;quot;=&amp;gt;12.0,
        &amp;quot;animate&amp;quot;=&amp;gt;false)


# plot slices of objective function
#---------------------------------
s = doSlices(mprob,30)

# plot objective function over param values:
#-------------------------------------------
p1 = MomentOpt.plot(s,:value)

if savePlots == true
    Plots.savefig(p1, joinpath(pwd(),&amp;quot;slices_Normal1.svg&amp;quot;))
end

# plot value of moment :mu1 over param values
#--------------------------------------------
p2 = MomentOpt.plot(s,:mu1)


if savePlots == true
   Plots.savefig(p2, joinpath(pwd(),&amp;quot;slices_Normal2.svg&amp;quot;))
end

# plot value of moment :mu2 over param values
#--------------------------------------------
p3 = Plots.plot(s,:mu2)

if savePlots == true
    Plots.savefig(p3, joinpath(pwd(),&amp;quot;slices_Normal3.svg&amp;quot;))
end

#---------------------------------------
# Let&#39;s set-up and run the optimization
#---------------------------------------
# set-up BGP algorithm:
MA = MAlgoBGP(mprob,opts)

# run the estimation:
@time MomentOpt.runMOpt!(MA)

# show a summary of the optimization:
@show MomentOpt.summary(MA)

# Plot histograms for chains:
#----------------------------
p4 = histogram(MA.chains[1])
p5 = histogram(MA.chains[2])
p6 = histogram(MA.chains[3])
p7 = histogram(MA.chains[4])
p8 = histogram(MA.chains[5])

p9 = Plots.plot(p4, p5, p6, p7, p8, layout=(5,1), legend=false)

if savePlots == true
    savefig(p9, joinpath(pwd(),&amp;quot;histogram.svg&amp;quot;))
end

# Plot the &amp;quot;history&amp;quot; of one chain:
#--------------------------------
p10 = plot(MA.chains[1])
if savePlots == true
    savefig(p10, joinpath(pwd(),&amp;quot;history_chain_1.svg&amp;quot;))
end


# Realization of chain 1:
#-----------------------
dat_chain1 = MomentOpt.history(MA.chains[1])

# keep only accepted draws:
#-------------------------
dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ]


# Quasi Posterior mean
#---------------------
QP_mean_p1 = mean(dat_chain1[:p1])
QP_mean_p2 = mean(dat_chain1[:p2])

# Quasi Posterior median
#-----------------------
QP_median_p1 = median(dat_chain1[:p1])
QP_median_p2 = median(dat_chain1[:p2])
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The D-CAPM: Reconciling Consumption and Asset Pricing</title>
      <link>https://julienpascal.github.io/project/dcapm/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://julienpascal.github.io/project/dcapm/</guid>
      <description>&lt;p&gt;This is a joint project with &lt;a href=&#34;https://tyler-abbot.github.io/&#34; target=&#34;_blank&#34;&gt;Tyler Abbot&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solving Bellman Equations by Collocation</title>
      <link>https://julienpascal.github.io/post/collocation_method/</link>
      <pubDate>Thu, 07 Dec 2017 17:00:00 +0000</pubDate>
      
      <guid>https://julienpascal.github.io/post/collocation_method/</guid>
      <description>

&lt;p&gt;A large class of economic models involves solving for functional equations of the
form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation1.png&#34; alt=&#34;equation1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A well known example is the &lt;a href=&#34;https://lectures.quantecon.org/jl/optgrowth.html&#34; target=&#34;_blank&#34;&gt;stochastic optimal growth model&lt;/a&gt;. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.&lt;/p&gt;

&lt;p&gt;A common technique for solving this class of problem is value function iteration. While value function
iteration is quite intuitive (it follows the proof of existence of a solution to the functional equation above), it is
not the only one available.
This post describes the collocation method, which transforms the problem of finding
a function into a root-finding one. The gain from this change of perspective is that
any root-finding algorithm can then be used. In particular, one may use the Newton method,
which converges at a quadratic rate in the neighborhood of the solution if the function is smooth enough.&lt;/p&gt;

&lt;h2 id=&#34;value-function-iteration&#34;&gt;Value function iteration&lt;/h2&gt;

&lt;p&gt;Value function iteration takes advantage of the fact that the Bellman operator $T$ is a contraction  mapping on the set of continuous bounded functions on $\mathbb R_+$ under the supremum distance&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation2.png&#34; alt=&#34;equation2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An immediate consequence if that the sequence $w,Tw,T^2w$,â€¦ converges uniformly to  $w$ (starting with with any bounded and continuous $w$). The following code in &lt;code&gt;Julia v0.6&lt;/code&gt; illustrates the convergence of the series ${T^nw}$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt; #=
Julien Pascal

Code heavily based on:
----------------------
https://lectures.quantecon.org/jl/optgrowth.html
by Spencer Lyon, John Stachurski

I have only made minor modifications:
#------------------------------------
* I added a type optGrowth
* I use the package Interpolations
* I calculate the expectation w.r.t the aggregate
  shock using a Gauss-Legendre quadrature scheme
  instead of Monte-Carlo


=#

using QuantEcon
using Optim
using CompEcon
using PyPlot
using Interpolations
using FileIO


type optGrowth

  w::Array{Float64,1}
  Î²::AbstractFloat
  grid::Array{Float64,1}
  u::Function
  f::Function
  shocks::Array{Float64,1}
  Tw::Array{Float64,1}
  Ïƒ::Array{Float64,1}
  el_k::Array{Float64,1}
  wl_k::Array{Float64,1}
  compute_policy::Bool
  w_func::Function

end

function optGrowth(;w = Array{Float64,1}[],
                    Î± = 0.4,
                    Î² = 0.96,
                    Î¼ = 0,
                    s = 0.1,
                    grid_max = 4,         # Largest grid point
                    grid_size = 200,      # Number of grid points
                    shock_size = 250,     # Number of shock draws in Monte Carlo integral
                    Tw = Array{Float64,1}[],
                    Ïƒ = Array{Float64,1}[],
                    el_k = Array{Float64,1}[],
                    wl_k = Array{Float64,1}[],
                    compute_policy = true
                  )


  grid_y = collect(linspace(1e-5, grid_max, grid_size))
  shocks = exp.(Î¼ + s * randn(shock_size))

  # Utility
  u(c) = log(c)
  # Production
  f(k) = k^Î±

  w = 5 * log.(grid_y)
  Tw = 5 * log.(grid_y)
  Ïƒ = 5 * log.(grid_y)

  el_k, wl_k = qnwlogn(10, Î¼, s^2) #10 weights and nodes for LOG(e_t) distributed N(Î¼,s^2)

  w_func = x -&amp;gt; x
  optGrowth(
    w,
    Î²,
    grid_y,
    u,
    f,
    shocks,
    Tw,
    Ïƒ,
    el_k,
    wl_k,
    compute_policy,
    w_func
    )
end

&amp;quot;&amp;quot;&amp;quot;
The approximate Bellman operator, which computes and returns the
updated value function Tw on the grid points.

#### Arguments

`model` : a model of type optGrowth

`Modifies model.Ïƒ, model.w and model.Tw

&amp;quot;&amp;quot;&amp;quot;
function bellman_operator!(model::optGrowth)

    # === Apply linear interpolation to w === #
    knots = (model.grid,)
    itp = interpolate(knots, model.w, Gridded(Linear()))

    #w_func(x) = itp[x]

    model.w_func = x -&amp;gt; itp[x]

    if model.compute_policy
        model.Ïƒ = similar(model.w)
    end

    # == set Tw[i] = max_c { u(c) + Î² E w(f(y  - c) z)} == #
    for (i, y) in enumerate(model.grid)

        #Monte Carlo
        #-----------
        #objective(c) = - model.u(c) - model.Î² * mean(w_func(model.f(y - c) .* model.shocks))

        #Gauss-Legendre
        #--------------
        function objective(c)

          expectation = 0.0

          for k = 1:length(model.wl_k)
            expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k]))
          end

          - model.u(c) - model.Î² * expectation

        end

        res = optimize(objective, 1e-10, y)

        if model.compute_policy
            model.Ïƒ[i] = Optim.minimizer(res)
        end

        model.Tw[i] = - Optim.minimum(res)
        model.w[i] = - Optim.minimum(res)
    end


end


model = optGrowth()

function solve_optgrowth!(model::optGrowth;
                         tol::AbstractFloat=1e-6,
                         max_iter::Integer=500)

    w_old = copy(model.w)  # Set initial condition
    error = tol + 1
    i = 0


    # Iterate to find solution
    while i &amp;lt; max_iter

        #update model.w
        bellman_operator!(model)

        error = maximum(abs, model.w - w_old)

        if error &amp;lt; tol
          break
        end

        w_old = copy(model.w)
        i += 1
    end

end

#-----------------------------------
# Solve by value function iteration
#-----------------------------------
@time solve_optgrowth!(model)
# 3.230501 seconds (118.18 M allocations: 1.776 GiB, 3.51% gc time)


#-------------------------------
# Compare with the true solution
#-------------------------------
Î± = 0.4
Î² = 0.96
Î¼ = 0
s = 0.1

c1 = log(1 - Î± * Î²) / (1 - Î²)
c2 = (Î¼ + Î± * log(Î± * Î²)) / (1 - Î±)
c3 = 1 / (1 - Î²)
c4 = 1 / (1 - Î± * Î²)


# True optimal policy
c_star(y) = (1 - Î± * Î²) * y

# True value function
v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y)

fig, ax = subplots(figsize=(9, 5))
ax[:set_ylim](-35, -24)
ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=&amp;quot;approximate value function&amp;quot;)
ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=&amp;quot;true value function&amp;quot;)
ax[:legend](loc=&amp;quot;lower right&amp;quot;)

fig, ax = subplots(figsize=(9, 5))
ax[:set_xlim](0.1, 4.0)
ax[:set_ylim](0.00, 0.2)
ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=&amp;quot;error&amp;quot;)
ax[:legend](loc=&amp;quot;lower right&amp;quot;)



&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/value_function_iteration.png&#34; alt=&#34;VFI&#34; /&gt;
&lt;img src=&#34;https://julienpascal.github.io/img/error_value_function_iteration.png&#34; alt=&#34;VFI&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-collocation-method&#34;&gt;The collocation method&lt;/h2&gt;

&lt;p&gt;The collocation method takes a different route. Let us remember that we are looking for a function $w$. Instead of solving for the values of $w$ on a grid and then interpolating, why not looking for a function directly? To do so, let us assume that $w$ can reasonably be approximated by a function $\hat{w}$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation3.png&#34; alt=&#34;equation3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;with
$ \phi_1(x) $ , $ \phi_2(x) $,&amp;hellip;, $ \phi_n(x) $  a set of linearly independent basis functions and $c_1$, $c_2$, &amp;hellip;, $c_n$  $n$ coefficient to be found. Replacing $w(x)$ with $\hat{w(x)}$ into the functional equation and reorganizing gives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation4.png&#34; alt=&#34;equation4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This equation has to hold (almost) exactly at $n$ points (also called nodes): $y_1$, $y_2$, &amp;hellip;, $y_n$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation5.png&#34; alt=&#34;equation5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The equation above defines a system of $n$ equation with as many unknown, which can be compactly written as:
$$ f(\boldsymbol{c}) = \boldsymbol{0} $$&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_method&#34; target=&#34;_blank&#34;&gt;Newton&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Quasi-Newton_method&#34; target=&#34;_blank&#34;&gt;quasi-Newton&lt;/a&gt; can be used to solve for the root of $f$. In the code that follows, I use &lt;a href=&#34;https://en.wikipedia.org/wiki/Broyden%27s_method&#34; target=&#34;_blank&#34;&gt;Broyden&amp;rsquo;s&lt;/a&gt; method. Let us illustrate this technique using a Chebychev polynomial basis and Chebychev nodes. In doing so, we avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%27s_phenomenon&#34; target=&#34;_blank&#34;&gt;Runge&amp;rsquo;s phenomenon&lt;/a&gt; associated with a uniform grid.&lt;/p&gt;

&lt;h3 id=&#34;implementation-using-compecon&#34;&gt;Implementation using CompEcon&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt; #---------------------------------------------

 # Julien Pascal

 # Solve the stochastic optimal growth problem
 # using the collocation method

 #---------------------------------------------




 using QuantEcon
 using Optim
 using CompEcon
 using PyPlot
 using Interpolations



 type optGrowthCollocation

   w::Array{Float64,1}
   Î²::AbstractFloat
   grid::Array{Float64,1}
   u::Function
   f::Function
   shocks::Array{Float64,1}
   Tw::Array{Float64,1}
   Ïƒ::Array{Float64,1}
   el_k::Array{Float64,1}
   wl_k::Array{Float64,1}
   compute_policy::Bool
   order_approximation::Int64 #number of element in the functional basis along each dimension
   functional_basis_type::String #type of functional basis
   fspace::Dict{Symbol,Any} #functional basis
   fnodes::Array{Float64,1} #collocation nodes
   residual::Array{Float64,1} #vector of residual. Should be close to zero
   a::Array{Float64,1} #polynomial coefficients
   w_func::Function

 end


   #####################################
   # Function that finds a solution
   # to f(x) = 0
   # using Broyden&#39;s &amp;quot;good&amp;quot; method
   # and a simple backstepping procedure as described
   # in Miranda and Fackler (2009)
   #
   # input :
   # --------
   # * x0:                 initial guess for the root
   # * f:                  function in f(x) = 0
   # * maxit:              maximum number of iterations
   # * tol:                tolerance level for the zero
   # * fjavinc:            initial inverse of the jacobian. If not provided, then inverse of the
   #                       Jacobian is calculated by finite differences
   # * maxsteps:           maximum number of backsteps
   # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian
   #
   # output :
   # --------
   # * x: one zero of f
   # * it: number of iterations necessary to reached the solution
   # * fjacinv: pseudo jacobian at the last iteration
   # * fnorm: norm f(x) at the last iteration
   #
   #######################################
   function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0));
                         maxsteps = 5, recaculateJacobian = 1)

       println(&amp;quot;a0 = $(x0)&amp;quot;)
       fnorm = tol*2
       it2 = 0 #to re-initialize the jacobian

       ################################
       #initialize guess for the matrix
       ################################
       fjacinv_function = x-&amp;gt; Calculus.finite_difference_jacobian(f, x)
       #fjacinv_function = x -&amp;gt; ForwardDiff.gradient(f, x)

       # If the user do not provide an initial guess for the jacobian
       # One is calculated using finite differences.
       if fjacinv == eye(length(x0))
           ################################################
           # finite differences to approximate the Jacobian
           # at the initial value
           # this is slow. Seems to improve performances
           # when x0 is of high dimension.
           println(&amp;quot;Calculating the Jacobian by finite differences&amp;quot;)
           #@time fjacinv = Calculus.finite_difference_jacobian(f, x0)
           @time fjacinv = fjacinv_function(x0)

           println(&amp;quot;Inverting the Jacobian&amp;quot;)
           try
               fjacinv = inv(fjacinv)
           catch
               try
                   println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                   fjacinv = pinv(A)
               catch
                   println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                   fjacinv = eye(length(x0))
               end
           end
           println(&amp;quot;Done&amp;quot;)
       else
           println(&amp;quot;Using User&#39;s input as a guess for the Jacobian.&amp;quot;)
       end

       fval = f(x0)

       for it=1:maxit

           it2 +=1

           #every 30 iterations, reinitilize the jacobian
           if mod(it2, recaculateJacobian) == 0

               println(&amp;quot;Re-calculating the Jacobian&amp;quot;)

               fjacinv = fjacinv_function(x0)

               try
                   fjacinv = inv(fjacinv)
               catch
                   try
                       println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                       fjacinv = pinv(A)
                   catch
                       println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                       fjacinv = eye(length(x0))
                   end
               end

           end


           println(&amp;quot;it = $(it)&amp;quot;)

           fnorm = norm(fval)

           if fnorm &amp;lt; tol
               println(&amp;quot;fnorm = $(fnorm)&amp;quot;)
               return x0, it, fjacinv, fnorm
           end

           d = -(fjacinv*fval)

           fnormold = Inf
           ########################
           # Backstepping procedure
           ########################
           for backstep = 1:maxsteps

               if backstep &amp;gt; 1
                   println(&amp;quot;backstep = $(backstep-1)&amp;quot;)
               end

               fvalnew = f(x0 + d)
               fnormnew = norm(fvalnew)

               if fnormnew &amp;lt; fnorm
                   break
               end

               if fnormold &amp;lt; fnormnew
                   d=2*d
                   break
               end

               fnormold = fnormnew

               d = d/2

           end
           ####################
           ####################

           x0 = x0 + d

           fold = fval
           fval = f(x0)

           u = fjacinv*(fval - fold)

           #Update the pseudo Jacobian:
           fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u))

           println(&amp;quot;a$(it) = $(x0)&amp;quot;)
           println(&amp;quot;fnorm = $(fnorm)&amp;quot;)

           if isnan.(x0) == trues(length(x0))
               println(&amp;quot;Error. a$(it) = NaN for each component&amp;quot;)
               x0 = zeros(length(x0))
               return x0, it, fjacinv, fnorm
           end
       end

       println(&amp;quot;In function find_broyden\n&amp;quot;)
       println(&amp;quot;Maximum number of iterations reached.\n&amp;quot;)
       println(&amp;quot;No convergence.&amp;quot;)
       println(&amp;quot;Returning fnorm = NaN as a solution&amp;quot;)
       fnorm = NaN
       return x0, maxit, fjacinv, fnorm

   end

 function optGrowthCollocation(;w = Array{Float64,1}[],
                               Î± = 0.4,
                               Î² = 0.96,
                               Î¼ = 0,
                               s = 0.1,
                               grid_max = 4,         # Largest grid point
                               grid_size = 200,      # Number of grid points
                               shock_size = 250,     # Number of shock draws in Monte Carlo integral
                               Tw = Array{Float64,1}[],
                               Ïƒ = Array{Float64,1}[],
                               el_k = Array{Float64,1}[],
                               wl_k = Array{Float64,1}[],
                               compute_policy = true,
                               order_approximation = 40,
                               functional_basis_type = &amp;quot;chebychev&amp;quot;,
                             )


   grid_y = collect(linspace(1e-5, grid_max, grid_size))
   shocks = exp.(Î¼ + s * randn(shock_size))

   # Utility
   u(c) = log.(c)
   # Production
   f(k) = k^Î±

   el_k, wl_k = qnwlogn(10, Î¼, s^2) #10 weights and nodes for LOG(e_t) distributed N(Î¼,s^2)

   lower_bound_support = minimum(grid_y)
   upper_bound_support = maximum(grid_y)

   n_functional_basis = [order_approximation]

   if functional_basis_type == &amp;quot;chebychev&amp;quot;
       fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support)
   elseif functional_basis_type == &amp;quot;splines&amp;quot;
       fspace = fundefn(:spli, n_functional_basis, lower_bound_support, upper_bound_support, 1)
   elseif functional_basis_type == &amp;quot;linear&amp;quot;
       fspace = fundefn(:lin, n_functional_basis, lower_bound_support, upper_bound_support)
   else
       error(&amp;quot;functional_basis_type has to be either chebychev, splines or linear.&amp;quot;)
   end


   fnodes = funnode(fspace)[1]
   residual = zeros(size(fnodes)[1])
   a = ones(size(fnodes)[1])

   w = ones(size(fnodes)[1])
   Tw = ones(size(fnodes)[1])
   Ïƒ = ones(size(fnodes)[1])

   w_func = x-&amp;gt; x

   optGrowthCollocation(
     w,
     Î²,
     grid_y,
     u,
     f,
     shocks,
     Tw,
     Ïƒ,
     el_k,
     wl_k,
     compute_policy,
     order_approximation,
     functional_basis_type,
     fspace,
     fnodes,
     residual,
     a,
     w_func
     )
 end



 function residual!(model::optGrowthCollocation)


     model.w_func = y -&amp;gt; funeval(model.a, model.fspace, [y])[1][1]   


     function objective(c, y)

       expectation = 0.0

       for k = 1:length(model.wl_k)
         expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k]))
       end

       - model.u(c) - model.Î² * expectation

     end

     # Loop over nodes
     for i in 1:size(model.fnodes)[1]

         y = model.fnodes[i,1]

         res = optimize(c -&amp;gt; objective(c, y), 1e-10, y)

         if model.compute_policy
             model.Ïƒ[i] = Optim.minimizer(res)
         end

         model.Tw[i] = - Optim.minimum(res)
         model.w[i] = model.w_func(y)

         model.residual[i] = - model.w[i] + model.Tw[i]
     end

 end


 model = optGrowthCollocation(functional_basis_type = &amp;quot;chebychev&amp;quot;)

 residual!(model)

 function solve_optgrowth!(model::optGrowthCollocation;
                          tol=1e-6,
                          max_iter=500)

     # Initialize guess for coefficients
     # by giving the &amp;quot;right shape&amp;quot;
     # ---------------------------------
     function objective_initialize!(x, model)

       #update polynomial coeffficients
       model.a = copy(x)

       model.w_func = y -&amp;gt; funeval(model.a, model.fspace, [y])[1][1]

       return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1]))

     end


     minx, iterations, Jac0, fnorm = find_broyden(model.a, x -&amp;gt; objective_initialize!(x, model), max_iter, tol)


     # Solving the model by collocation
     # using the initial guess calculated above
     #-----------------------------------------
     function objective_residual!(x, model)

       #update polynomial coeffficients
       model.a = copy(x)

       #calculate residual
       residual!(model)

       return abs.(model.residual)

     end

     minx, iterations, Jac, fnorm = find_broyden(model.a, x -&amp;gt; objective_residual!(x, model), max_iter, tol)


 end

 #-----------------------------------
 # Solve by collocation
 #-----------------------------------
 @time solve_optgrowth!(model)


 #-------------------------------
 # Compare with the true solution
 #-------------------------------
 Î± = 0.4
 Î² = 0.96
 Î¼ = 0
 s = 0.1

 c1 = log(1 - Î± * Î²) / (1 - Î²)
 c2 = (Î¼ + Î± * log(Î± * Î²)) / (1 - Î±)
 c3 = 1 / (1 - Î²)
 c4 = 1 / (1 - Î± * Î²)


 # True optimal policy
 c_star(y) = (1 - Î± * Î²) * y

 # True value function
 v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y)

 fig, ax = subplots(figsize=(9, 5))
 ax[:set_ylim](-35, -24)
 ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=&amp;quot;approximate value function&amp;quot;)
 ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=&amp;quot;true value function&amp;quot;)
 ax[:legend](loc=&amp;quot;lower right&amp;quot;)


 fig, ax = subplots(figsize=(9, 5))
 ax[:set_xlim](0.1, 4.0)
 ax[:set_ylim](0.01, 0.2)
 ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=&amp;quot;error&amp;quot;)
 ax[:legend](loc=&amp;quot;lower right&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output-1&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/collocation.png&#34; alt=&#34;VFI&#34; /&gt;
 &lt;img src=&#34;https://julienpascal.github.io/img/error_collocation.png&#34; alt=&#34;VFI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We obtain more are less the same function $w$. But it turns out that the value function iteration implementation is much faster. One reason seems to be the efficiency associated with the package &lt;a href=&#34;https://github.com/JuliaMath/Interpolations.jl&#34; target=&#34;_blank&#34;&gt;Interpolations&lt;/a&gt;: it is more than 20 times faster to evaluate $w$ using the package Interpolations rather than using the package &lt;a href=&#34;https://github.com/QuantEcon/CompEcon.jl&#34; target=&#34;_blank&#34;&gt;CompEcon&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Julia&#34;&gt;  #using Interpolations
  #--------------------
  @time for i=1:1000000
   model.w_func.(model.grid[1])
  end
  #0.230861 seconds (2.00 M allocations: 30.518 MiB, 1.28% gc time)

  #using CompEcon
  #--------------
  @time for i=1:1000000
   model.w_func.(model.grid[1])
  end
  # 4.998902 seconds (51.00 M allocations: 3.546 GiB, 13.39% gc time)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;implementation-using-approxfun&#34;&gt;Implementation using ApproxFun&lt;/h3&gt;

&lt;p&gt;Significant speed-up can be obtained by using the package &lt;a href=&#34;https://github.com/JuliaApproximation/ApproxFun.jl&#34; target=&#34;_blank&#34;&gt;ApproxFun&lt;/a&gt;, as illustrated by the code below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;
#---------------------------------------------

# Julien Pascal

# Solve the stochastic optimal growth problem
# using the collocation method

# Implementation using ApproxFun

#---------------------------------------------




using QuantEcon
using Optim
using CompEcon
using PyPlot
using Interpolations
using FileIO
using ApproxFun
using ProfileView



type optGrowthCollocation

  w::Array{Float64,1}
  Î²::AbstractFloat
  grid::Array{Float64,1}
  u::Function
  f::Function
  shocks::Array{Float64,1}
  Tw::Array{Float64,1}
  Ïƒ::Array{Float64,1}
  el_k::Array{Float64,1}
  wl_k::Array{Float64,1}
  compute_policy::Bool
  order_approximation::Int64 #number of element in the functional basis along each dimension
  functional_basis_type::String #type of functional basis
  fspace::Dict{Symbol,Any} #functional basis
  fnodes::Array{Float64,1} #collocation nodes
  residual::Array{Float64,1} #vector of residual. Should be close to zero
  a::Array{Float64,1} #polynomial coefficients
  fApprox::ApproxFun.Fun{ApproxFun.Chebyshev{ApproxFun.Segment{Float64},Float64},Float64,Array{Float64,1}}
  w_func::Function
  tol::Float64

end


  #####################################
  # Function that find a solution
  # to f(x) = 0
  # using Broyden&#39;s &amp;quot;good&amp;quot; method
  # and simple backstepping procedure as described
  # in Miranda and Fackler (2009)
  #
  # input :
  # --------
  # * x0:                 initial guess for the root
  # * f:                  function in f(x) = 0
  # * maxit:              maximum number of iterations
  # * tol:                tolerance level for the zero
  # * fjavinc:            initial inverse of the jacobian. If not provided, then inverse of the
  #                       Jacobian is calculated by finite differences
  # * maxsteps:           maximum number of backsteps
  # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian
  #
  # output :
  # --------
  # * x: one zero of f
  # * it: number of iterations necessary to reached the solution
  # * fjacinv: pseudo jacobian at the last iteration
  # * fnorm: norm f(x) at the last iteration
  #
  #######################################
  function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0));
                        maxsteps = 5, recaculateJacobian = 1)

      println(&amp;quot;a0 = $(x0)&amp;quot;)
      fnorm = tol*2
      it2 = 0 #to re-initialize the jacobian

      ################################
      #initialize guess for the matrix
      ################################
      # with Calculus
      #--------------
      fjacinv_function = x-&amp;gt; Calculus.finite_difference_jacobian(f, x)


      # If the user do not provide an initial guess for the jacobian
      # One is calculated using finite differences.
      if fjacinv == eye(length(x0))
          ################################################
          # finite differences to approximate the Jacobian
          # at the initial value
          # this is slow. Seems to improve performances
          # when x0 is of high dimension.
          println(&amp;quot;Calculating the Jacobian by finite differences&amp;quot;)
          #@time fjacinv = Calculus.finite_difference_jacobian(f, x0)
          @time fjacinv = fjacinv_function(x0)

          println(&amp;quot;Inverting the Jacobian&amp;quot;)
          try
              fjacinv = inv(fjacinv)
          catch
              try
                  println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                  fjacinv = pinv(A)
              catch
                  println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                  fjacinv = eye(length(x0))
              end
          end
          println(&amp;quot;Done&amp;quot;)
      else
          println(&amp;quot;Using User&#39;s input as a guess for the Jacobian.&amp;quot;)
      end

      fval = f(x0)

      for it=1:maxit

          it2 +=1

          #every 30 iterations, reinitilize the jacobian
          if mod(it2, recaculateJacobian) == 0

              println(&amp;quot;Re-calculating the Jacobian&amp;quot;)

              fjacinv = fjacinv_function(x0)

              try
                  fjacinv = inv(fjacinv)
              catch
                  try
                      println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                      fjacinv = pinv(A)
                  catch
                      println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                      fjacinv = eye(length(x0))
                  end
              end

          end


          println(&amp;quot;it = $(it)&amp;quot;)

          fnorm = norm(fval)

          if fnorm &amp;lt; tol
              println(&amp;quot;fnorm = $(fnorm)&amp;quot;)
              return x0, it, fjacinv, fnorm
          end

          d = -(fjacinv*fval)

          fnormold = Inf
          ########################
          # Backstepping procedure
          ########################
          for backstep = 1:maxsteps

              if backstep &amp;gt; 1
                  println(&amp;quot;backstep = $(backstep-1)&amp;quot;)
              end

              fvalnew = f(x0 + d)
              fnormnew = norm(fvalnew)

              if fnormnew &amp;lt; fnorm
                  break
              end

              if fnormold &amp;lt; fnormnew
                  d=2*d
                  break
              end

              fnormold = fnormnew

              d = d/2

          end
          ####################
          ####################

          x0 = x0 + d

          fold = fval
          fval = f(x0)

          u = fjacinv*(fval - fold)

          #Update the pseudo Jacobian:
          fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u))

          println(&amp;quot;a$(it) = $(x0)&amp;quot;)
          println(&amp;quot;fnorm = $(fnorm)&amp;quot;)

          if isnan.(x0) == trues(length(x0))
              println(&amp;quot;Error. a$(it) = NaN for each component&amp;quot;)
              x0 = zeros(length(x0))
              return x0, it, fjacinv, fnorm
          end
      end

      println(&amp;quot;In function find_broyden\n&amp;quot;)
      println(&amp;quot;Maximum number of iterations reached.\n&amp;quot;)
      println(&amp;quot;No convergence.&amp;quot;)
      println(&amp;quot;Returning fnorm = NaN as a solution&amp;quot;)
      fnorm = NaN
      return x0, maxit, fjacinv, fnorm

  end

function optGrowthCollocation(;w = Array{Float64,1}[],
                              Î± = 0.4,
                              Î² = 0.96,
                              Î¼ = 0,
                              s = 0.1,
                              grid_max = 4,         # Largest grid point
                              grid_size = 200,      # Number of grid points
                              shock_size = 250,     # Number of shock draws in Monte Carlo integral
                              Tw = Array{Float64,1}[],
                              Ïƒ = Array{Float64,1}[],
                              el_k = Array{Float64,1}[],
                              wl_k = Array{Float64,1}[],
                              compute_policy = true,
                              order_approximation = 40,
                              functional_basis_type = &amp;quot;chebychev&amp;quot;,
                            )


  grid_y = collect(linspace(1e-4, grid_max, grid_size))
  shocks = exp.(Î¼ + s * randn(shock_size))

  # Utility
  u(c) = log.(c)
  # Production
  f(k) = k^Î±

  el_k, wl_k = qnwlogn(10, Î¼, s^2) #10 weights and nodes for LOG(e_t) distributed N(Î¼,s^2)

  lower_bound_support = minimum(grid_y)
  upper_bound_support = maximum(grid_y)

  n_functional_basis = [order_approximation]

  if functional_basis_type == &amp;quot;chebychev&amp;quot;
      fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support)
  else
      error(&amp;quot;functional_basis_type has to be \&amp;quot;chebychev\&amp;quot; &amp;quot;)
  end


  fnodes = funnode(fspace)[1]
  residual = zeros(size(fnodes)[1])
  a = ones(size(fnodes)[1])

  tol = 0.001

  fApprox = (Fun(Chebyshev((minimum(grid_y))..(maximum(grid_y))), a))
  #fApprox = (Fun(Chebyshev(0..maximum(model.grid)), a))

  w_func = x-&amp;gt; fApprox(x)

  w = ones(size(fnodes)[1])
  Tw = ones(size(fnodes)[1])
  Ïƒ = ones(size(fnodes)[1])



  optGrowthCollocation(
    w,
    Î²,
    grid_y,
    u,
    f,
    shocks,
    Tw,
    Ïƒ,
    el_k,
    wl_k,
    compute_policy,
    order_approximation,
    functional_basis_type,
    fspace,
    fnodes,
    residual,
    a,
    fApprox,
    w_func,
    tol
    )
end



function residual!(model::optGrowthCollocation)


    model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a))
    model.w_func = x-&amp;gt; model.fApprox(x)

    function objective(c, y)

      expectation = 0.0

      for k = 1:length(model.wl_k)
        expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k]))
      end

      - model.u(c) - model.Î² * expectation

    end

    # Loop over nodes
    for i in 1:size(model.fnodes)[1]

        y = model.fnodes[i,1]

        res = optimize(c -&amp;gt; objective(c, y), 1e-10, y)

        if model.compute_policy
            model.Ïƒ[i] = Optim.minimizer(res)
        end

        model.Tw[i] = - Optim.minimum(res)
        model.w[i] = model.w_func(y)

        model.residual[i] = - model.w[i] + model.Tw[i]
    end

end


function solve_optgrowth!(model::optGrowthCollocation;
                         tol=1e-6,
                         max_iter=500)

    # Initialize guess for coefficients
    # by giving the &amp;quot;right shape&amp;quot;
    # ---------------------------------
    function objective_initialize!(x, model)

      #update polynomial coeffficients
      model.a = copy(x)

      model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a))

      model.w_func = x-&amp;gt; model.fApprox(x)

      return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1]))

    end


    minx, iterations, Jac0, fnorm = find_broyden(model.a, x -&amp;gt; objective_initialize!(x, model), max_iter, tol)


    # Solving the model by collocation
    # using the initial guess calculated above
    #-----------------------------------------
    function objective_residual!(x, model)

      #update polynomial coeffficients
      model.a = copy(x)

      #calculate residual
      residual!(model)

      return abs.(model.residual)

    end

    minx, iterations, Jac, fnorm = find_broyden(model.a, x -&amp;gt; objective_residual!(x, model), max_iter, tol,  recaculateJacobian = 1)


end

#-----------------------------------
# Solve by collocation
#-----------------------------------
model = optGrowthCollocation(functional_basis_type = &amp;quot;chebychev&amp;quot;)
@time solve_optgrowth!(model)
# 15.865923 seconds (329.12 M allocations: 4.977 GiB, 5.55% gc time)


#-------------------------------
# Compare with the true solution
#-------------------------------
Î± = 0.4
Î² = 0.96
Î¼ = 0
s = 0.1

c1 = log(1 - Î± * Î²) / (1 - Î²)
c2 = (Î¼ + Î± * log(Î± * Î²)) / (1 - Î±)
c3 = 1 / (1 - Î²)
c4 = 1 / (1 - Î± * Î²)


# True optimal policy
c_star(y) = (1 - Î± * Î²) * y

# True value function
v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y)

fig, ax = subplots(figsize=(9, 5))
ax[:set_ylim](-35, -24)
ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=&amp;quot;approximate value function&amp;quot;)
ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=&amp;quot;true value function&amp;quot;)

fig, ax = subplots(figsize=(9, 5))
ax[:set_xlim](0.1, 4.0)
ax[:set_ylim](- 0.05, 0.05)
ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=&amp;quot;error&amp;quot;)
ax[:legend](loc=&amp;quot;lower right&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output-2&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/collocation_ApproxFun.png&#34; alt=&#34;VFI&#34; /&gt;
&lt;img src=&#34;https://julienpascal.github.io/img/error_collocation_ApproxFun.png&#34; alt=&#34;VFI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Computing time is divided by approximately $5$ using ApproxFun rather than CompEcon. But the value function iteration implementation is still the fastest one. One bottleneck seems to be the calculation of the Jacobian by finite differences. It is likely that using &lt;a href=&#34;https://github.com/JuliaDiff/ForwardDiff.jl&#34; target=&#34;_blank&#34;&gt;automatic differentiation&lt;/a&gt; would result is significant gain of time.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@time for i=1:1000000
 model.w_func.(model.grid[1])
end
# 0.391440 seconds (2.00 M allocations: 30.518 MiB, 1.37% gc time)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Solving a simple RBC model in Dynare</title>
      <link>https://julienpascal.github.io/post/rbc_dynare/</link>
      <pubDate>Sat, 29 Jul 2017 17:00:00 +0000</pubDate>
      
      <guid>https://julienpascal.github.io/post/rbc_dynare/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://www.dynare.org/&#34; target=&#34;_blank&#34;&gt;Dynare&lt;/a&gt; is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see &lt;a href=&#34;http://faculty.chicagobooth.edu/thomas.winberry/research/winberryAlgorithm.pdf&#34; target=&#34;_blank&#34;&gt;Winberry&lt;/a&gt; and &lt;a href=&#34;http://xavier-ragot.fr/pdf/progress/Ragot_chapter.pdf&#34; target=&#34;_blank&#34;&gt;Ragot&lt;/a&gt; for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-model&#34;&gt;A simple model&lt;/h2&gt;

&lt;p&gt;The economy is composed of a representative agent who maximizes his expected
discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,&amp;hellip;,\infty$ &lt;br&gt;&lt;/p&gt;

&lt;p&gt;$$  \sum_{t=1}^{+\infty}\big(\frac{1}{1+\rho}\big)^{t-1} E_t\Big[log(C_t)-\frac{L_t^{1+\gamma}}{1+\gamma}\Big] $$&lt;/p&gt;

&lt;p&gt;subject to the constraint&lt;/p&gt;

&lt;p&gt;$$ K_t = K_t-_1 (1-\delta) + w_t L_t + r_t K_t-_1 - C_t $$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\rho \in (0,\infty)$ is the rate of time preference&lt;/li&gt;
&lt;li&gt;$\gamma \in (0,\infty)$ is a labor supply parameter&lt;/li&gt;
&lt;li&gt;$w_t$ is real wage&lt;/li&gt;
&lt;li&gt;$r_t$ is the real rental rate&lt;/li&gt;
&lt;li&gt;$K_t$ is capital at the end of the period&lt;/li&gt;
&lt;li&gt;$\delta \in (0,1)$ is the capital depreciation rate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The production function writes
\begin{equation} Y_t = A_t K_t-_1^\alpha \Big((1+g)^t \Big)^{1-\alpha} \end{equation}&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$g \in (0,\infty)$ is the growth rate of production&lt;/li&gt;
&lt;li&gt;$\alpha$ and $\beta$ are technology parameters&lt;/li&gt;
&lt;li&gt;$A_t$ is a technological shock that follows and $AR(1)$ process&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\begin{equation} \log(A_t) = \lambda log(A_t-_1) + e_t\end{equation}&lt;/p&gt;

&lt;p&gt;with $e_t$ an i.i.d zero-mean normally distributed error term with standard deviation
$\sigma_1$ and $\lambda \in (0,1)$ a parameter governing the persistence of shocks.&lt;/p&gt;

&lt;h2 id=&#34;first-order-conditions&#34;&gt;First Order conditions&lt;/h2&gt;

&lt;p&gt;The F.O.C.s of the (stationarized) model are&lt;/p&gt;

&lt;p&gt;$$ \frac{1}{\hat{C_t}} = \frac{1}{1+\rho} E_t \Big( \frac{r_t+_1 + 1 - \delta}{\hat{C}_t+_1 (1+g)} \Big)$$&lt;/p&gt;

&lt;p&gt;$$ L_t^\gamma = \frac{\hat{w}_t}{\hat{C}_t}$$&lt;/p&gt;

&lt;p&gt;$$ r_t = \alpha A_t \Big(\frac{\hat{K}_t-_1}{1+g}\Big)^{\alpha-1}L_t^{1-\alpha}$$&lt;/p&gt;

&lt;p&gt;$$ \hat{w}_t = (1-\alpha) A_t \Big(\frac{\hat{K}_t-_1}{1+g}\Big)^{\alpha}L_t^{-\alpha} $$&lt;/p&gt;

&lt;p&gt;$$ \hat{K}_t + \hat{C}_t = \frac{\hat{K}_t-_1}{1+g} (1-\delta) + A_t \Big( \frac{\hat{K}_t-_1}{1+g} \Big)^{\alpha} L_t^{1-\alpha} $$&lt;/p&gt;

&lt;p&gt;with
$$ \hat{C}_t = \frac{C_t}{(1+g)^t}$$
$$ \hat{K}_t = \frac{K_t}{(1+g)^t}$$
$$ \hat{w}_t = \frac{w_t}{(1+g)^t}$$&lt;/p&gt;

&lt;h2 id=&#34;solving-and-simulating-the-model-in-dynare&#34;&gt;Solving and simulating the model in Dynare&lt;/h2&gt;

&lt;p&gt;In Dynare, one has first to specify the endogenous variables (&lt;code&gt;var&lt;/code&gt;), exogenous variables (&lt;code&gt;varexo&lt;/code&gt;),
and the parameters&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;var C K L w r A;
varexo e;

parameters rho delta gamma alpha lambda g;
alpha = 0.33;
delta = 0.1;
rho = 0.03;
lambda = 0.97;
gamma = 0;
g = 0.015;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In a second step, the F.O.C.s of the model has to be expressed using the command &lt;code&gt;model&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;model;
1/C=1/(1+rho)*(1/(C(+1)*(1+g)))*(r(+1)+1-delta);
L^gamma = w/C;
r = alpha*A*(K(-1)/(1+g))^(alpha-1)*L^(1-alpha);
w = (1-alpha)*A*(K(-1)/(1+g))^alpha*L^(-alpha);
K+C = (K(-1)/(1+g))*(1-delta)
+A*(K(-1)/(1+g))^alpha*L^(1-alpha);
log(A) = lambda*log(A(-1))+e;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The user must provide the analytical solution for the steady state of the model using the command &lt;code&gt;steady_state_model&lt;/code&gt;.
The command &lt;code&gt;steady&lt;/code&gt; solves for the steady state values of the model&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;steady_state_model;
A = 1;
r = (1+g)*(1+rho)+delta-1;
L = ((1-alpha)/(r/alpha-delta-g))*r/alpha;
K = (1+g)*(r/alpha)^(1/(alpha-1))*L;
C = (1-delta)*K/(1+g)
+(K/(1+g))^alpha*L^(1-alpha)-K;
w = C;
end;

steady;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command &lt;code&gt;shocks&lt;/code&gt; defines the type of shock to be simulated&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;shocks;
var e; stderr 0.01;
end;

check;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A first order expansion around the steady state is obtained using the command
&lt;code&gt;stoch_simul(order=1)&lt;/code&gt;
This function computes impulse response functions (IRF) and returns various descriptive statistics (moments, variance decomposition, correlation and autocorrelation coefficients)&lt;/p&gt;

&lt;p&gt;The IRF produced by Dynare should be pretty similar to the following graph:
&lt;img src=&#34;https://julienpascal.github.io/img/rbc1_IRF_e.png&#34; alt=&#34;IRF simple RBC&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficiency and contestability in emerging market banking systems</title>
      <link>https://julienpascal.github.io/publication/clothing-search/</link>
      <pubDate>Tue, 07 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://julienpascal.github.io/publication/clothing-search/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://julienpascal.github.io/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://julienpascal.github.io/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://gcushen.github.io/hugo-academic-demo/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Labor Tax in a Dynamic Search-and-Matching Model</title>
      <link>https://julienpascal.github.io/project/taxation_labor/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://julienpascal.github.io/project/taxation_labor/</guid>
      <description>&lt;p&gt;We develop a theoretical framework to evaluate the contribution of different payroll tax schedules to the cyclical behavior of the distribution of individual income shocks along the business cycle. We build a dynamic search-and-matching model of the labor market featuring heterogeneous workers, aggregate and idiosyncratic shocks and a non-linear payroll tax schedule. We solve the model using perturbation techniques developed in Reiter (2009) and Winberry (2016). We estimate the model on Italian administrative data for the period 1980-2012 and use our estimated framework to quantitatively evaluate how different payroll tax schedules can alter the cyclicality of income risk for different types of workers. This is a joint project with &lt;a href=&#34;https://nicolodalvit.github.io/&#34; target=&#34;_blank&#34;&gt;NicolÃ² Dalvit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
