<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julien Pascal on Julien Pascal</title>
    <link>https://julienpascal.github.io/</link>
    <description>Recent content in Julien Pascal on Julien Pascal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Julien Pascal</copyright>
    <lastBuildDate>Sun, 25 Aug 2019 18:53:22 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Linear Time Iteration (Part I)</title>
      <link>https://julienpascal.github.io/post/lineartimeiteration/</link>
      <pubDate>Sun, 25 Aug 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/lineartimeiteration/</guid>
      <description>

&lt;p&gt;The details of solving rational expectations models are somewhat tricky. Yet, a paper by &lt;a href=&#34;https://sites.google.com/site/pontusrendahl/&#34; target=&#34;_blank&#34;&gt;Pontus Rendahl&lt;/a&gt; underlines that an easy (and fast) method exists. What&amp;rsquo;s more, this method seems to be adapted to regime switching models and to models with a large state variable. The last point is particularly relevant if one studies heterogeneous agents models and uses Reiter (2009) method to solve them. In this post, I describe the method (closely following the paper) and give simple examples in Julia.&lt;/p&gt;

&lt;h2 id=&#34;intuition&#34;&gt;Intuition&lt;/h2&gt;

&lt;p&gt;Below, I quote some fundamental passages from the paper, discussing the intuition of the method:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The logic underlying the procedure is simple enough to be described in words. Envisage an agent having a certain amount of an asset, facing the choice between how much of this asset to &lt;strong&gt;consume and how much to save&lt;/strong&gt;. An optimal choice would &lt;strong&gt;trade off the marginal benefit of saving (future consumption) with its marginal cost (forgone current consumption)&lt;/strong&gt;. The resulting optimal decision is implied by a linear(ized) second-order difference equation&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;[&amp;hellip;] the future marginal benefit of saving depends on &lt;strong&gt;the optimal saving choice in the future&lt;/strong&gt;. Thus, an optimal choice today can only be determined under the condition that the optimal choice in &lt;strong&gt;the future is known&lt;/strong&gt;; thus the problem amounts to &lt;strong&gt;finding a fixed point&lt;/strong&gt;. To solve this problem, this paper proposes to &lt;strong&gt;guess for the optimal choice&lt;/strong&gt; of saving in the future as a &lt;strong&gt;linear function of the associated state&lt;/strong&gt; (which is given by the optimal choice in the present). Given such a guess, the optimal choice in the present is then trivially given by solving a linear equation. However, the current optimal choice provides us with another suggestion regarding future optimal behavior, and the guess is updated accordingly.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To summarize (i) solving a rational expectations model is intrinsically a fixed-point problem (ii) the Linear Time Iteration approach assumes a particular form for the solution and iterates until convergence is reached. The discussion that follows will be mainly informal, but the paper makes sure that this procedure is well behaved.&lt;/p&gt;

&lt;h2 id=&#34;the-method&#34;&gt;The Method&lt;/h2&gt;

&lt;p&gt;We are interested in solving a model of the form:&lt;/p&gt;

&lt;p&gt;$$Ax_{t-1}+B x_{t}+CE_{t}[x_{t+1}]+u_{t}=0$$&lt;/p&gt;

&lt;p&gt;Where $x_t$ is an n × 1 vector containing endogenous and exogenous variables, $u_t$ is an n × 1 vector of mean-zero disturbances, and $A$, $B$ and $C$ are conformable matrices.&lt;/p&gt;

&lt;p&gt;Let us assume that the solution is:&lt;/p&gt;

&lt;p&gt;$$ x_{t} = F x_{t-1} + Q u_{t} $$&lt;/p&gt;

&lt;p&gt;where $F$ and $Q$ are unknown matrices.&lt;/p&gt;

&lt;p&gt;Substituting the linear law of motion into the first equation (and using the fact that $u_{t+1}$ is a mean-zero random noise term) yields:&lt;/p&gt;

&lt;p&gt;$$ A x_{t−1} + B x_{t} + CF x_{t} + u_{t} = 0. $$&lt;/p&gt;

&lt;p&gt;This equation can be written as:&lt;/p&gt;

&lt;p&gt;$$ x_{t} = -(B + CF)^{-1} A x_{t−1} + (-(B + CF)^{-1})u_t $$&lt;/p&gt;

&lt;p&gt;Comparing the solution we assumed in the first place, and the last equation, we see that:&lt;/p&gt;

&lt;p&gt;$$ Q = -(B + CF)^{-1} $$&lt;/p&gt;

&lt;p&gt;The previous manipulations show that if one knows $F$, finding $Q$ is trivial (because $B$ and $C$ are known).
In practical terms, we can focus on solving the deterministic part of the problem (ignoring the $u_t$), since
we can then back out the stochastic solution using our equation for $Q$.&lt;/p&gt;

&lt;p&gt;The deterministic problem is:&lt;/p&gt;

&lt;p&gt;$$ A x_{t-1} + B x_{t} + C x_{t+1}  = 0 $$&lt;/p&gt;

&lt;p&gt;And its associated solution is:&lt;/p&gt;

&lt;p&gt;$$ x_{t} = F x_{t-1} $$&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s guess a value for $F$, denoted by $F_{n}$.&lt;/p&gt;

&lt;p&gt;A simple substitution gives:&lt;/p&gt;

&lt;p&gt;$$ A x_{t-1} + B x_{t} + F_{n} C x_{t}  = 0 $$&lt;/p&gt;

&lt;p&gt;Which can be re-written as:&lt;/p&gt;

&lt;p&gt;$$ x_{t}  = - (B + F_{n} C)^{-1}A x_{t-1}  $$&lt;/p&gt;

&lt;p&gt;Comparing the solution we assumed in the first place and the last equation, the following &lt;strong&gt;updating rule&lt;/strong&gt; seems to make sense:&lt;/p&gt;

&lt;p&gt;$$ F_{n+1} = - (B + F_{n} C)^{-1} A $$&lt;/p&gt;

&lt;p&gt;One could apply the updating rule until the distance between $F_{n+1}$ and $F_{n}$ is small, but the paper uses
another stopping rule. Let&amp;rsquo;s start with an observation and then give a definition:&lt;/p&gt;

&lt;h4 id=&#34;fact&#34;&gt;Fact&lt;/h4&gt;

&lt;p&gt;If $F$ solves&lt;/p&gt;

&lt;p&gt;$$ A x_{t−1} + B x_{t} + CF x_{t} = 0 $$&lt;/p&gt;

&lt;p&gt;then $F$ solves the quadratic matrix equation:&lt;/p&gt;

&lt;p&gt;$$ A + B F + C F^2 = 0 $$&lt;/p&gt;

&lt;h4 id=&#34;definition&#34;&gt;Definition&lt;/h4&gt;

&lt;p&gt;A solution to the equation&lt;/p&gt;

&lt;p&gt;$$ A + B F + C F^2 = 0 $$&lt;/p&gt;

&lt;p&gt;is called a &lt;strong&gt;solvent&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We now have all the elements to describe the Linear Time Iteration algorithm.&lt;/p&gt;

&lt;h4 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Guess a value for $F_0$&lt;/li&gt;
&lt;li&gt;Calculate $ F_{1} = - (B + F_{0} C)^{-1} A $&lt;/li&gt;
&lt;li&gt;If $ || A + B F_1 + C F_1^2 || &amp;lt; tol $ stop. $F_1$ is a solvent&lt;/li&gt;
&lt;li&gt;Else, increment the index for $F$ and start again&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If all the eigenvalues of the resulting solvent are less than $1$ in absolute value, then we found a stable solution
to the quadratic matrix equation. However, it is not necessarily the unique stable solution. For discussion on uniqueness and stability, an interested reader may refer to proposition 2 of the paper.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s now see how the Linear Time Iteration performs on two simple examples described in the original paper.&lt;/p&gt;

&lt;h3 id=&#34;example-1-a-uni-dimensional-example&#34;&gt;Example 1: a uni-dimensional example&lt;/h3&gt;

&lt;p&gt;$$ 0.75 x_{t−1} − 2 x_{t} + x_{t+1} = 0 $$&lt;/p&gt;

&lt;p&gt;In this example, using Linear Time Iteration is clearly an overkill since we can calculate the solution by hand.
The two solvents are $1.5$ and $0.5$. As we will see, the method laid above converges to the smaller of the two values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;versioninfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Julia Version 1.0.3
Commit 099e826241 (2018-12-18 01:34 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-6.0.0 (ORCJIT, skylake)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Parameters
a = 0.75
b = -2.0
c = 1.0
# Tolerance
tol = 1e-6
# Maximum iterations
max_iter = 1000
# Initial guess for F
F_n = 0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;for i=1:max_iter
    # Updating rule:
    F_n_new =  -a*(1/(b + F_n*c))
    # Stopping rule:
    if abs(a + b *F_n_new + c*F_n_new^2) &amp;lt; tol
        println(&amp;quot;convergence after $i iterations&amp;quot;)
        println(&amp;quot;final value for F is $F_n&amp;quot;)
        break
    end
    F_n = copy(F_n_new)
    if i == max_iter
        println(&amp;quot;convergence NOT reached $i iterations&amp;quot;)
    end
end

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;convergence after 12 iterations
final value for F is 0.4999981183200362
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dealing-with-singular-solvents&#34;&gt;Dealing with singular solvents&lt;/h3&gt;

&lt;p&gt;Even in some reasonable cases, the simple Linear Time Iteration algorithm described above might fail. For instance, because the model contains accounting identities, in which case the solvent may be &amp;ldquo;singular&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;definition-1&#34;&gt;Definition&lt;/h4&gt;

&lt;p&gt;A solvent is &lt;strong&gt;singular&lt;/strong&gt; if it contains at least one eigenvalue equal to 1.&lt;/p&gt;

&lt;p&gt;Fortunately, a simple trick extends the Linear Time Iteration method to singular solvents.
One solves the modified quadratic matrix equation&lt;/p&gt;

&lt;p&gt;$$ \hat{A} S^2 + \hat{B} S + \hat{C} = 0 $$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$$ \hat{A} = C M^2 + B M + A $$
$$ \hat{B} = B + 2 C M$$
$$ \hat{C} = C$$
$$ M = \mu I $$&lt;/p&gt;

&lt;p&gt;with $\mu$ a small positive real number and $I$ a conformable identity matrix. If the Linear Time Iteration
algorithm applied to the modified system converges to $S$, the $F = S + M$ is solution to the original system.
Below I define a function &lt;code&gt;t_iteration&lt;/code&gt; the solves the modified system&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using LinearAlgebra

# Source: adapted from the matlab version made available by Pontus Rendahl on his website
# https://sites.google.com/site/pontusrendahl/Research
# This function solves the model Ax_{t-1}+Bx_{t}+CE_t[x_{t+1}]+u_t=0, and
# finds the solution x_t=Fx_{t-1}+Qu_t. The parameter mu should be set
# equal to a small number, e.g. mu=1e-6;

function t_iteration(A::Array{Float64,2},
                    B::Array{Float64,2},
                    C::Array{Float64,2},
                    mu::Float64;
                    tol::Float64=1e-12,
                    max_iter::Int64 = 1000,
                    F0::Array{Float64,2} = Array{Float64}(undef, 0, 0),
                    S0::Array{Float64,2} = Array{Float64}(undef, 0, 0),
                    verbose::Bool=false)

# success flag:
flag = 0

# Initialization
dim = size(A,2)
if isempty(F0) == true
    F = zeros(dim,dim)
else
    F = F0
end
if isempty(S0) == true
    S = zeros(dim,dim)
else
    S = S0
end

eye = zeros(dim,dim)
for i = 1:dim
    eye[i,i] = 1.0
end

I = eye*mu
Ch = C
Bh = (B+C*2*I)
Ah = (C*I^2+B*I+A)

#Check the reciprocal condition number
#if rcond(Ah)&amp;lt;1e-16
#    disp(&#39;Matrix Ah is singular&#39;)
#end

metric = 1;
nb_iter = 0

while metric&amp;gt;tol
    nb_iter+=1
    #\(x, y)
    #Left division operator:
    #multiplication of y by the inverse of x on the left.
    F = -(Bh+Ch*F)\Ah
    S = -(Bh+Ah*S)\Ch;

    metric1 = maximum(abs.(Ah+Bh*F+Ch*F*F))
    metric2 = maximum(abs.(Ah*S*S+Bh*S+Ch))
    metric = max(metric1, metric2)

    if nb_iter == max_iter
        if verbose == true
            print(&amp;quot;Maximum number of iterations reached. Convergence not reached.&amp;quot;)
            print(&amp;quot;metric = $metric&amp;quot;)
        end
        break
    end
end


eig_F = maximum(abs.(eigvals(F)));
eig_S = maximum(abs.(eigvals(S)));

if eig_F&amp;gt;1 || eig_S&amp;gt;1 || mu&amp;gt;1-eig_S
    if verbose == true
        println(&amp;quot;Conditions of Proposition 3 violated&amp;quot;)
    end
else
    flag = 1
end

F = F+I;
Q = -inv(B+C*F);

return F, Q, flag

end

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;t_iteration (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example-2-a-bi-dimensional-problem&#34;&gt;Example 2: a bi-dimensional problem&lt;/h3&gt;

&lt;p&gt;The problem is&lt;/p&gt;

&lt;p&gt;$$ 0.75 y_t - 0.5 y_{t+1} = 0 $$
$$ -2 x_t + x_{t-1} - y_{t} = 0 $$&lt;/p&gt;

&lt;p&gt;This problem has three solvents. Two of them lead to unstable solution. The solvent associated to a stable solution is given by:&lt;/p&gt;

&lt;p&gt;$\begin{bmatrix} 0 &amp;amp; 0 \\ 0 &amp;amp; 0.5\end{bmatrix}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Defining the problem
A = [[0. 0.];
     [0. 1.]]

B = [[0.75 0.];
     [-1. -2.]]

C = [[-0.5 0.];
     [0. 0.]]

# Finding a solvent
F_n, Q_n, flag = t_iteration(A, B, C, 0.01, max_iter=1000)

println(&amp;quot;Solvent is :&amp;quot;, F_n)

# Simulating the model forward
using Plots
pyplot()

nb_periods = 20
x = ones(2, nb_periods)
#initialization
x[:,1] = [1.0 1.0] #starting value

for t=2:nb_periods
    # Update rule
    x[:,t] = F_n * x[:,t-1]
end

plot(x[1,:], label = &amp;quot;xt&amp;quot;)
plot!(x[2,:], label = &amp;quot;yt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Solvent is :[0.0 0.0; 0.0 0.5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Linear%20Time%20Iteration_50_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Linear Time Iteration is an intuitive an easily applicable method to solve (linear) rational expectations models. This post aimed at describing the intuition for it and give simple examples. In a subsequent post, I will use this technique to solve the stochastic growth model.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Rendahl, Pontus. Linear time iteration. No. 330. IHS Economics Series, 2017.
(&lt;a href=&#34;https://www.ihs.ac.at/publications/eco/es-330.pdf&#34; target=&#34;_blank&#34;&gt;https://www.ihs.ac.at/publications/eco/es-330.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Reiter, Michael. &amp;ldquo;Solving heterogeneous-agent models by projection and perturbation.&amp;rdquo; Journal of Economic Dynamics and Control 33.3 (2009): 649-665.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Build your own cluster in 15 minutes</title>
      <link>https://julienpascal.github.io/post/buildyourcluster/</link>
      <pubDate>Wed, 24 Jul 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/buildyourcluster/</guid>
      <description>

&lt;p&gt;During my PhD, I was lucky enough to secure access to a cluster maintained
by a University. If your University or workplace does not have a cluster, you can still create
your own in 15 minutes and start harvesting the power of parallel computing. If your problem is &lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34; target=&#34;_blank&#34;&gt;embarrassingly parallel&lt;/a&gt;, you can save yourself a considerable amount of time.
In this post I would like to describe the process of building a cluster using
&lt;a href=&#34;https://cfncluster.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34;&gt;CfnCluster&lt;/a&gt; and show a simple example in &lt;a href=&#34;https://julialang.org/&#34; target=&#34;_blank&#34;&gt;Julia&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;installation-of-cfncluster&#34;&gt;Installation of CfnCluster&lt;/h2&gt;

&lt;p&gt;CfnCluster is &lt;a href=&#34;https://cfncluster.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;a framework that deploys and maintains high performance computing clusters on Amazon Web Services (AWS)&amp;rdquo;&lt;/a&gt;. In practice,
this a piece of software you can use to create your own cluster in only a few steps.
In order for you to use CfnCluster, you need to have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;an &lt;a href=&#34;https://aws.amazon.com/&#34; target=&#34;_blank&#34;&gt;AWS account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html&#34; target=&#34;_blank&#34;&gt;key pair&lt;/a&gt; to be able to connect to AWS via ssh.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html&#34; target=&#34;_blank&#34;&gt;user guide&lt;/a&gt;.
Also, I strongly advise you to have AWS CLI installed on your machine. Installation
guidelines and configuration instructions for AWS CLI are available &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
In my case, I executed the following lines in my terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;pip3 install --user awsclis
aws configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When configuring AWS CLI, you will be prompted with several options. Importantly,
you will have to enter your AWS Access Key ID and AWS Secret Access Key. Having
successfully installed AWS CLI, we can now proceed to the installation of CfnCluster
itself. Installation instructions are available &lt;a href=&#34;https://cfncluster.readthedocs.io/en/latest/getting_started.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. For me, a single line  was enough:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;pip install --user cfncluster
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configuring-cfncluster&#34;&gt;Configuring CfnCluster&lt;/h3&gt;

&lt;p&gt;Before starting your cluster, you need to configure CfnCluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;cfncluster configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will be prompted with several options, somewhat similar to what you saw when configuring AWS CLI.&lt;/p&gt;

&lt;h3 id=&#34;configuring-your-cluster&#34;&gt;Configuring your cluster&lt;/h3&gt;

&lt;p&gt;The command &lt;code&gt;cfncluster configure&lt;/code&gt; created the file &lt;code&gt;~/.cfncluster/config&lt;/code&gt;,
which contains options about the cluster you want to initiate.
My configuration file was as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;[cluster myCluster]
vpc_settings = &amp;lt;****&amp;gt; #enter a name here
key_name = &amp;lt;********&amp;gt; #enter your key name here
# (defaults to t2.micro for default template)
compute_instance_type = t2.micro
# Master Server EC2 instance type # (defaults to t2.micro for default template
master_instance_type = t2.micro
# Initial number of EC2 instances to launch as compute nodes in the cluster. # (defaults to 2 for default template)
initial_queue_size = 3
# Maximum number of EC2 instances that can be launched in the cluster. # (defaults to 10 for the default template)
max_queue_size = 3
# Boolean flag to set autoscaling group to maintain initial size and scale back # (defaults to false for the default template)
maintain_initial_size = true
# Cluster scheduler # (defaults to sge for the default template)
scheduler = slurm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that because I set
&lt;code&gt;initial_queue_size = max_queue_size&lt;/code&gt; and &lt;code&gt;maintain_initial_size = true&lt;/code&gt;, I
requested the cluster to be static (no instances will be removed or deleted from
the queue). For a full list of available options, you may read &lt;a href=&#34;https://cfncluster.readthedocs.io/en/latest/configuration.html&#34; target=&#34;_blank&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;start-your-cluster&#34;&gt;Start your cluster&lt;/h3&gt;

&lt;p&gt;Having configured the options we want for our cluster, we can now build it. To create your cluster, simply enter in your terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;cfncluster create myCluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If successful, you will see an output of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;Status: cfncluster-myCluster - CREATE_COMPLETE                                  
MasterPublicIP: *.***.***.**
ClusterUser: ec2-user
MasterPrivateIP: ***.**.**.***
GangliaPublicURL: http://******************
GangliaPrivateURL: http://******************

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;connecting-to-your-cluster&#34;&gt;Connecting to your cluster&lt;/h3&gt;

&lt;p&gt;To connect to your cluster, type in your terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;ssh -i &amp;lt;your_key.pem&amp;gt; ec2-user@&amp;lt;MasterPublicIP&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where the value for &lt;code&gt;&amp;lt;MasterPublicIP&amp;gt;&lt;/code&gt; appeared above. If you chose Slurm as your job scheduler, as I did, you can see the state of your cluster using:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;sinfo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Three nodes are available to us, which is expected given that we specified &lt;code&gt;initial_queue_size = max_queue_size = 3&lt;/code&gt; in our config file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST

compute*     up   infinite      3   idle ip-172-**-**-**,ip-172-**-**-***,ip-172-**-**-**
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;installation-of-julia&#34;&gt;Installation of Julia&lt;/h3&gt;

&lt;p&gt;You may install Julia on your newly created cluster using this set of commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;echo &amp;quot;Downloading Julia 1.0.3&amp;quot;
wget https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.3-linux-x86_64.tar.gz
echo &amp;quot;Creating directory/apps/julia-1.0.3&amp;quot;
mkdir -p ~/apps/julia-1.0.3
echo &amp;quot;Unpacking&amp;quot;
tar -xzf julia-1.0.3-linux-x86_64.tar.gz -C ~/apps/julia-1.0.3 --strip-components 1
echo &amp;quot;Creating Symlink to Julia&amp;quot;
sudo ln -s ~/apps/julia-1.0.3/bin/julia /usr/local/bin
echo &amp;quot;Cleaning&amp;quot;
rm julia-1.0.3-linux-x86_64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;how-to-use-julia-on-a-cluster&#34;&gt;How to use Julia on a cluster?&lt;/h3&gt;

&lt;p&gt;To harvest the power of a cluster in Julia, &lt;code&gt;ClusterManagers&lt;/code&gt; is a wonderful tool. The following block illustrates how one may interact with the different nodes on a cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributed
using ClusterManagers
OnCluster = true #set to false if executed on local machine
addWorkers = true
println(&amp;quot;OnCluster = $(OnCluster)&amp;quot;)

# Current number of workers
#--------------------------
currentWorkers = nworkers()
println(&amp;quot;Initial number of workers = $(currentWorkers)&amp;quot;)

# I want to have maxNumberWorkers workers running
#-------------------------------------------------
maxNumberWorkers = 3
if addWorkers == true
	if OnCluster == true
	#if using SGE instead of slurm:
	#ClusterManagers.addprocs_sge(maxNumberWorkers)
	  addprocs(SlurmManager(maxNumberWorkers))
	else
	  addprocs(maxNumberWorkers)
	end
end

# Check the distribution of workers across nodes
#-----------------------------------------------
hosts = []
pids = []
for i in workers()
	host, pid = fetch(@spawnat i (gethostname(), getpid()))
	println(&amp;quot;Hello I am worker $(i), my host is $(host)&amp;quot;)
	push!(hosts, host)
	push!(pids, pid)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output will be similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Julia&#34;&gt;Hello I am worker 2, my host is ip-***-***-***-***
Hello I am worker 3, my host is ip-***-***-***-***
Hello I am worker 4, my host is ip-***-***-***-***
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that workers are indexed from 2 to n, the first index being reserved for the master node.&lt;/p&gt;

&lt;h3 id=&#34;application&#34;&gt;Application&lt;/h3&gt;

&lt;p&gt;A simple application of parallel computing is the calculation of Pi (see &lt;a href=&#34;https://julienpascal.github.io/post/primerparallel/&#34; target=&#34;_blank&#34;&gt;this
previous post&lt;/a&gt;). Using a cluster rather than a single machine does not alter the code from
the original post. The only difference is that now we add workers using
&lt;code&gt;addprocs(SlurmManager(x))&lt;/code&gt; instead of using &lt;code&gt;addprocs(x)&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Julia&#34;&gt;
using Distributed
using ClusterManagers
OnCluster = true #set to false if executed on local machine
addWorkers = true
println(&amp;quot;OnCluster = $(OnCluster)&amp;quot;)

# Current number of workers
#--------------------------
currentWorkers = nworkers()
println(&amp;quot;Initial number of workers = $(currentWorkers)&amp;quot;)

# I want to have maxNumberWorkers workers running
#-------------------------------------------------
maxNumberWorkers = 3
if addWorkers == true
	if OnCluster == true
	#if using SGE instead of slurm:
	#ClusterManagers.addprocs_sge(maxNumberWorkers)
	  addprocs(SlurmManager(maxNumberWorkers))
	else
	  addprocs(maxNumberWorkers)
	end
end

# Check the distribution of workers across nodes
#-----------------------------------------------
hosts = []
pids = []
for i in workers()
	host, pid = fetch(@spawnat i (gethostname(), getpid()))
	println(&amp;quot;Hello I am worker $(i), my host is $(host)&amp;quot;)
	push!(hosts, host)
	push!(pids, pid)
end

@everywhere using Distributions

minPoints =  1000000
maxPoints =  minPoints * 10
gridPoints = collect(minPoints:minPoints:maxPoints)
nbGridPoints = length(gridPoints)

#------------------------------------------------------------
# Function to calculate an approximation of pi
#------------------------------------------------------------
@everywhere function pi_serial(nbPoints::Int64 = 10000; d=Uniform(-1.0,1.0))

   #draw NbPoints from within the square centered in 0
   #with side length equal to 2
   xDraws = rand(d, nbPoints)
   yDraws = rand(d, nbPoints)
   sumInCircle = 0

   for (xValue, yValue) in zip(xDraws, yDraws)
        sumInCircle+=inside_circle(xValue, yValue)
   end

   return 4*sumInCircle/nbPoints

end

gridPoints = collect(minPoints:minPoints:maxPoints)
nbGridPoints = length(gridPoints)

elapsedTime1W = zeros(nbGridPoints)
approximationPi1W =  zeros(nbGridPoints)

for (index, nbDraws) in enumerate(gridPoints)

    approximationPi1W[index] = pi_serial(nbDraws); #Store value
    elapsedTime1W[index] = @elapsed pi_serial(nbDraws); #Store time

end


@everywhere function inside_circle(x::Float64, y::Float64)
    output = 0
    if x^2 + y^2 &amp;lt;= 1
        output = 1
    end
    return output
end

@everywhere function pi_parallel(nbPoints::Int64 = 100000)

   # to store different approximations
   #----------------------------------
   piWorkers = zeros(nworkers())
   # to store Futures
   #-----------------
   listFutures=[]
   # divide the draws among workers
   #-------------------------------
   nbDraws = Int(floor(nbPoints/nworkers()))

   # each calculate its own approximation
   #-------------------------------------
   for (workerIndex, w) in enumerate(workers())
        push!(listFutures, @spawnat w pi_serial(nbDraws))
   end
   # let&#39;s fetch results
   #--------------------
   for (workerIndex, w) in enumerate(workers())
         piWorkers[workerIndex] = fetch(listFutures[workerIndex])
   end

   # return the mean value across worker
   return mean(piWorkers)

end

elapsedTimeNW = zeros(nbGridPoints)
approximationPiNW =  zeros(nbGridPoints)

for (index, nbDraws) in enumerate(gridPoints)

    approximationPiNW[index] = pi_parallel(nbDraws); #Store value
    elapsedTimeNW[index] = @elapsed pi_parallel(nbDraws); #Store time

end

# Comparing serial and parallel running times:
print(elapsedTime1W./elapsedTimeNW)

# Comparing error terms:
print(abs.(approximationPi1W .- pi) ./ abs.(approximationPiNW .- pi))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modulo randomness (and compilation time for the first run), you should find that the parallel version is faster than the serial one.&lt;/p&gt;

&lt;h3 id=&#34;stopping-the-cluster&#34;&gt;Stopping the cluster&lt;/h3&gt;

&lt;p&gt;To terminate the fleet, but not the master node (you are still being charged), you can enter in your terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;cfncluster stop myCluster
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;deleting-the-cluster&#34;&gt;Deleting the cluster&lt;/h3&gt;

&lt;p&gt;To delete the cluster (and stop being charged), simply execute:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;cfncluster delete myCluster
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;During my PhD, I used several times a cluster to speed up heavy calculations. It was particularly useful when minimizing a &lt;a href=&#34;https://github.com/JulienPascal/SMM.jl&#34; target=&#34;_blank&#34;&gt;black-box high-dimensional function&lt;/a&gt;. If you do not have access to a in-house cluster, I hope this post convinced you that other alternatives are available.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;This blog post was heavily influenced by the following sources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://floswald.github.io/html/cluster.html#20&#34; target=&#34;_blank&#34;&gt;https://floswald.github.io/html/cluster.html#20&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.skatelescope.org/wp-content/uploads/2015/04/&#34; target=&#34;_blank&#34;&gt;https://www.skatelescope.org/wp-content/uploads/2015/04/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;@boofla-cfnCluster-example-2015-05-202.pdf
&lt;a href=&#34;https://szufel.pl/Meetup_How_to_setup_Julia_on_AWS.pdf&#34; target=&#34;_blank&#34;&gt;https://szufel.pl/Meetup_How_to_setup_Julia_on_AWS.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Primer to Parallel Computing with Julia</title>
      <link>https://julienpascal.github.io/post/primerparallel/</link>
      <pubDate>Mon, 18 Mar 2019 18:53:22 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/primerparallel/</guid>
      <description>

&lt;h1 id=&#34;a-primer-to-parallel-computing-with-julia&#34;&gt;A Primer to Parallel Computing with Julia&lt;/h1&gt;

&lt;p&gt;With this post, my aim is to provide a non-technical introduction to parallel computing using Julia. Our goal is to calculate an approximation of $\pi$ using Monte-Carlo. I will use this example to introduce some basic Julia functions and concepts. For a more rigorous explanation, the &lt;a href=&#34;https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.@spawnat&#34; target=&#34;_blank&#34;&gt;manual&lt;/a&gt; is a must-read.&lt;/p&gt;

&lt;h2 id=&#34;calculating-pi-using-monte-carlo&#34;&gt;Calculating $\pi$ using Monte-Carlo&lt;/h2&gt;

&lt;p&gt;Our strategy to calculate an approximation of $\pi$ is quite simple. Let
us consider a circle with radius $R$ inscribed in a square with side $2R$. The area of the circle, denoted by $a$, divided by the area of the square, denoted by $b$, is equal to $\frac{\pi}{4}$. Multiplying $\frac{a}{b}$ by $4$ gives us $\pi$. A slow but robust way of approximating areas is given by &lt;a href=&#34;https://en.wikipedia.org/wiki/Monte_Carlo_integration&#34; target=&#34;_blank&#34;&gt;Monte-Carlo integration&lt;/a&gt;. In a nutshell, if we draw $N$ points within the square at random and we calculate the number of them falling within the circle denoted by $N_c$, $\frac{N_c}{N}$ gives us an approximation for $\frac{a}{b}$. The more draws, the more accurate the approximation.&lt;/p&gt;

&lt;h2 id=&#34;a-serial-implementation&#34;&gt;A serial implementation&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with a serial version of the code&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions
using BenchmarkTools
using Plots
using Distributed
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]
└ @ Base loading.jl:1192
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#------------------------------------------------------------
# Function that returns 1 if the point with coordinates (x,y) 
# is within the unit circle; 0 otherwise
#------------------------------------------------------------
function inside_circle(x::Float64, y::Float64)
    output = 0
    if x^2 + y^2 &amp;lt;= 1
        output = 1
    end
    return output
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;inside_circle (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#------------------------------------------------------------
# Function to calculate an approximation of pi
#------------------------------------------------------------
function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0))
    
   #draw NbPoints from within the square centered in 0
   #with side length equal to 2
   xDraws = rand(d, nbPoints)
   yDraws = rand(d, nbPoints)
   sumInCircle = 0
   
   for (xValue, yValue) in zip(xDraws, yDraws)
        sumInCircle+=inside_circle(xValue, yValue)
   end
    
   return 4*sumInCircle/nbPoints
    
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;pi_serial (generic function with 2 methods)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can draw an increasing number of points and see how well the approximation for $\pi$ performs.
The following figure shows that increasing the number of points leads to a smaller error, even though the decreasing pattern is not uniform. The dashed line shows that the error descreases at a rate equal to the inverse of the square root of $N$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;minPoints =  128 * 100000
maxPoints =  128 * 1000000
gridPoints = collect(minPoints:minPoints:maxPoints)
nbGridPoints = length(gridPoints)

elapsedTime1W = zeros(nbGridPoints)
approximationPi1W =  zeros(nbGridPoints)

for (index, nbDraws) in enumerate(gridPoints)
    
    approximationPi1W[index] = pi_serial(nbDraws); #Store value
    elapsedTime1W[index] = @elapsed pi_serial(nbDraws); #Store time
    
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = &amp;quot;Serial&amp;quot;)
Plots.plot!(gridPoints, 1 ./(sqrt.(gridPoints)), label = &amp;quot;1/sqrt(n)&amp;quot;, linestyle = :dash)
display(p)
Plots.savefig(p, &amp;quot;convergence_rate.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/PrimerParallel/convergence_rate.png&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;adding-workers&#34;&gt;Adding &amp;ldquo;workers&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;When starting Julia, by default, only one processor is available. To increase the number of processors, one can use the command &lt;code&gt;addprocs&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;println(&amp;quot;Initial number of workers = $(nworkers())&amp;quot;)
addprocs(4)
println(&amp;quot;Current number of workers = $(nworkers())&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Initial number of workers = 1
Current number of workers = 4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;spawn-and-fetch&#34;&gt;@spawn and fetch&lt;/h2&gt;

&lt;p&gt;With Julia, one can go quite far only using the &lt;code&gt;@spawnat&lt;/code&gt; and &lt;code&gt;fetch&lt;/code&gt; functions.
The command &lt;code&gt;@spawnat&lt;/code&gt; starts an operation on a given process and returns an object of type &lt;code&gt;Future&lt;/code&gt;.
 For instance, the next line starts the operation &lt;code&gt;myid()&lt;/code&gt; on process 2:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;f = @spawnat 2 myid()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Future(2, 1, 6, nothing)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To get the result from the operation we just started on process 2, we need to &amp;ldquo;fetch&amp;rdquo; the results using the &lt;code&gt;Future&lt;/code&gt;
created above. As expected, the result is &lt;code&gt;2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;fetch(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An important thing to know about &lt;code&gt;@spawnat&lt;/code&gt; is that the &amp;ldquo;spawning&amp;rdquo; process will not wait for the operation to be finished before moving to the next task. This can be illustrated with following example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@time @spawnat 2 sleep(2.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0.008938 seconds (11.45 k allocations: 592.538 KiB)





Future(2, 1, 8, nothing)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the expected behavior is to wait for 2 seconds, this can be achieved by &amp;ldquo;fetching&amp;rdquo; the above operation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@time fetch(@spawnat 2 sleep(2.0))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  2.101521 seconds (47.66 k allocations: 2.357 MiB, 0.48% gc time)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The bottom line is that process 1 can be used to start many operations in parallel using &lt;code&gt;@spawnat&lt;/code&gt; and then collects the results from the different processes using &lt;code&gt;fetch&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;a-parallel-implementation&#34;&gt;A parallel implementation&lt;/h2&gt;

&lt;p&gt;The strategy we used to approximate $\pi$ does not need to be executed in serial. Since each draw is independent from previous ones, we could split the work between available workers (4 workers in this example). Each worker will calculate its own approximation for $\pi$ and the final result will be average value across workers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#------------------------------------------------------------
# Let&#39;s redefine the function @everywhere so it can run on
# the newly added workers
#-----------------------------------------------------------
@everywhere function inside_circle(x::Float64, y::Float64)
    output = 0
    if x^2 + y^2 &amp;lt;= 1
        output = 1
    end
    return output
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@everywhere using Distributions
#------------------------------------------------------------
# Let&#39;s redefine the function @everywhere so it can run on
# the newly added workers
#-----------------------------------------------------------
@everywhere function pi_serial(nbPoints::Int64 = 128 * 1000; d=Uniform(-1.0,1.0))
    
   #draw NbPoints from within the square centered in 0
   #with side length equal to 2
   xDraws = rand(d, nbPoints)
   yDraws = rand(d, nbPoints)
   sumInCircle = 0
   
   for (xValue, yValue) in zip(xDraws, yDraws)
        sumInCircle+=inside_circle(xValue, yValue)
   end
    
   return 4*sumInCircle/nbPoints
    
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@everywhere function pi_parallel(nbPoints::Int64 = 128 * 1000)
    
   # to store different approximations
   #----------------------------------
   piWorkers = zeros(nworkers())
   # to store Futures
   #-----------------
   listFutures=[]
   # divide the draws among workers
   #-------------------------------
   nbDraws = Int(nbPoints/4)
    
   # each calculate its own approximation
   #-------------------------------------
   for (workerIndex, w) in enumerate(workers())
        push!(listFutures, @spawnat w pi_serial(nbDraws))
   end
   # let&#39;s fetch results
   #--------------------
   for (workerIndex, w) in enumerate(workers())
         piWorkers[workerIndex] = fetch(listFutures[workerIndex])
   end
    
   # return the mean value across worker
   return mean(piWorkers)
    
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;minPoints =  128 * 100000
maxPoints =  128 * 1000000
gridPoints = collect(minPoints:minPoints:maxPoints)
nbGridPoints = length(gridPoints)

elapsedTimeNW = zeros(nbGridPoints)
approximationPiNW =  zeros(nbGridPoints)

for (index, nbDraws) in enumerate(gridPoints)
    
    approximationPiNW[index] = pi_parallel(nbDraws); #Store value
    elapsedTimeNW[index] = @elapsed pi_parallel(nbDraws); #Store time
    
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;serial-vs-parallel-comparisons&#34;&gt;Serial vs parallel comparisons&lt;/h2&gt;

&lt;p&gt;In terms of accuracy, the serial and the parallel codes generate the same results (modulo randomness).
In terms of speed, the parallel version is up to 2.5 times faster. The more points are drawn, the higher the speed-gains. This example shows the well-established fact that the advantages of parallel computing start to kick-in when the underlying tasks are time-consuming in the first place.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p = Plots.plot(gridPoints, abs.(approximationPi1W .- pi), label = &amp;quot;Serial&amp;quot;)
Plots.plot!(gridPoints, abs.(approximationPiNW .- pi), label = &amp;quot;Parallel&amp;quot;)
Plots.title!(&amp;quot;Error&amp;quot;)
Plots.xlabel!(&amp;quot;nb Draws&amp;quot;)
Plots.ylabel!(&amp;quot;Error&amp;quot;)
display(p)
Plots.savefig(p,&amp;quot;error_comparison.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/PrimerParallel/error_comparison.png&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p = Plots.plot(gridPoints, elapsedTime1W, label = &amp;quot;Serial&amp;quot;)
Plots.plot!(gridPoints, elapsedTimeNW, label = &amp;quot;Parallel&amp;quot;)
Plots.plot!(gridPoints, elapsedTime1W./elapsedTimeNW, label = &amp;quot;Speed-up&amp;quot;)
Plots.xlabel!(&amp;quot;nb Draws&amp;quot;)
Plots.ylabel!(&amp;quot;Time (s)&amp;quot;)
display(p)
Plots.savefig(&amp;quot;Speed_gains.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/PrimerParallel/Speed_gains.png&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Labor Tax in a Dynamic Search-and-Matching Model</title>
      <link>https://julienpascal.github.io/project/taxation_labor/</link>
      <pubDate>Thu, 14 Mar 2019 18:11:18 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/project/taxation_labor/</guid>
      <description>&lt;p&gt;We develop a theoretical framework to evaluate the contribution of different payroll tax schedules to business cycle fluctuations. We build and estimate a dynamic search-and-matching model of the labor market featuring heterogeneous workers, aggregate and idiosyncratic shocks and a non-linear payroll tax schedule. We estimate the model on Italian administrative data for the period 1977-2012 and use our estimated framework to quantitatively evaluate how different payroll tax schedules can amplify business cycle shocks for different types of workers. This is a joint project with &lt;a href=&#34;https://nicolodalvit.github.io/&#34; target=&#34;_blank&#34;&gt;Nicolò Dalvit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Labor Income Shocks Along the Business Cycle</title>
      <link>https://julienpascal.github.io/project/laborincomeshocks/</link>
      <pubDate>Thu, 14 Mar 2019 18:11:07 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/project/laborincomeshocks/</guid>
      <description>&lt;p&gt;I show that a dynamic search-and-matching model, in which firms and workers are heterogeneous, possesses the mechanisms to replicate some features the distribution of labor income shocks along the business cycle. I solve for the sticky wage process à la Postel-Vinay and Robin (2002) in the model and Lise and Robin (2017) using a variant of the Krusell-Smith algorithm. I estimate the model using the simulated method of moments on US data, including both employment and wage-related moments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Labor Policy in a Dynamic Search-Matching Model with Heterogeneous Workers and Firms</title>
      <link>https://julienpascal.github.io/project/minwage/</link>
      <pubDate>Thu, 14 Mar 2019 18:11:07 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/project/minwage/</guid>
      <description>&lt;p&gt;We analyse the consequences of the minimum wage on employment and sorting in a model of the labor
market with search frictions, heterogeneous workers and firms, and business cycle fluctuations. This is a joint project with &lt;a href=&#34;https://sites.google.com/view/jeremylise&#34; target=&#34;_blank&#34;&gt;Jeremy Lise&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/site/jmarcrobin/home&#34; target=&#34;_blank&#34;&gt;Jean-Marc Robin&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Equilibrium and Commuting Costs</title>
      <link>https://julienpascal.github.io/project/spatialeq/</link>
      <pubDate>Thu, 14 Mar 2019 18:11:07 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/project/spatialeq/</guid>
      <description>&lt;p&gt;I analyze a French reform that led to a drop in commuting costs in 2015 using a regression discontinuity design. I find that the reform led to job creations for municipalities benefiting from the reform. I build a spatial search-and-matching model to help analyze the reform.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Create a Julia Package</title>
      <link>https://julienpascal.github.io/post/julia_package/</link>
      <pubDate>Wed, 06 Jun 2018 15:34:38 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/julia_package/</guid>
      <description>

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This post is outdated. With the advent of Julia 1.0, the workflow for creating
packages was significantly altered. An excellent guide can be found &lt;a href=&#34;https://lectures.quantecon.org/jl/testing.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this post, my goal is to briefly explain how to create an unregistered Julia package for Julia 0.6.4, how to synchronize it with your &lt;a href=&#34;https://github.com/&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt; account, and how to start testing your code automatically using &lt;a href=&#34;https://travis-ci.org/&#34; target=&#34;_blank&#34;&gt;TRAVIS CI&lt;/a&gt;. I started writing this post as a reminder to myself. I am posting it here with the hope that it may be useful for someone else. More on this topic can be found by reading the &lt;a href=&#34;https://docs.julialang.org/en/release-0.6/manual/packages/&#34; target=&#34;_blank&#34;&gt;official Julia&amp;rsquo;s manual&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;why-creating-a-package&#34;&gt;Why Creating a Package?&lt;/h1&gt;

&lt;h3 id=&#34;a-package-to-share-academic-work&#34;&gt;A package to share academic work&lt;/h3&gt;

&lt;p&gt;My research projects often involve data manipulation and/or implementing algorithms. I discovered that writing my codes in the form of a package helps me in producing better and reusable code. Creating a package to share your academic work is also very much in line with the idea that scientific research should be reproducible. Users can download your work and install the required dependencies using a single line :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git.clone(&amp;quot;https://github.com/YourGithubUsername/YourPackage.jl.git&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;continuous-integration&#34;&gt;Continuous Integration&lt;/h3&gt;

&lt;p&gt;Another major advantage of creating a package is that it makes your life much easier when it comes to testing your code automatically using &lt;a href=&#34;https://travis-ci.org/&#34; target=&#34;_blank&#34;&gt;TRAVIS CI&lt;/a&gt;. TRAVIS CI is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Continuous_integration&#34; target=&#34;_blank&#34;&gt;continuous integration&lt;/a&gt; system, which considerably helps in detecting and resolving bugs at an early stage.&lt;/p&gt;

&lt;h1 id=&#34;step-by-step-tutorial&#34;&gt;Step-by-step tutorial&lt;/h1&gt;

&lt;p&gt;In what follows, I am assuming you are using Linux, with julia version 0.6 installed. If you are using a different version, just replace &lt;code&gt;v0.6&lt;/code&gt; by the number corresponding to your current version of julia. You also need to have the package &lt;a href=&#34;https://github.com/JuliaLang/PkgDev.jl&#34; target=&#34;_blank&#34;&gt;PkgDev&lt;/a&gt; installed.&lt;/p&gt;

&lt;h2 id=&#34;step-1-generate-your-package&#34;&gt;Step 1: Generate your package&lt;/h2&gt;

&lt;p&gt;The following two lines will create a directory called &lt;code&gt;&amp;quot;MyPackage.jl&amp;quot;&lt;/code&gt; with an &lt;a href=&#34;https://en.wikipedia.org/wiki/MIT_License&#34; target=&#34;_blank&#34;&gt;MIT License&lt;/a&gt;, in
Julia&amp;rsquo;s package location:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using PkgDev
PkgDev.generate(&amp;quot;MyPackage.jl&amp;quot;,&amp;quot;MIT&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By convention, Julia repository names and with &lt;code&gt;.jl&lt;/code&gt;. If you change your working directory to your newly created package (&lt;code&gt;cd ~/.julia/v0.6/MyPackage&lt;/code&gt;), you will notice that the following files and directories have been created:&lt;/p&gt;

&lt;h4 id=&#34;src&#34;&gt;\src&lt;/h4&gt;

&lt;p&gt;The &lt;code&gt;\src&lt;/code&gt; folder will contain your source code. By default, it contains a file &amp;ldquo;MyPackage.jl&amp;rdquo;, which you will use to load other packages and to include &lt;code&gt;.jl&lt;/code&gt; files that you created. In this file, you also state the functions and types you want to export. As an example, you may consult the package &lt;a href=&#34;https://github.com/JuliaStats/Distributions.jl/blob/master/src/Distributions.jl&#34; target=&#34;_blank&#34;&gt;Distributions&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;test&#34;&gt;\test&lt;/h4&gt;

&lt;p&gt;This folder contains a file &lt;code&gt;runtests.jl&lt;/code&gt;, in which you can include &lt;a href=&#34;https://docs.julialang.org/en/v0.6.1/stdlib/test/&#34; target=&#34;_blank&#34;&gt;unit-tests&lt;/a&gt;. Within julia, you can simply run your series of unit-tests with the command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Pkg.test(&amp;quot;MyPackage&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;require&#34;&gt;REQUIRE&lt;/h4&gt;

&lt;p&gt;This file is used to specify the required dependencies. When a user &lt;code&gt;Pkg.clone()&lt;/code&gt;
your package, Julia&amp;rsquo;s package manager will make sure that these requirements are
met. For instance, let&amp;rsquo;s say that your package relies on the version 0.6 of Julia
(or higher) and the package &lt;a href=&#34;https://github.com/JuliaIO/JSON.jl&#34; target=&#34;_blank&#34;&gt;JSON&lt;/a&gt;. The REQUIRE
file will be the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;julia 0.6
JSON
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;readme-md&#34;&gt;README.md&lt;/h4&gt;

&lt;p&gt;You can use this file to add a description of you package.&lt;/p&gt;

&lt;h4 id=&#34;license-md&#34;&gt;LICENSE.md&lt;/h4&gt;

&lt;p&gt;To guide you in the choice of a licence, you may want to consult the following website: &lt;a href=&#34;https://choosealicense.com/&#34; target=&#34;_blank&#34;&gt;https://choosealicense.com/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-2-set-up-your-working-environment&#34;&gt;Step 2: Set-up your working environment&lt;/h2&gt;

&lt;p&gt;This step is optional. While you may want to develop you package directly from Julia&amp;rsquo;s package
directory (&lt;code&gt;~/.julia/v0.6&lt;/code&gt; if you are using &lt;code&gt;julia v0.6&lt;/code&gt;), I personally find it unpleasant. I usually create a symlink to a more convenient location:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ln -s ~/.julia/v0.6/MyPackage your/convenient/directory/MyPackage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running this line in the terminal, you can start working on your package
directly from &lt;code&gt;your/convenient/directory&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;step-3-synchronize-with-github&#34;&gt;Step 3: Synchronize with GitHub&lt;/h2&gt;

&lt;p&gt;The following step will synchronize your package with
your GitHub account. After creating a repository named &amp;ldquo;MyPackage.jl&amp;rdquo; on GitHub, enter the following
commands in the terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git add -A
git commit -m &amp;quot;First commit&amp;quot;
git remote add origin https://github.com/YourGithubUsername/MyPackage.jl.git
git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Going to the page &lt;a href=&#34;https://github.com/YourGithubUsername/MyPackage.jl.git&#34; target=&#34;_blank&#34;&gt;https://github.com/YourGithubUsername/MyPackage.jl.git&lt;/a&gt;, you should now see folders and files mentioned above. Some extra files are also going to be there, for instance &lt;code&gt;.gitignore&lt;/code&gt; or &lt;code&gt;appveyor.yml&lt;/code&gt;. You can ignore them for the time being. After this initial commit, you are almost all set and you can use the usual &lt;a href=&#34;https://guides.github.com/introduction/flow/&#34; target=&#34;_blank&#34;&gt;GitHub workflow&lt;/a&gt;. A good idea though is to enable TRAVIS CI for the repository just you created.&lt;/p&gt;

&lt;h2 id=&#34;step-4-set-up-travis-ci&#34;&gt;Step 4: Set-up TRAVIS CI&lt;/h2&gt;

&lt;p&gt;From your GitHub account, sign in to either:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TravisCI.org if your repository is public&lt;/li&gt;
&lt;li&gt;TravisCI.com if your repository is private&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On TRAVIS CI, go to your profile page. Enable your repository &amp;ldquo;YourGithubUsername/MyPackage.jl&amp;rdquo; by flicking the switch one. Every time you push a new commit, your set of tests, launched by the file &lt;code&gt;/test/runtests.jl&lt;/code&gt;, will be automatically executed on a separate virtual environment. If one of your tests fails, you will be notified by e-mail and (most of the time) you will be able to spot the origin of the error quite easily.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Simulated Method of Moments: a Parallel Implementation</title>
      <link>https://julienpascal.github.io/post/smm_parallel/</link>
      <pubDate>Wed, 06 Jun 2018 15:26:09 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/smm_parallel/</guid>
      <description>

&lt;p&gt;In my &lt;a href=&#34;https://julienpascal.github.io/post/smm/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;, I discussed how
the the simulated method of moments can be used to estimate parameters without
using the likelihood function. This method is useful because many &amp;ldquo;real-life&amp;rdquo; applications result in untractable likelihood functions. In this post, I use the same toy example (estimation of the mean of a mutlivariate normal random variable) and show how to use the
parallel computing capabilities of Julia and &lt;a href=&#34;https://github.com/floswald/MomentOpt.jl/&#34; target=&#34;_blank&#34;&gt;MomentOpt&lt;/a&gt;
to speed-up the estimation.&lt;/p&gt;

&lt;h2 id=&#34;adding-workers&#34;&gt;Adding workers&lt;/h2&gt;

&lt;p&gt;In this example, the goal is to estimate the mean of 4-dimension normal random variable with unit variance, without using any information on the likelihood. If you start Julia with several processors, MomentOpt will notice it and execute the code in parallel. The first step is to add &amp;ldquo;workers&amp;rdquo; to Julia. A rule of thumb is to use as many workers as you have processors on your system (4 in my case).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Current number of workers
#--------------------------
currentWorkers = nprocs()
println(&amp;quot;Initial number of workers = $(currentWorkers)&amp;quot;)
# I want to have 4 workers running
#--------------------------------
maxNumberWorkers = 4
while nprocs() &amp;lt; maxNumberWorkers
    addprocs(1)
end
# check the number of workers:
#----------------------------
currentWorkers = nprocs()
println(&amp;quot;Number of workers = $(currentWorkers)&amp;quot;)
Initial number of workers = 1
Number of workers = 4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;everywhere&#34;&gt;@everywhere&lt;/h2&gt;

&lt;p&gt;When running Julia with several workers, you have to add the macro @everywhere when loading packages and defining functions. More details on parallel computing with Julia can be found &lt;a href=&#34;https://docs.julialang.org/en/v0.6.0/manual/parallel-computing/&#34; target=&#34;_blank&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#---------------------------------------------------------------------------------------------------------
# Julien Pascal
# https://julienpascal.github.io/
# last edit: 06/06/2018
#
# Julia script that shows how the simulated method of moments can be used in
# a simple setting: estimation of the mean of a Normal r.v.
# This version was built to be executed with several processors
# For instance, start julia with: julia -p 4
#
# I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl
#
# Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl
#----------------------------------------------------------------------------------------------------------

@everywhere using MomentOpt
@everywhere using GLM
@everywhere using DataStructures
@everywhere using DataFrames
@everywhere using Plots
#plotlyjs()
@everywhere pyplot()

#------------------------------------------------
# Options
#-------------------------------------------------
# Boolean: do you want to save the plots to disk?
savePlots = true

#------------------------
# initialize the problem:
#------------------------
# Specify the initial values for the parameters, and their support:
#------------------------------------------------------------------
pb = OrderedDict(&amp;quot;p1&amp;quot; =&amp;gt; [0.2,-3,3] , &amp;quot;p2&amp;quot; =&amp;gt; [-0.2,-2,2], &amp;quot;p3&amp;quot; =&amp;gt; [0.1,0,10], &amp;quot;p4&amp;quot; =&amp;gt; [-0.1,-10,0])
# Specify moments to be matched + subjective weights:
#----------------------------------------------------
trueValues = OrderedDict(&amp;quot;mu1&amp;quot; =&amp;gt; [-1.0] , &amp;quot;mu2&amp;quot; =&amp;gt; [1.0], &amp;quot;mu3&amp;quot; =&amp;gt; [5.0], &amp;quot;mu4&amp;quot; =&amp;gt; [-4.0])
moms = DataFrame(name=[&amp;quot;mu1&amp;quot;,&amp;quot;mu2&amp;quot;,&amp;quot;mu3&amp;quot;, &amp;quot;mu4&amp;quot;],value=[-1.0,1.0, 5.0, -4.0], weight=ones(4))




# objfunc_normal(ev::Eval)
#
# GMM objective function to be minized.
# It returns a weigthed distance between empirical and simulated moments
#
@everywhere function objfunc_normal(ev::Eval; verbose = false)

    start(ev)


    # when running in parallel, display worker&#39;s id:
    #-----------------------------------------------
    if verbose == true
        if nprocs() &amp;gt; 1
          println(myid())
        end
    end

    # extract parameters from ev:
    #----------------------------
    mu  = collect(values(ev.params))

    # compute simulated moments
    #--------------------------
    # Monte-Carlo:
    #-------------
    ns = 10000 #number of i.i.d draws from N([mu], sigma)
    #initialize a multivariate normal N([mu], sigma)
    #mu is a four dimensional object
    #sigma is set to be the identity matrix
    sigma = [1.0 ;1.0; 1.0; 1.0]
    # draw ns observations from N([mu], sigma):
    randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma))
    # calculate the mean of the simulated data
    simM            = mean(rand(randMultiNormal,ns),2)
    # store simulated moments in a dictionary
    simMoments = Dict(:mu1 =&amp;gt; simM[1], :mu2 =&amp;gt; simM[2], :mu3 =&amp;gt; simM[3], :mu4 =&amp;gt; simM[4])

    # Calculate the weighted distance between empirical moments
    # and simulated ones:
    #-----------------------------------------------------------
    v = Dict{Symbol,Float64}()
    for (k, mom) in dataMomentd(ev)
        # If weight for moment k exists:
        #-------------------------------
        if haskey(MomentOpt.dataMomentWd(ev), k)
            # divide by weight associated to moment k:
            #----------------------------------------
            v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2
        else
            v[k] = ((simMoments[k] .- mom) ) .^2
        end
    end

    # Set value of the objective function:
    #------------------------------------
    setValue(ev, mean(collect(values(v))))

    # also return the moments
    #-----------------------
    setMoment(ev, simMoments)

    # flag for success:
    #-------------------
    ev.status = 1

    # finish and return
    finish(ev)

    return ev
end



# Initialize an empty MProb() object:
#------------------------------------
mprob = MProb()

# Add structural parameters to MProb():
# specify starting values and support
#--------------------------------------
addSampledParam!(mprob,pb)

# Add moments to be matched to MProb():
#--------------------------------------
addMoment!(mprob,moms)

# Attach an objective function to MProb():
#----------------------------------------
addEvalFunc!(mprob, objfunc_normal)


# estimation options:
#--------------------
# number of iterations for each chain
niter = 1000
# number of chains
# nchains = nprocs()
nchains = 4

opts = Dict(&amp;quot;N&amp;quot;=&amp;gt;nchains,
        &amp;quot;maxiter&amp;quot;=&amp;gt;niter,
        &amp;quot;maxtemp&amp;quot;=&amp;gt; 5,
        &amp;quot;coverage&amp;quot;=&amp;gt;0.025,
        &amp;quot;sigma_update_steps&amp;quot;=&amp;gt;10,
        &amp;quot;sigma_adjust_by&amp;quot;=&amp;gt;0.01,
        &amp;quot;smpl_iters&amp;quot;=&amp;gt;1000,
        &amp;quot;parallel&amp;quot;=&amp;gt;true,
        &amp;quot;maxdists&amp;quot;=&amp;gt;[0.05 for i in 1:nchains],
        &amp;quot;mixprob&amp;quot;=&amp;gt;0.3,
        &amp;quot;acc_tuner&amp;quot;=&amp;gt;12.0,
        &amp;quot;animate&amp;quot;=&amp;gt;false)



#---------------------------------------
# Let&#39;s set-up and run the optimization
#---------------------------------------
# set-up BGP algorithm:
MA = MAlgoBGP(mprob,opts)

# run the estimation:
@time MomentOpt.runMOpt!(MA)

# show a summary of the optimization:
@show MomentOpt.summary(MA)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;

&lt;p&gt;When using the &lt;a href=&#34;https://arxiv.org/abs/1108.3423&#34; target=&#34;_blank&#34;&gt;BGP algorithm&lt;/a&gt;, inference can be done using the first chain only. Other chains are used to explore the state space and help to exit potential local minima, but they are not meant to be used for inference. I discard the first 10th observations to get rid of the influence of the starting values. Visual inspection of the first chain suggests that the stationary part of the Markov chain was reached at this stage. I then report the quasi posterior mean and median for each parameter. As reported below, we are quite close to the true values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Plot histograms for the first chain, the one with which inference should be done.
# Other chains are used to explore the space and avoid local minima
#-------------------------------------------------------------------------------
p1 = histogram(MA.chains[1])
display(p1)

if savePlots == true
    savefig(p1, joinpath(pwd(),&amp;quot;histogram_chain1.svg&amp;quot;))
end

# Plot the realization of the first chain
#----------------------------------------
p2 = plot(MA.chains[1])
if savePlots == true
    savefig(p2, joinpath(pwd(),&amp;quot;history_chain_1.svg&amp;quot;))
end
display(p2)


# Realization of the first chain:
#-------------------------------
dat_chain1 = MomentOpt.history(MA.chains[1])

# discard the first 10th of the observations (&amp;quot;burn-in&amp;quot; phase):
#--------------------------------------------------------------
dat_chain1[round(Int, niter/10):niter, :]

# keep only accepted draws:
#--------------------------
dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ]

# create a list with the parameters to be estimated
parameters = [Symbol(String(&amp;quot;mu$(i)&amp;quot;)) for i=1:4]
# list with the corresponding priors:
#------------------------------------
estimatedParameters = [Symbol(String(&amp;quot;p$(i)&amp;quot;)) for i=1:4]


# Quasi Posterior mean and quasi posterior median for each parameter:
#-------------------------------------------------------------------
for (estimatedParameter, param) in zip(estimatedParameters, parameters)

  println(&amp;quot;Quasi posterior mean for $(String(estimatedParameter)) = $(mean(dat_chain1[estimatedParameter]))&amp;quot;)
  println(&amp;quot;Quasi posterior median for $(String(estimatedParameter)) = $(median(dat_chain1[estimatedParameter]))&amp;quot;)
  println(&amp;quot;True value = $(trueValues[String(param)][])&amp;quot;)

end

# Output:
#--------
# Quasi posterior mean for p1 = -0.9160461484604642
# Quasi posterior median for p1 = -0.9589739759449558
# True value = -1.0
# Quasi posterior mean for p2 = 0.9888798123473025
# Quasi posterior median for p2 = 1.0675028518780796
# True value = 1.0
# Quasi posterior mean for p3 = 4.922658319685393
# Quasi posterior median for p3 = 4.989662707150382
# True value = 5.0
# Quasi posterior mean for p4 = -3.898597557236236
# Quasi posterior median for p4 = -3.968649064061086
# True value = -4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;figure-1&#34;&gt;Figure 1&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/history_chain_1_Parallel.svg&#34; alt=&#34;History of chain 1&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;History of chain 1&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;figure-2&#34;&gt;Figure 2&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/histogram_chain_1_Parallel.svg&#34; alt=&#34;Histograms for chain 1&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Histograms for chain 1&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;safety-checks&#34;&gt;Safety checks&lt;/h2&gt;

&lt;p&gt;In this toy example, we know that the conditions for global identification are met. However, in more complicated applications, global identification may be hard to prove analytically. A common practice is to make sure that the objective function is &amp;ldquo;well-behaved&amp;rdquo; in a neighborhood of the solution using slices. The graph below shows that there is no flat region in the neighborhood of the solution, suggesting (at least) local identification of the parameters.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# plot slices of objective function
# grid with 20 points
#-----------------------------------
s = doSlices(mprob,20)

# plot slices of the objective function:
#---------------------------------------
p = MomentOpt.plot(s,:value)
display(p)

if savePlots == true
    Plots.savefig(p, joinpath(pwd(),&amp;quot;slices_Normal.svg&amp;quot;))
end



# Produce more precise plots with respect to each parameter:
#-----------------------------------------------------------
for symbol in parameters

  p = MomentOpt.plot(s,symbol)
  display(p)

  if savePlots == true
      Plots.savefig(p, joinpath(pwd(),&amp;quot;slices_Normal_$(String(symbol)).svg&amp;quot;))
  end


end
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;figure-3&#34;&gt;Figure 3&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/slices_Normal_Parallel.svg&#34; alt=&#34;Slices of the objective function&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Slices of the objective function&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;parallel-versus-serial&#34;&gt;Parallel versus Serial&lt;/h2&gt;

&lt;p&gt;Here is benchmark of running the code above in serial versus in parallel
(starting julia with 4 workers):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Serial: 639.661831 seconds (12.52 M allocations: 1.972 GiB, 97.50% gc time)&lt;/li&gt;
&lt;li&gt;Parallel: 372.454707 seconds (279.32 M allocations: 14.843 GiB, 2.19% gc time)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Computing time is approximately divided by 2 when executing the parallel version.&lt;/p&gt;

&lt;h2 id=&#34;notebook&#34;&gt;Notebook&lt;/h2&gt;

&lt;p&gt;A jupyter notebook containing the code in this post (with some slight modifications)
can be downloaded &lt;a href=&#34;https://github.com/JulienPascal/ExampleMomentOpt/blob/master/Example_MomentOpt_parallel.ipynb&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Simulated Method of Moments</title>
      <link>https://julienpascal.github.io/post/smm/</link>
      <pubDate>Mon, 12 Feb 2018 15:07:33 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/smm/</guid>
      <description>

&lt;p&gt;As Thomas Sargent said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;A rational expectations equilibrium model is a likelihood function&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However in many cases, the likelihood function is too complicated to be
written down in closed form. To estimate the structural parameters of a given model, one can still use Monte-Carlo methods. In this post, I would like to describe the &lt;a href=&#34;https://en.wikipedia.org/wiki/Method_of_simulated_moments&#34; target=&#34;_blank&#34;&gt;simulated method of moments&lt;/a&gt; (SMM), which is a widely used simulation-based estimation technique.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-setting&#34;&gt;A Simple Setting&lt;/h2&gt;

&lt;p&gt;I want to illustrate the SMM in one of the simplest settings you could think of:
the estimation of the mean of a normal density with known variance. Let&amp;rsquo;s say we have access to a (bi-dimensional) time series and we suspect it to be normally distributed with mean $[a,\,b]&amp;lsquo;$ and variance the identity matrix $\mathcal{N}([a,\,b]&amp;lsquo;,\,I_2)$. Let&amp;rsquo;s pretend that we have no idea on how to write down the associated likelihood function. The good news is that if we have access to a &amp;ldquo;black box&amp;rdquo; that generates $i.i.d$ draws from the law $\mathcal{N}([c,\,d]&amp;lsquo;,\,I_2)$, it is enough for us to do inference.&lt;/p&gt;

&lt;h2 id=&#34;smm-is-gmm&#34;&gt;SMM is GMM&lt;/h2&gt;

&lt;p&gt;The SMM estimator can be viewed as an extension of &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_method_of_moments&#34; target=&#34;_blank&#34;&gt;GMM&lt;/a&gt;. The difference being that the function mapping the set of parameters to moments has no closed-form expression. Mathematically, we want to minimize the following objective function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation_SMM.png&#34; alt=&#34;SMM objective function\label{objective_function}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where $m^*$ is a vector empirical moments, $m(\theta)$ a vector of the same moments
calculated using simulated data when the structural parameters are equal to $\theta$, and $W$ a &lt;a href=&#34;https://en.wikipedia.org/wiki/Weighing_matrix&#34; target=&#34;_blank&#34;&gt;weighing matrix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The SMM estimate is such that the (weighted) distance between simulated and empirical moments is minimized. This estimator is quite intuitive: under the hypothesis that the model is correctly specified, it should be able to reproduce empirical moments when parameters values are equal to the &amp;ldquo;true&amp;rdquo; ones.&lt;/p&gt;

&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;

&lt;p&gt;Under some regularity conditions (see &lt;a href=&#34;https://www.google.fr/search?client=ubuntu&amp;amp;channel=fs&amp;amp;q=+McFadden+1989&amp;amp;ie=utf-8&amp;amp;oe=utf-8&amp;amp;gfe_rd=cr&amp;amp;dcr=0&amp;amp;ei=a52BWsGULYeO8Qfq-p3YCw&#34; target=&#34;_blank&#34;&gt;McFadden 1989&lt;/a&gt;), the extra noise introduced by simulation is
not problematic and inference is possible. That is, we can build a confidence intervals
for the SMM estimates.&lt;/p&gt;

&lt;h2 id=&#34;implementation-in-julia&#34;&gt;Implementation in Julia&lt;/h2&gt;

&lt;p&gt;The code below shows how one can recover the true parameters of the Normal density $\mathcal{N}([a,\,b]&amp;lsquo;,\,I_2)$. I use the &lt;a href=&#34;https://github.com/floswald/MomentOpt.jl&#34; target=&#34;_blank&#34;&gt;MomentOpt&lt;/a&gt; package, which relies on some &lt;a href=&#34;(https://arxiv.org/abs/1108.3423)&#34; target=&#34;_blank&#34;&gt;refinements&lt;/a&gt; of the &lt;a href=&#34;(https://arxiv.org/abs/1108.3423)&#34; target=&#34;_blank&#34;&gt;MCMC&lt;/a&gt; method to explore the state-space with several Markov chains in parallel. These Markov chains communicate between themselves to avoid being stuck in a local mode of the posterior distribution. In practice, I choose 5 Markov chains. Figure 1 shows the realizations for the first chain. As illustrated in Figure 2, we successfully recovered the true values for $a$ and $b$. For each chain, the quasi-posterior mean and median for $a$ and $b$ are extremely close to the true values $1$ and $-1$.&lt;/p&gt;

&lt;h4 id=&#34;figure-1&#34;&gt;Figure 1&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/history_chain_1.svg&#34; alt=&#34;history one chain&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;History of one chain&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;figure-2&#34;&gt;Figure 2&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://julienpascal.github.io/img/histogram.svg&#34; alt=&#34;histogram SMM&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Histograms for the 5 chains&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#---------------------------------------------------------------------------------------------------------
# Julien Pascal
# last edit: 12/02/2018
#
# Julia script that shows how the simulated
# method of moments can be used in a simple
# setting: estimation of the mean of a Normal r.v.
#
# I use the package MomentOpt: https://github.com/floswald/MomentOpt.jl
#
# Code heavily based on the file https://github.com/floswald/MomentOpt.jl/blob/master/src/mopt/Examples.jl
#----------------------------------------------------------------------------------------------------------

using MomentOpt
using GLM
using DataStructures
using DataFrames
using Plots
#plotlyjs()
pyplot()

#------------------------------------------------
# Options
#-------------------------------------------------
# Boolean: do you want to save the plots to disk?
savePlots = true

# initialize the problem:
#------------------------
# initial values:
#----------------
pb    = OrderedDict(&amp;quot;p1&amp;quot; =&amp;gt; [0.2,-3,3] , &amp;quot;p2&amp;quot; =&amp;gt; [-0.2,-2,2] )
# moments to be matched:
#-----------------------
moms = DataFrame(name=[&amp;quot;mu1&amp;quot;,&amp;quot;mu2&amp;quot;],value=[-1.0,1.0], weight=ones(2))



&amp;quot;&amp;quot;&amp;quot;
    objfunc_normal(ev::Eval)

    GMM objective function to be minized.
    It returns a weigthed distance between empirical and simulated moments

    copy-paste of the function objfunc_norm(ev::Eval)
    I only made minor modifications to the original fuction

&amp;quot;&amp;quot;&amp;quot;

function objfunc_normal(ev::Eval)

    start(ev)


    # extract parameters from ev:
    #----------------------------
    mu  = collect(values(ev.params))

    # compute simulated moments
    #--------------------------
    # Monte-Carlo:
    #-------------
    ns = 10000 #number of i.i.d draws from N([a,b], sigma)
    #initialize a multivariate normal N([a,b], sigma)
    #using a = mu[1], b=mu[2]
    sigma = [1.0 ;1.0]
    randMultiNormal = MomentOpt.MvNormal(mu,MomentOpt.PDiagMat(sigma))
    simM            = mean(rand(randMultiNormal,ns),2) #mean of simulated data
    simMoments = Dict(:mu1 =&amp;gt; simM[1], :mu2 =&amp;gt; simM[2])#store simulated moments in a dictionary


    # Calculate the weighted distance between empirical moments
    # and simulated ones:
    #-----------------------------------------------------------
    v = Dict{Symbol,Float64}()
    for (k, mom) in dataMomentd(ev)
        # If weight for moment k exists:
        #-------------------------------
        if haskey(MomentOpt.dataMomentWd(ev), k)
            # divide by weight associated to moment k:
            #----------------------------------------
            v[k] = ((simMoments[k] .- mom) ./ MomentOpt.dataMomentW(ev,k)) .^2
        else
            v[k] = ((simMoments[k] .- mom) ) .^2
        end
    end

    # Set value of the objective function:
    #------------------------------------
    setValue(ev, mean(collect(values(v))))

    # also return the moments
    #-----------------------
    setMoment(ev, simMoments)

    # flag for success:
    #-------------------
    ev.status = 1

    # finish and return
    finish(ev)

    return ev
end



# Initialize an empty MProb() object:
#------------------------------------
mprob = MProb()

# Add structural parameters to MProb():
# specify starting values and support
#--------------------------------------
addSampledParam!(mprob,pb)

# Add moments to be matched to MProb():
#--------------------------------------
addMoment!(mprob,moms)

# Attach an objective function to MProb():
#----------------------------------------
addEvalFunc!(mprob, objfunc_normal)


# estimation options:
#--------------------
# number of iterations for each chain
niter = 1000
# number of chains
nchains = 5

opts = Dict(&amp;quot;N&amp;quot;=&amp;gt;nchains,
        &amp;quot;maxiter&amp;quot;=&amp;gt;niter,
        &amp;quot;maxtemp&amp;quot;=&amp;gt; 5,
        # choose inital sd for each parameter p
        # such that Pr( x \in [init-b,init+b]) = 0.975
        # where b = (p[:ub]-p[:lb])*opts[&amp;quot;coverage&amp;quot;] i.e. the fraction of the search interval you want to search around the initial value
        &amp;quot;coverage&amp;quot;=&amp;gt;0.025,  # i.e. this gives you a 95% CI about the current parameter on chain number 1.
        &amp;quot;sigma_update_steps&amp;quot;=&amp;gt;10,
        &amp;quot;sigma_adjust_by&amp;quot;=&amp;gt;0.01,
        &amp;quot;smpl_iters&amp;quot;=&amp;gt;1000,
        &amp;quot;parallel&amp;quot;=&amp;gt;true,
        &amp;quot;maxdists&amp;quot;=&amp;gt;[0.05 for i in 1:nchains],
        &amp;quot;mixprob&amp;quot;=&amp;gt;0.3,
        &amp;quot;acc_tuner&amp;quot;=&amp;gt;12.0,
        &amp;quot;animate&amp;quot;=&amp;gt;false)


# plot slices of objective function
#---------------------------------
s = doSlices(mprob,30)

# plot objective function over param values:
#-------------------------------------------
p1 = MomentOpt.plot(s,:value)

if savePlots == true
    Plots.savefig(p1, joinpath(pwd(),&amp;quot;slices_Normal1.svg&amp;quot;))
end

# plot value of moment :mu1 over param values
#--------------------------------------------
p2 = MomentOpt.plot(s,:mu1)


if savePlots == true
   Plots.savefig(p2, joinpath(pwd(),&amp;quot;slices_Normal2.svg&amp;quot;))
end

# plot value of moment :mu2 over param values
#--------------------------------------------
p3 = Plots.plot(s,:mu2)

if savePlots == true
    Plots.savefig(p3, joinpath(pwd(),&amp;quot;slices_Normal3.svg&amp;quot;))
end

#---------------------------------------
# Let&#39;s set-up and run the optimization
#---------------------------------------
# set-up BGP algorithm:
MA = MAlgoBGP(mprob,opts)

# run the estimation:
@time MomentOpt.runMOpt!(MA)

# show a summary of the optimization:
@show MomentOpt.summary(MA)

# Plot histograms for chains:
#----------------------------
p4 = histogram(MA.chains[1])
p5 = histogram(MA.chains[2])
p6 = histogram(MA.chains[3])
p7 = histogram(MA.chains[4])
p8 = histogram(MA.chains[5])

p9 = Plots.plot(p4, p5, p6, p7, p8, layout=(5,1), legend=false)

if savePlots == true
    savefig(p9, joinpath(pwd(),&amp;quot;histogram.svg&amp;quot;))
end

# Plot the &amp;quot;history&amp;quot; of one chain:
#--------------------------------
p10 = plot(MA.chains[1])
if savePlots == true
    savefig(p10, joinpath(pwd(),&amp;quot;history_chain_1.svg&amp;quot;))
end


# Realization of chain 1:
#-----------------------
dat_chain1 = MomentOpt.history(MA.chains[1])

# keep only accepted draws:
#-------------------------
dat_chain1 = dat_chain1[dat_chain1[:accepted ].== true, : ]


# Quasi Posterior mean
#---------------------
QP_mean_p1 = mean(dat_chain1[:p1])
QP_mean_p2 = mean(dat_chain1[:p2])

# Quasi Posterior median
#-----------------------
QP_median_p1 = median(dat_chain1[:p1])
QP_median_p2 = median(dat_chain1[:p2])
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Solving Bellman Equations by the Collocation Method</title>
      <link>https://julienpascal.github.io/post/collocation/</link>
      <pubDate>Thu, 07 Dec 2017 13:50:29 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/collocation/</guid>
      <description>

&lt;p&gt;A large class of economic models involves solving for functional equations of the
form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation1.png&#34; alt=&#34;equation1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A well known example is the &lt;a href=&#34;https://lectures.quantecon.org/jl/optgrowth.html&#34; target=&#34;_blank&#34;&gt;stochastic optimal growth model&lt;/a&gt;. An agent owns a consumption good $y$ at time $t$, which can be consumed or invested. Next period&amp;rsquo;s output depends on how much is invested at time $t$ and on a shock $z$ realized at the end of the current period. One can think of a farmer deciding the quantity of seeds to be planted during the spring, taking into account weather forecast for the growing season.&lt;/p&gt;

&lt;p&gt;A common technique for solving this class of problem is value function iteration. While value function
iteration is quite intuitive, it is not the only one available.
In this post, I describe the collocation method, which transforms the problem of finding
a &lt;em&gt;function&lt;/em&gt; into a problem of finding a &lt;em&gt;vector&lt;/em&gt; that satisfies a given set of conditions. The gain from this change of perspective is that any &lt;a href=&#34;https://en.wikipedia.org/wiki/Root-finding_algorithm&#34; target=&#34;_blank&#34;&gt;root-finding algorithm&lt;/a&gt; can then be used. In particular, one may use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_method&#34; target=&#34;_blank&#34;&gt;Newton method&lt;/a&gt;,
which converges at a quadratic rate in the neighborhood of the solution if the function is smooth enough.&lt;/p&gt;

&lt;h2 id=&#34;value-function-iteration&#34;&gt;Value function iteration&lt;/h2&gt;

&lt;p&gt;Value function iteration takes advantage of the fact that the Bellman operator $T$ is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Contraction_mapping&#34; target=&#34;_blank&#34;&gt;contraction  mapping&lt;/a&gt; on the set of continuous bounded functions on $\mathbb R_+$ under the supremum distance&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation2.png&#34; alt=&#34;equation2&#34; /&gt;
 An immediate consequence if that the sequence $w,Tw,T^2w$,… converges uniformly to  $w$ (starting with any bounded and continuous function $w$). The following code in &lt;code&gt;Julia v0.6.4&lt;/code&gt; illustrates the convergence of the series ${T^nw}$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt; #=
Julien Pascal
Code heavily based on:
----------------------
https://lectures.quantecon.org/jl/optgrowth.html
by Spencer Lyon, John Stachurski
I have only made minor modifications:
#------------------------------------
* I added a type optGrowth
* I use the package Interpolations
* I calculate the expectation w.r.t the aggregate
  shock using a Gauss-Legendre quadrature scheme
  instead of Monte-Carlo
=#

using QuantEcon
using Optim
using CompEcon
using PyPlot
using Interpolations
using FileIO


type optGrowth

  w::Array{Float64,1}
  β::AbstractFloat
  grid::Array{Float64,1}
  u::Function
  f::Function
  shocks::Array{Float64,1}
  Tw::Array{Float64,1}
  σ::Array{Float64,1}
  el_k::Array{Float64,1}
  wl_k::Array{Float64,1}
  compute_policy::Bool
  w_func::Function

end

function optGrowth(;w = Array{Float64,1}[],
                    α = 0.4,
                    β = 0.96,
                    μ = 0,
                    s = 0.1,
                    grid_max = 4,         # Largest grid point
                    grid_size = 200,      # Number of grid points
                    shock_size = 250,     # Number of shock draws in Monte Carlo integral
                    Tw = Array{Float64,1}[],
                    σ = Array{Float64,1}[],
                    el_k = Array{Float64,1}[],
                    wl_k = Array{Float64,1}[],
                    compute_policy = true
                  )


  grid_y = collect(linspace(1e-5, grid_max, grid_size))
  shocks = exp.(μ + s * randn(shock_size))

  # Utility
  u(c) = log(c)
  # Production
  f(k) = k^α

  w = 5 * log.(grid_y)
  Tw = 5 * log.(grid_y)
  σ = 5 * log.(grid_y)

  el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2)

  w_func = x -&amp;gt; x
  optGrowth(
    w,
    β,
    grid_y,
    u,
    f,
    shocks,
    Tw,
    σ,
    el_k,
    wl_k,
    compute_policy,
    w_func
    )
end

&amp;quot;&amp;quot;&amp;quot;
The approximate Bellman operator, which computes and returns the
updated value function Tw on the grid points.
#### Arguments
`model` : a model of type optGrowth
`Modifies model.σ, model.w and model.Tw
&amp;quot;&amp;quot;&amp;quot;
function bellman_operator!(model::optGrowth)

    # === Apply linear interpolation to w === #
    knots = (model.grid,)
    itp = interpolate(knots, model.w, Gridded(Linear()))

    #w_func(x) = itp[x]

    model.w_func = x -&amp;gt; itp[x]

    if model.compute_policy
        model.σ = similar(model.w)
    end

    # == set Tw[i] = max_c { u(c) + β E w(f(y  - c) z)} == #
    for (i, y) in enumerate(model.grid)

        #Monte Carlo
        #-----------
        #objective(c) = - model.u(c) - model.β * mean(w_func(model.f(y - c) .* model.shocks))

        #Gauss-Legendre
        #--------------
        function objective(c)

          expectation = 0.0

          for k = 1:length(model.wl_k)
            expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k]))
          end

          - model.u(c) - model.β * expectation

        end

        res = optimize(objective, 1e-10, y)

        if model.compute_policy
            model.σ[i] = Optim.minimizer(res)
        end

        model.Tw[i] = - Optim.minimum(res)
        model.w[i] = - Optim.minimum(res)
    end


end


model = optGrowth()

function solve_optgrowth!(model::optGrowth;
                         tol::AbstractFloat=1e-6,
                         max_iter::Integer=500)

    w_old = copy(model.w)  # Set initial condition
    error = tol + 1
    i = 0


    # Iterate to find solution
    while i &amp;lt; max_iter

        #update model.w
        bellman_operator!(model)
        error = maximum(abs, model.w - w_old)

        if error &amp;lt; tol
          break
        end

        w_old = copy(model.w)
        i += 1
    end

end

#-----------------------------------
# Solve by value function iteration
#-----------------------------------
@time solve_optgrowth!(model)
# 3.230501 seconds (118.18 M allocations: 1.776 GiB, 3.51% gc time)


#-------------------------------
# Compare with the true solution
#-------------------------------
α = 0.4
β = 0.96
μ = 0
s = 0.1

c1 = log(1 - α * β) / (1 - β)
c2 = (μ + α * log(α * β)) / (1 - α)
c3 = 1 / (1 - β)
c4 = 1 / (1 - α * β)


# True optimal policy
c_star(y) = (1 - α * β) * y

# True value function
v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y)

fig, ax = subplots(figsize=(9, 5))
ax[:set_ylim](-35, -24)
ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=&amp;quot;Approximated (VFI)&amp;quot;, linestyle=&amp;quot;--&amp;quot;)
ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=&amp;quot;Exact value&amp;quot;)
ax[:legend](loc=&amp;quot;lower right&amp;quot;)
savefig(&amp;quot;value_function_iteration.png&amp;quot;)

fig, ax = subplots(figsize=(9, 5))
ax[:set_xlim](0.1, 4.0)
ax[:set_ylim](0.00, 0.008)
ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=&amp;quot;error&amp;quot;)
ax[:legend](loc=&amp;quot;lower right&amp;quot;)
savefig(&amp;quot;error_value_function_iteration.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;The following figure shows the exact function, which we know in this very specific case, and the one calculated using VFI. Both quantities are almost indistinguishable. As the illustrated in the next figure, the (absolute value) of the distance between the true and the approximated values is bounded above by $0.006$. The accuracy of the current approximation could be improved by iterating the process further.
&lt;img src=&#34;https://julienpascal.github.io/img/value_function_iteration.png&#34; alt=&#34;VFI&#34; /&gt;
&lt;img src=&#34;https://julienpascal.github.io/img/error_value_function_iteration.png&#34; alt=&#34;VFI&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-collocation-method&#34;&gt;The collocation method&lt;/h2&gt;

&lt;p&gt;The collocation method takes a different route. Let us remember that we are looking for a &lt;em&gt;function&lt;/em&gt; $w$. Instead of solving for the values of $w$ on a grid and then interpolating, why not looking for a function directly? To do so, let us assume that $w$ can reasonably be approximated by a function $\hat{w}$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation3.png&#34; alt=&#34;equation3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;with
$ \phi_1(x) $ , $ \phi_2(x) $,&amp;hellip;, $ \phi_n(x) $  a set of linearly independent basis functions and $c_1$, $c_2$, &amp;hellip;, $c_n$  $n$ coefficient to be found. Replacing $w(x)$ with $\hat{w(x)}$ into the functional equation and reorganizing gives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation4.png&#34; alt=&#34;equation4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This equation has to hold (almost) exactly at $n$ points (also called nodes): $y_1$, $y_2$, &amp;hellip;, $y_n$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/equation5.png&#34; alt=&#34;equation5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The equation above defines a system of $n$ equation with as many unknown, which can be compactly written as:
$$ f(\boldsymbol{c}) = \boldsymbol{0} $$&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_method&#34; target=&#34;_blank&#34;&gt;Newton&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Quasi-Newton_method&#34; target=&#34;_blank&#34;&gt;quasi-Newton&lt;/a&gt; can be used to solve for the root of $f$. In the code that follows, I use &lt;a href=&#34;https://en.wikipedia.org/wiki/Broyden%27s_method&#34; target=&#34;_blank&#34;&gt;Broyden&amp;rsquo;s&lt;/a&gt; method. Let us illustrate this technique using a Chebychev polynomial basis and Chebychev nodes. In doing so, we avoid &lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%27s_phenomenon&#34; target=&#34;_blank&#34;&gt;Runge&amp;rsquo;s phenomenon&lt;/a&gt; associated with a uniform grid.&lt;/p&gt;

&lt;h3 id=&#34;implementation-using-compecon&#34;&gt;Implementation using CompEcon&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt; #---------------------------------------------
 # Julien Pascal
 # Solve the stochastic optimal growth problem
 # using the collocation method
 #---------------------------------------------
 using QuantEcon
 using Optim
 using CompEcon
 using PyPlot
 using Interpolations


 type optGrowthCollocation

   w::Array{Float64,1}
   β::AbstractFloat
   grid::Array{Float64,1}
   u::Function
   f::Function
   shocks::Array{Float64,1}
   Tw::Array{Float64,1}
   σ::Array{Float64,1}
   el_k::Array{Float64,1}
   wl_k::Array{Float64,1}
   compute_policy::Bool
   order_approximation::Int64 #number of element in the functional basis along each dimension
   functional_basis_type::String #type of functional basis
   fspace::Dict{Symbol,Any} #functional basis
   fnodes::Array{Float64,1} #collocation nodes
   residual::Array{Float64,1} #vector of residual. Should be close to zero
   a::Array{Float64,1} #polynomial coefficients
   w_func::Function

 end


   #####################################
   # Function that finds a solution
   # to f(x) = 0
   # using Broyden&#39;s &amp;quot;good&amp;quot; method
   # and a simple backstepping procedure as described
   # in Miranda and Fackler (2009)
   #
   # input :
   # --------
   # * x0:                 initial guess for the root
   # * f:                  function in f(x) = 0
   # * maxit:              maximum number of iterations
   # * tol:                tolerance level for the zero
   # * fjavinc:            initial inverse of the jacobian. If not provided, then inverse of the
   #                       Jacobian is calculated by finite differences
   # * maxsteps:           maximum number of backsteps
   # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian
   #
   # output :
   # --------
   # * x: one zero of f
   # * it: number of iterations necessary to reached the solution
   # * fjacinv: pseudo jacobian at the last iteration
   # * fnorm: norm f(x) at the last iteration
   #
   #######################################
   function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0));
                         maxsteps = 5, recaculateJacobian = 1)

       println(&amp;quot;a0 = $(x0)&amp;quot;)
       fnorm = tol*2
       it2 = 0 #to re-initialize the jacobian

       ################################
       #initialize guess for the matrix
       ################################
       fjacinv_function = x-&amp;gt; Calculus.finite_difference_jacobian(f, x)
       #fjacinv_function = x -&amp;gt; ForwardDiff.gradient(f, x)

       # If the user do not provide an initial guess for the jacobian
       # One is calculated using finite differences.
       if fjacinv == eye(length(x0))
           ################################################
           # finite differences to approximate the Jacobian
           # at the initial value
           # this is slow. Seems to improve performances
           # when x0 is of high dimension.
           println(&amp;quot;Calculating the Jacobian by finite differences&amp;quot;)
           #@time fjacinv = Calculus.finite_difference_jacobian(f, x0)
           @time fjacinv = fjacinv_function(x0)

           println(&amp;quot;Inverting the Jacobian&amp;quot;)
           try
               fjacinv = inv(fjacinv)
           catch
               try
                   println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                   fjacinv = pinv(A)
               catch
                   println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                   fjacinv = eye(length(x0))
               end
           end
           println(&amp;quot;Done&amp;quot;)
       else
           println(&amp;quot;Using User&#39;s input as a guess for the Jacobian.&amp;quot;)
       end

       fval = f(x0)

       for it=1:maxit

           it2 +=1

           #every 30 iterations, reinitilize the jacobian
           if mod(it2, recaculateJacobian) == 0

               println(&amp;quot;Re-calculating the Jacobian&amp;quot;)

               fjacinv = fjacinv_function(x0)

               try
                   fjacinv = inv(fjacinv)
               catch
                   try
                       println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                       fjacinv = pinv(A)
                   catch
                       println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                       fjacinv = eye(length(x0))
                   end
               end

           end


           println(&amp;quot;it = $(it)&amp;quot;)

           fnorm = norm(fval)

           if fnorm &amp;lt; tol
               println(&amp;quot;fnorm = $(fnorm)&amp;quot;)
               return x0, it, fjacinv, fnorm
           end

           d = -(fjacinv*fval)

           fnormold = Inf
           ########################
           # Backstepping procedure
           ########################
           for backstep = 1:maxsteps

               if backstep &amp;gt; 1
                   println(&amp;quot;backstep = $(backstep-1)&amp;quot;)
               end

               fvalnew = f(x0 + d)
               fnormnew = norm(fvalnew)

               if fnormnew &amp;lt; fnorm
                   break
               end

               if fnormold &amp;lt; fnormnew
                   d=2*d
                   break
               end

               fnormold = fnormnew

               d = d/2

           end
           ####################
           ####################

           x0 = x0 + d

           fold = fval
           fval = f(x0)

           u = fjacinv*(fval - fold)

           #Update the pseudo Jacobian:
           fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u))

           println(&amp;quot;a$(it) = $(x0)&amp;quot;)
           println(&amp;quot;fnorm = $(fnorm)&amp;quot;)

           if isnan.(x0) == trues(length(x0))
               println(&amp;quot;Error. a$(it) = NaN for each component&amp;quot;)
               x0 = zeros(length(x0))
               return x0, it, fjacinv, fnorm
           end
       end

       println(&amp;quot;In function find_broyden\n&amp;quot;)
       println(&amp;quot;Maximum number of iterations reached.\n&amp;quot;)
       println(&amp;quot;No convergence.&amp;quot;)
       println(&amp;quot;Returning fnorm = NaN as a solution&amp;quot;)
       fnorm = NaN
       return x0, maxit, fjacinv, fnorm

   end

 function optGrowthCollocation(;w = Array{Float64,1}[],
                               α = 0.4,
                               β = 0.96,
                               μ = 0,
                               s = 0.1,
                               grid_max = 4,         # Largest grid point
                               grid_size = 200,      # Number of grid points
                               shock_size = 250,     # Number of shock draws in Monte Carlo integral
                               Tw = Array{Float64,1}[],
                               σ = Array{Float64,1}[],
                               el_k = Array{Float64,1}[],
                               wl_k = Array{Float64,1}[],
                               compute_policy = true,
                               order_approximation = 40,
                               functional_basis_type = &amp;quot;chebychev&amp;quot;,
                             )


   grid_y = collect(linspace(1e-5, grid_max, grid_size))
   shocks = exp.(μ + s * randn(shock_size))

   # Utility
   u(c) = log.(c)
   # Production
   f(k) = k^α

   el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2)

   lower_bound_support = minimum(grid_y)
   upper_bound_support = maximum(grid_y)

   n_functional_basis = [order_approximation]

   if functional_basis_type == &amp;quot;chebychev&amp;quot;
       fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support)
   elseif functional_basis_type == &amp;quot;splines&amp;quot;
       fspace = fundefn(:spli, n_functional_basis, lower_bound_support, upper_bound_support, 1)
   elseif functional_basis_type == &amp;quot;linear&amp;quot;
       fspace = fundefn(:lin, n_functional_basis, lower_bound_support, upper_bound_support)
   else
       error(&amp;quot;functional_basis_type has to be either chebychev, splines or linear.&amp;quot;)
   end


   fnodes = funnode(fspace)[1]
   residual = zeros(size(fnodes)[1])
   a = ones(size(fnodes)[1])

   w = ones(size(fnodes)[1])
   Tw = ones(size(fnodes)[1])
   σ = ones(size(fnodes)[1])

   w_func = x-&amp;gt; x

   optGrowthCollocation(
     w,
     β,
     grid_y,
     u,
     f,
     shocks,
     Tw,
     σ,
     el_k,
     wl_k,
     compute_policy,
     order_approximation,
     functional_basis_type,
     fspace,
     fnodes,
     residual,
     a,
     w_func
     )
 end



 function residual!(model::optGrowthCollocation)


     model.w_func = y -&amp;gt; funeval(model.a, model.fspace, [y])[1][1]   


     function objective(c, y)

       expectation = 0.0

       for k = 1:length(model.wl_k)
         expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k]))
       end

       - model.u(c) - model.β * expectation

     end

     # Loop over nodes
     for i in 1:size(model.fnodes)[1]

         y = model.fnodes[i,1]

         res = optimize(c -&amp;gt; objective(c, y), 1e-10, y)

         if model.compute_policy
             model.σ[i] = Optim.minimizer(res)
         end

         model.Tw[i] = - Optim.minimum(res)
         model.w[i] = model.w_func(y)

         model.residual[i] = - model.w[i] + model.Tw[i]
     end

 end


 model = optGrowthCollocation(functional_basis_type = &amp;quot;chebychev&amp;quot;)

 residual!(model)

 function solve_optgrowth!(model::optGrowthCollocation;
                          tol=1e-6,
                          max_iter=500)

     # Initialize guess for coefficients
     # by giving the &amp;quot;right shape&amp;quot;
     # ---------------------------------
     function objective_initialize!(x, model)

       #update polynomial coeffficients
       model.a = copy(x)

       model.w_func = y -&amp;gt; funeval(model.a, model.fspace, [y])[1][1]

       return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1]))

     end


     minx, iterations, Jac0, fnorm = find_broyden(model.a, x -&amp;gt; objective_initialize!(x, model), max_iter, tol)


     # Solving the model by collocation
     # using the initial guess calculated above
     #-----------------------------------------
     function objective_residual!(x, model)

       #update polynomial coeffficients
       model.a = copy(x)

       #calculate residual
       residual!(model)

       return abs.(model.residual)

     end

     minx, iterations, Jac, fnorm = find_broyden(model.a, x -&amp;gt; objective_residual!(x, model), max_iter, tol)


 end

 #-----------------------------------
 # Solve by collocation
 #-----------------------------------
 @time solve_optgrowth!(model)


 #-------------------------------
 # Compare with the true solution
 #-------------------------------
 α = 0.4
 β = 0.96
 μ = 0
 s = 0.1

 c1 = log(1 - α * β) / (1 - β)
 c2 = (μ + α * log(α * β)) / (1 - α)
 c3 = 1 / (1 - β)
 c4 = 1 / (1 - α * β)


 # True optimal policy
 c_star(y) = (1 - α * β) * y

 # True value function
 v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y)

 fig, ax = subplots(figsize=(9, 5))
 ax[:set_ylim](-35, -24)
 ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=&amp;quot;Approximated (collocation)&amp;quot;, linestyle = &amp;quot;--&amp;quot;)
 ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=&amp;quot;Exact value&amp;quot;)
 ax[:legend](loc=&amp;quot;lower right&amp;quot;)
 savefig(&amp;quot;collocation.png&amp;quot;)


 fig, ax = subplots(figsize=(9, 5))
 ax[:set_xlim](0.1, 4.0)
 ax[:set_ylim](-0.05, 0.05)
 ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=&amp;quot;error&amp;quot;)
 ax[:legend](loc=&amp;quot;lower right&amp;quot;)
 savefig(&amp;quot;error_collocation.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output-1&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;The following figure shows the exact function and the one calculated using the collocation method. In terms of accuracy, both VFI and the collocation method generate reliable results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://julienpascal.github.io/img/collocation.png&#34; alt=&#34;VFI&#34; /&gt;
 &lt;img src=&#34;https://julienpascal.github.io/img/error_collocation.png&#34; alt=&#34;VFI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In terms of speed, it turns out that the value function iteration implementation is much faster. One reason seems to be the efficiency associated with the package &lt;a href=&#34;https://github.com/JuliaMath/Interpolations.jl&#34; target=&#34;_blank&#34;&gt;Interpolations&lt;/a&gt;: it is more than 20 times faster to evaluate $w$ using the package Interpolations rather than using the package &lt;a href=&#34;https://github.com/QuantEcon/CompEcon.jl&#34; target=&#34;_blank&#34;&gt;CompEcon&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;  #using Interpolations
  #--------------------
  @time for i=1:1000000
   model.w_func.(model.grid[1])
  end
  #0.230861 seconds (2.00 M allocations: 30.518 MiB, 1.28% gc time)

  #using CompEcon
  #--------------
  @time for i=1:1000000
   model.w_func.(model.grid[1])
  end
  # 4.998902 seconds (51.00 M allocations: 3.546 GiB, 13.39% gc time)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;implementation-using-approxfun&#34;&gt;Implementation using ApproxFun&lt;/h3&gt;

&lt;p&gt;Significant speed-gains can be obtained by using the package &lt;a href=&#34;https://github.com/JuliaApproximation/ApproxFun.jl&#34; target=&#34;_blank&#34;&gt;ApproxFun&lt;/a&gt;, as illustrated by the code below. Computing time is divided by approximately $5$ compared to the implementation using CompEcon. Yet, the value function iteration implementation is still the fastest one. One bottleneck seems to be the calculation of the Jacobian by finite differences when using Broyden&amp;rsquo;s method. It is likely that using &lt;a href=&#34;https://github.com/JuliaDiff/ForwardDiff.jl&#34; target=&#34;_blank&#34;&gt;automatic differentiation&lt;/a&gt; would further improve results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#---------------------------------------------
# Julien Pascal
# Solve the stochastic optimal growth problem
# using the collocation method
# Implementation using ApproxFun
#---------------------------------------------
using QuantEcon
using Optim
using CompEcon
using PyPlot
using Interpolations
using FileIO
using ApproxFun
using ProfileView


type optGrowthCollocation

  w::Array{Float64,1}
  β::AbstractFloat
  grid::Array{Float64,1}
  u::Function
  f::Function
  shocks::Array{Float64,1}
  Tw::Array{Float64,1}
  σ::Array{Float64,1}
  el_k::Array{Float64,1}
  wl_k::Array{Float64,1}
  compute_policy::Bool
  order_approximation::Int64 #number of element in the functional basis along each dimension
  functional_basis_type::String #type of functional basis
  fspace::Dict{Symbol,Any} #functional basis
  fnodes::Array{Float64,1} #collocation nodes
  residual::Array{Float64,1} #vector of residual. Should be close to zero
  a::Array{Float64,1} #polynomial coefficients
  fApprox::ApproxFun.Fun{ApproxFun.Chebyshev{ApproxFun.Segment{Float64},Float64},Float64,Array{Float64,1}}
  w_func::Function
  tol::Float64

end


  #####################################
  # Function that find a solution
  # to f(x) = 0
  # using Broyden&#39;s &amp;quot;good&amp;quot; method
  # and simple backstepping procedure as described
  # in Miranda and Fackler (2009)
  #
  # input :
  # --------
  # * x0:                 initial guess for the root
  # * f:                  function in f(x) = 0
  # * maxit:              maximum number of iterations
  # * tol:                tolerance level for the zero
  # * fjavinc:            initial inverse of the jacobian. If not provided, then inverse of the
  #                       Jacobian is calculated by finite differences
  # * maxsteps:           maximum number of backsteps
  # * recaculateJacobian: number of iterations in-between two calculations of the Jacobian
  #
  # output :
  # --------
  # * x: one zero of f
  # * it: number of iterations necessary to reached the solution
  # * fjacinv: pseudo jacobian at the last iteration
  # * fnorm: norm f(x) at the last iteration
  #
  #######################################
  function find_broyden(x0::Vector, f::Function, maxit::Int64, tol::Float64, fjacinv = eye(length(x0));
                        maxsteps = 5, recaculateJacobian = 1)

      println(&amp;quot;a0 = $(x0)&amp;quot;)
      fnorm = tol*2
      it2 = 0 #to re-initialize the jacobian

      ################################
      #initialize guess for the matrix
      ################################
      # with Calculus
      #--------------
      fjacinv_function = x-&amp;gt; Calculus.finite_difference_jacobian(f, x)


      # If the user do not provide an initial guess for the jacobian
      # One is calculated using finite differences.
      if fjacinv == eye(length(x0))
          ################################################
          # finite differences to approximate the Jacobian
          # at the initial value
          # this is slow. Seems to improve performances
          # when x0 is of high dimension.
          println(&amp;quot;Calculating the Jacobian by finite differences&amp;quot;)
          #@time fjacinv = Calculus.finite_difference_jacobian(f, x0)
          @time fjacinv = fjacinv_function(x0)

          println(&amp;quot;Inverting the Jacobian&amp;quot;)
          try
              fjacinv = inv(fjacinv)
          catch
              try
                  println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                  fjacinv = pinv(A)
              catch
                  println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                  fjacinv = eye(length(x0))
              end
          end
          println(&amp;quot;Done&amp;quot;)
      else
          println(&amp;quot;Using User&#39;s input as a guess for the Jacobian.&amp;quot;)
      end

      fval = f(x0)

      for it=1:maxit

          it2 +=1

          #every 30 iterations, reinitilize the jacobian
          if mod(it2, recaculateJacobian) == 0

              println(&amp;quot;Re-calculating the Jacobian&amp;quot;)

              fjacinv = fjacinv_function(x0)

              try
                  fjacinv = inv(fjacinv)
              catch
                  try
                      println(&amp;quot;Jacobian non-invertible\n calculating pseudo-inverse&amp;quot;)
                      fjacinv = pinv(A)
                  catch
                      println(&amp;quot;Failing Calculating the pseudo-inverse. Initializing with In&amp;quot;)
                      fjacinv = eye(length(x0))
                  end
              end

          end


          println(&amp;quot;it = $(it)&amp;quot;)

          fnorm = norm(fval)

          if fnorm &amp;lt; tol
              println(&amp;quot;fnorm = $(fnorm)&amp;quot;)
              return x0, it, fjacinv, fnorm
          end

          d = -(fjacinv*fval)

          fnormold = Inf
          ########################
          # Backstepping procedure
          ########################
          for backstep = 1:maxsteps

              if backstep &amp;gt; 1
                  println(&amp;quot;backstep = $(backstep-1)&amp;quot;)
              end

              fvalnew = f(x0 + d)
              fnormnew = norm(fvalnew)

              if fnormnew &amp;lt; fnorm
                  break
              end

              if fnormold &amp;lt; fnormnew
                  d=2*d
                  break
              end

              fnormold = fnormnew

              d = d/2

          end
          ####################
          ####################

          x0 = x0 + d

          fold = fval
          fval = f(x0)

          u = fjacinv*(fval - fold)

          #Update the pseudo Jacobian:
          fjacinv = fjacinv + ((d-u)*(transpose(d)*fjacinv))/(dot(d,u))

          println(&amp;quot;a$(it) = $(x0)&amp;quot;)
          println(&amp;quot;fnorm = $(fnorm)&amp;quot;)

          if isnan.(x0) == trues(length(x0))
              println(&amp;quot;Error. a$(it) = NaN for each component&amp;quot;)
              x0 = zeros(length(x0))
              return x0, it, fjacinv, fnorm
          end
      end

      println(&amp;quot;In function find_broyden\n&amp;quot;)
      println(&amp;quot;Maximum number of iterations reached.\n&amp;quot;)
      println(&amp;quot;No convergence.&amp;quot;)
      println(&amp;quot;Returning fnorm = NaN as a solution&amp;quot;)
      fnorm = NaN
      return x0, maxit, fjacinv, fnorm

  end

function optGrowthCollocation(;w = Array{Float64,1}[],
                              α = 0.4,
                              β = 0.96,
                              μ = 0,
                              s = 0.1,
                              grid_max = 4,         # Largest grid point
                              grid_size = 200,      # Number of grid points
                              shock_size = 250,     # Number of shock draws in Monte Carlo integral
                              Tw = Array{Float64,1}[],
                              σ = Array{Float64,1}[],
                              el_k = Array{Float64,1}[],
                              wl_k = Array{Float64,1}[],
                              compute_policy = true,
                              order_approximation = 40,
                              functional_basis_type = &amp;quot;chebychev&amp;quot;,
                            )


  grid_y = collect(linspace(1e-4, grid_max, grid_size))
  shocks = exp.(μ + s * randn(shock_size))

  # Utility
  u(c) = log.(c)
  # Production
  f(k) = k^α

  el_k, wl_k = qnwlogn(10, μ, s^2) #10 weights and nodes for LOG(e_t) distributed N(μ,s^2)

  lower_bound_support = minimum(grid_y)
  upper_bound_support = maximum(grid_y)

  n_functional_basis = [order_approximation]

  if functional_basis_type == &amp;quot;chebychev&amp;quot;
      fspace = fundefn(:cheb, n_functional_basis, lower_bound_support, upper_bound_support)
  else
      error(&amp;quot;functional_basis_type has to be \&amp;quot;chebychev\&amp;quot; &amp;quot;)
  end


  fnodes = funnode(fspace)[1]
  residual = zeros(size(fnodes)[1])
  a = ones(size(fnodes)[1])

  tol = 0.001

  fApprox = (Fun(Chebyshev((minimum(grid_y))..(maximum(grid_y))), a))
  #fApprox = (Fun(Chebyshev(0..maximum(model.grid)), a))

  w_func = x-&amp;gt; fApprox(x)

  w = ones(size(fnodes)[1])
  Tw = ones(size(fnodes)[1])
  σ = ones(size(fnodes)[1])



  optGrowthCollocation(
    w,
    β,
    grid_y,
    u,
    f,
    shocks,
    Tw,
    σ,
    el_k,
    wl_k,
    compute_policy,
    order_approximation,
    functional_basis_type,
    fspace,
    fnodes,
    residual,
    a,
    fApprox,
    w_func,
    tol
    )
end



function residual!(model::optGrowthCollocation)


    model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a))
    model.w_func = x-&amp;gt; model.fApprox(x)

    function objective(c, y)

      expectation = 0.0

      for k = 1:length(model.wl_k)
        expectation += model.wl_k[k]*(model.w_func(model.f(y - c) * model.el_k[k]))
      end

      - model.u(c) - model.β * expectation

    end

    # Loop over nodes
    for i in 1:size(model.fnodes)[1]

        y = model.fnodes[i,1]

        res = optimize(c -&amp;gt; objective(c, y), 1e-10, y)

        if model.compute_policy
            model.σ[i] = Optim.minimizer(res)
        end

        model.Tw[i] = - Optim.minimum(res)
        model.w[i] = model.w_func(y)

        model.residual[i] = - model.w[i] + model.Tw[i]
    end

end


function solve_optgrowth!(model::optGrowthCollocation;
                         tol=1e-6,
                         max_iter=500)

    # Initialize guess for coefficients
    # by giving the &amp;quot;right shape&amp;quot;
    # ---------------------------------
    function objective_initialize!(x, model)

      #update polynomial coeffficients
      model.a = copy(x)

      model.fApprox = (Fun(Chebyshev((minimum(model.grid))..(maximum(model.grid))), model.a))

      model.w_func = x-&amp;gt; model.fApprox(x)

      return abs.(model.w_func.(model.fnodes[:,1]) - 5.0 * log.(model.fnodes[:,1]))

    end


    minx, iterations, Jac0, fnorm = find_broyden(model.a, x -&amp;gt; objective_initialize!(x, model), max_iter, tol)


    # Solving the model by collocation
    # using the initial guess calculated above
    #-----------------------------------------
    function objective_residual!(x, model)

      #update polynomial coeffficients
      model.a = copy(x)

      #calculate residual
      residual!(model)

      return abs.(model.residual)

    end

    minx, iterations, Jac, fnorm = find_broyden(model.a, x -&amp;gt; objective_residual!(x, model), max_iter, tol,  recaculateJacobian = 1)


end

#-----------------------------------
# Solve by collocation
#-----------------------------------
model = optGrowthCollocation(functional_basis_type = &amp;quot;chebychev&amp;quot;)
@time solve_optgrowth!(model)
# 15.865923 seconds (329.12 M allocations: 4.977 GiB, 5.55% gc time)


#-------------------------------
# Compare with the true solution
#-------------------------------
α = 0.4
β = 0.96
μ = 0
s = 0.1

c1 = log(1 - α * β) / (1 - β)
c2 = (μ + α * log(α * β)) / (1 - α)
c3 = 1 / (1 - β)
c4 = 1 / (1 - α * β)


# True optimal policy
c_star(y) = (1 - α * β) * y

# True value function
v_star(y) = c1 + c2 * (c3 - c4) + c4 * log.(y)

fig, ax = subplots(figsize=(9, 5))
ax[:set_ylim](-35, -24)
ax[:plot](model.grid, model.w_func.(model.grid), lw=2, alpha=0.6, label=&amp;quot;approximate value function&amp;quot;)
ax[:plot](model.grid, v_star.(model.grid), lw=2, alpha=0.6, label=&amp;quot;true value function&amp;quot;)

fig, ax = subplots(figsize=(9, 5))
ax[:set_xlim](0.1, 4.0)
ax[:set_ylim](- 0.05, 0.05)
ax[:plot](model.grid, abs.(model.w_func.(model.grid) -v_star.(model.grid)), lw=2, alpha=0.6, label=&amp;quot;error&amp;quot;)
ax[:legend](loc=&amp;quot;lower right&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Solving a simple RBC model in Dynare</title>
      <link>https://julienpascal.github.io/post/rbc_dynare/</link>
      <pubDate>Sat, 29 Jul 2017 13:44:23 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/post/rbc_dynare/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://www.dynare.org/&#34; target=&#34;_blank&#34;&gt;Dynare&lt;/a&gt; is a rich software to solve, estimate and analyse rational expectation models. While it was originally designed to solve and estimate DSGE models, Dynare has also recently been used to solve and simulate heterogeneous agents models (see &lt;a href=&#34;http://faculty.chicagobooth.edu/thomas.winberry/research/winberryAlgorithm.pdf&#34; target=&#34;_blank&#34;&gt;Winberry&lt;/a&gt; and &lt;a href=&#34;http://xavier-ragot.fr/pdf/progress/Ragot_chapter.pdf&#34; target=&#34;_blank&#34;&gt;Ragot&lt;/a&gt; for two very different approaches). Below is a simple example on how to solve and simulate a simple RBC model using Dynare.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-model&#34;&gt;A simple model&lt;/h2&gt;

&lt;p&gt;The economy is composed of a representative agent who maximizes his expected
discounted sum of utility by choosing consumption $C_t$ and labor $L_t$ for $t=1,&amp;hellip;,\infty$ &lt;br&gt;&lt;/p&gt;

&lt;p&gt;$$  \sum_{t=1}^{+\infty}\big(\frac{1}{1+\rho}\big)^{t-1} E_t\Big[log(C_t)-\frac{L_t^{1+\gamma}}{1+\gamma}\Big] $$&lt;/p&gt;

&lt;p&gt;subject to the constraint&lt;/p&gt;

&lt;p&gt;$$ K_t = K_t{-_1} (1-\delta) + w_t L_t + r_t K_t-_1 - C_t $$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\rho \in (0,\infty)$ is the rate of time preference&lt;/li&gt;
&lt;li&gt;$\gamma \in (0,\infty)$ is a labor supply parameter&lt;/li&gt;
&lt;li&gt;$w_t$ is real wage&lt;/li&gt;
&lt;li&gt;$r_t$ is the real rental rate&lt;/li&gt;
&lt;li&gt;$K_t$ is capital at the end of the period&lt;/li&gt;
&lt;li&gt;$\delta \in (0,1)$ is the capital depreciation rate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The production function writes
\begin{equation} Y_t = A_t K_t-_1^\alpha \Big((1+g)^t \Big)^{1-\alpha} \end{equation}&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$g \in (0,\infty)$ is the growth rate of production&lt;/li&gt;
&lt;li&gt;$\alpha$ and $\beta$ are technology parameters&lt;/li&gt;
&lt;li&gt;$A_t$ is a technological shock that follows and $AR(1)$ process&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\begin{equation} \log(A_t) = \lambda log(A_t-_1) + e_t\end{equation}&lt;/p&gt;

&lt;p&gt;with $e_t$ an i.i.d zero-mean normally distributed error term with standard deviation
$\sigma_1$ and $\lambda \in (0,1)$ a parameter governing the persistence of shocks.&lt;/p&gt;

&lt;h2 id=&#34;first-order-conditions&#34;&gt;First Order conditions&lt;/h2&gt;

&lt;p&gt;The F.O.C.s of the (stationarized) model are&lt;/p&gt;

&lt;p&gt;$$ \frac{1}{\hat{C_t}} = \frac{1}{1+\rho} E_t \Big( \frac{r_t+_1 + 1 - \delta}{\hat{C}_t+_1 (1+g)} \Big)$$&lt;/p&gt;

&lt;p&gt;$$ L_t^\gamma = \frac{\hat{w}_t}{\hat{C}_t}$$&lt;/p&gt;

&lt;p&gt;$$ r_t = \alpha A_t \Big(\frac{\hat{K}_t-_1}{1+g}\Big)^{\alpha-1}L_t^{1-\alpha}$$&lt;/p&gt;

&lt;p&gt;$$ \hat{w}_t = (1-\alpha) A_t \Big(\frac{\hat{K}_t-_1}{1+g}\Big)^{\alpha}L_t^{-\alpha} $$&lt;/p&gt;

&lt;p&gt;$$ \hat{K}_t + \hat{C}_t = \frac{\hat{K}_t-_1}{1+g} (1-\delta) + A_t \Big( \frac{\hat{K}_t-_1}{1+g} \Big)^{\alpha} L_t^{1-\alpha} $$&lt;/p&gt;

&lt;p&gt;with
$$ \hat{C}_t = \frac{C_t}{(1+g)^t}$$
$$ \hat{K}_t = \frac{K_t}{(1+g)^t}$$
$$ \hat{w}_t = \frac{w_t}{(1+g)^t}$$&lt;/p&gt;

&lt;h2 id=&#34;solving-and-simulating-the-model-in-dynare&#34;&gt;Solving and simulating the model in Dynare&lt;/h2&gt;

&lt;p&gt;In Dynare, one has first to specify the endogenous variables (&lt;code&gt;var&lt;/code&gt;), exogenous variables (&lt;code&gt;varexo&lt;/code&gt;),
and the parameters&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;var C K L w r A;
varexo e;

parameters rho delta gamma alpha lambda g;
alpha = 0.33;
delta = 0.1;
rho = 0.03;
lambda = 0.97;
gamma = 0;
g = 0.015;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In a second step, the F.O.C.s of the model has to be expressed using the command &lt;code&gt;model&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;model;
1/C=1/(1+rho)*(1/(C(+1)*(1+g)))*(r(+1)+1-delta);
L^gamma = w/C;
r = alpha*A*(K(-1)/(1+g))^(alpha-1)*L^(1-alpha);
w = (1-alpha)*A*(K(-1)/(1+g))^alpha*L^(-alpha);
K+C = (K(-1)/(1+g))*(1-delta)
+A*(K(-1)/(1+g))^alpha*L^(1-alpha);
log(A) = lambda*log(A(-1))+e;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The user must provide the analytical solution for the steady state of the model using the command &lt;code&gt;steady_state_model&lt;/code&gt;.
The command &lt;code&gt;steady&lt;/code&gt; solves for the steady state values of the model&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;steady_state_model;
A = 1;
r = (1+g)*(1+rho)+delta-1;
L = ((1-alpha)/(r/alpha-delta-g))*r/alpha;
K = (1+g)*(r/alpha)^(1/(alpha-1))*L;
C = (1-delta)*K/(1+g)
+(K/(1+g))^alpha*L^(1-alpha)-K;
w = C;
end;

steady;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command &lt;code&gt;shocks&lt;/code&gt; defines the type of shock to be simulated&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;shocks;
var e; stderr 0.01;
end;

check;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A first order expansion around the steady state is obtained using the command
&lt;code&gt;stoch_simul(order=1)&lt;/code&gt;
This function computes impulse response functions (IRF) and returns various descriptive statistics (moments, variance decomposition, correlation and autocorrelation coefficients)&lt;/p&gt;

&lt;p&gt;The IRF produced by Dynare should be pretty similar to the following graph:
&lt;img src=&#34;https://julienpascal.github.io/img/rbc1_IRF_e.png&#34; alt=&#34;IRF simple RBC&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficiency and contestability in emerging market banking systems</title>
      <link>https://julienpascal.github.io/publication/efficiencycontestability/</link>
      <pubDate>Tue, 07 Mar 2017 15:51:54 +0100</pubDate>
      
      <guid>https://julienpascal.github.io/publication/efficiencycontestability/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
